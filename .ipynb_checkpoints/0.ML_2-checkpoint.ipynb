{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48707e18acfa4157",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T19:36:18.073215900Z",
     "start_time": "2023-09-15T19:36:17.872173Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be06f6bdd87fe5f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Deep Learning Classifiers\n",
    "- deep refers to the number of layers in the neural network, not any deeper understanding\n",
    "#### Deep Belief Networks\n",
    "- a multilayer neural network that is trained in a greedy layer-wise fashion\n",
    "- a stack of restricted Boltzmann machines (RBMs) in which each RBM layer communicates with the previous and next layer\n",
    "- RBMs are another name for the simple neural network nodes we've been using\n",
    "- uses a softmax output layer to classify the data\n",
    "    - softmax is a generalization of the logistic function that squashes a K-dimensional vector of arbitrary real values to a K-dimensional vector of real values in the range (0, 1) that add up to 1\n",
    "    - i.e. it converts a vector of real values to a probability distribution that sums to 1\n",
    "    - <img src=\"images/softmax.png\">\n",
    "    - e.g. `Softmax([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0])`\n",
    "        - = `[0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175])`\n",
    "    - the output puts most of the weight where the 4 was in the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3e5ffd85d2ece",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Convolutional Neural Networks (CNNs)\n",
    "- a type of deep neural network that is often used for image recognition\n",
    "- regularized version of neural networks that use convolutional layers\n",
    "- convolution reduces the number of free parameters, reducing the chance of overfitting\n",
    "    - i.e. pictures have a lot of pixels, so the number of free parameters in a neural network would be huge\n",
    "- inputs are called tensors\n",
    "    - tensors are multidimensional arrays\n",
    "    - a 2D tensor is a matrix\n",
    "    - a 3D tensor is a cube\n",
    "    - a 4D tensor is a cube with multiple channels\n",
    "- hidden layers consist of\n",
    "    - convolutional layers\n",
    "    - pooling layers\n",
    "    - ReLU layers\n",
    "        - ReLU stands for rectified linear unit\n",
    "- convolutional layer\n",
    "    - a window slides over the input tensor and performs a function on each part of the input\n",
    "    - the window is called a kernel\n",
    "    - the kernel is a matrix of weights\n",
    "    - the kernel is applied to each part of the input\n",
    "    - <img src=\"images/sliding_pane_CNN.png\">\n",
    "    - reduces the feature map size via some function\n",
    "- pooling layer\n",
    "    - a window slides over the feature map and takes the average or max value in each window\n",
    "    - e.g. max pooling\n",
    "        - takes the max value in each window\n",
    "- ReLU layer\n",
    "    - applies the ReLU function to each element of the feature map\n",
    "        - ReLU(x) = max(0, x)\n",
    "            - i.e. if x is negative, it is replaced with 0, otherwise it is left alone\n",
    "- Keras (https://keras.io/) has probably the best python CNN setup out of the box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57306627fdd9050b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### IC 14Sep23\n",
    "1. $1 + 2 + 3 = 6$\n",
    "-  $6 \\cdot 6 + 3 = 39$\n",
    "2. index 0\n",
    "3. 1\n",
    "4. a. 3\n",
    "    b. 2\n",
    "5. ReLU(-3) = 0, ReLU(0) = 0, ReLU(3) = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe648a1d7b820b01",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Recurrent Neural Networks (RNNs)\n",
    "- HMM assumes that the probability of a state depends only on the previous state\n",
    "- RNN uses information from more than one previous input and output\n",
    "- RNN classifies a sequence of inputs in time or space\n",
    "- process one element at a time while retaining a memory of what has come before\n",
    "    - memory is called a cell state\n",
    "    - allows the network to learn long-term dependencies\n",
    "- how do you make an RNN remember things\n",
    "    - in a normal neural network, the output is a function of the input\n",
    "        - $f(x) = wx +b$\n",
    "    - in an RNN, the output is a function of the input and the previous state\n",
    "        - $h(t) = f_1(x(t))$\n",
    "                - $h(t)$ is the output at time t\n",
    "                - $x(t)$ is the input at time t\n",
    "                - $f_1(x_t)$ is the function that determines the output at time t\n",
    "        - expand that\n",
    "                - $h_t = f_1(x_t, h_{t-1})$\n",
    "    - recurrence is increased with cell output $c_t$\n",
    "        - $c_t = f_3(x_t, h_{t-1}, c_{t-1})$\n",
    "        - $h_t = f_4(x_t, h_{t-1}, c_t)$\n",
    "        - there are several methods for calculating $f_3$ and $f_4$\n",
    "            - LSTM (long short-term memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314a6bbeab77f3f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### LSTM\n",
    "- <img src=\"images/LSTM.png\" width=\"800\">\n",
    "- forget gate, $f_t$\n",
    "    - $W_f, U_f, & b_f$ are weight matrices and bias vector\n",
    "    - $x_t$ is the input at time t\n",
    "    - $h_{t-1}$ is the output at time t-1\n",
    "    - $\\sigma_g$ is the sigmoid function\n",
    "        - $\\sigma_g(x) = \\frac{1}{1+e^{-x}}$\n",
    "        - used extensively in neural networks\n",
    "        - squashes the input to a value between 0 and 1\n",
    "- $i_t$ is the input gate\n",
    "    - $i_t = \\sigma_g(W_i \\cdot x_t + U_i \\cdot h_{t-1} + b_i)$\n",
    "        - $W_i, U_i, & b_i$ are weight matrices and bias vector\n",
    "        - $x_t$ is the input at time t\n",
    "        - $h_{t-1}$ is the output at time t-1\n",
    "        - $\\sigma_g$ is the sigmoid function\n",
    "- $c_t$ is the cell state at time t\n",
    "    - $c_t = f_3(x_t, h_{t-1}, c_{t-1})$ _\n",
    "        - $c_t = f_t \\cdot c_{t-1} + i_t \\cdot \\sigma_c(W_c \\cdot x_t + U_c \\cdot h_{t-1} + b_c)$\n",
    "        - $f_t$ is the forget gate\n",
    "        - $i_t$ is the input gate\n",
    "        - $W_c, U_c, & b_c$ are weight matrices and bias vector\n",
    "        - $x_t$ is the input at time t\n",
    "        - $h_{t-1}$ is the output at time t-1\n",
    "        - $\\sigma_c$ is the tanh function\n",
    "        - $\\sigma_c(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "            - also used to squash the input to a value between -1 and 1\n",
    "        - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f3f75a61eded8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### IC 21Sep30\n",
    "1. $c_{t-1}$ is the previous cell output\n",
    "    - $h_{t-1}$ is the previous ouput\n",
    "    - $x_t$ is the current input at time t\n",
    "2. $c_t$ is the current cell output\n",
    "    - $h_t$ is the current output\n",
    "3. $c_{t-1}$ is used to \"remember\" the previous cell output\n",
    "    - $h_{t-1}$ is used to \"remember\" the previous output\n",
    "4. $c_t$ is the current cell output \n",
    "    - $h_t$ is the current output\n",
    "5. a)  the forget gate function uses $x_t$ and $h_{t-1}$\n",
    "    - b) the input gate function uses $x_t$ and $h_{t-1}$\n",
    "    - c) the output gate function uses $x_t$ and $h_{t-1}$\n",
    "6. the cell state function uses $c_{t-1}$, $x_t$, and $h_{t-1}$\n",
    "7. the output function uses $c_t$, $x_t$, and $h_{t-1}$\n",
    "8. $[1, 4, 5] \\cdot [2, 6, 9] = [2, 24, 45]$\n",
    "9. $f_t = [\\frac{1}{e^{-(W_fx_t + U_fh_{t-1} + b_f)}]$\n",
    "10.  $c_t = tanh(W_f x_t + U_f h_{t-1} + b_f)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ad2f2b02cccd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### IC 21Sep30\n",
    "1. c_{t-1} is the previous cell output\n",
    "    - h_{t-1} is the previous ouput\n",
    "    - x_t is the current input at time t\n",
    "2. c_t is the current cell output\n",
    "    - h_t is the current output\n",
    "3. c_{t-1} is used to \"remember\" the previous cell output\n",
    "    - h_{t-1} is used to \"remember\" the previous output\n",
    "4. c_t is the current cell output \n",
    "    - h_t is the current output\n",
    "5. a)  the forget gate function uses x_t and $h_{t-1}\n",
    "    - b) the input gate function uses x_t and $h_{t-1}\n",
    "    - c) the output gate function uses x_t and $h_{t-1}\n",
    "6. the cell state function uses c_{t-1}, x_t, and h_{t-1}\n",
    "7. the output function uses c_t, x_t, and h_{t-1}\n",
    "8. [1, 4, 5] dot [2, 6, 9] = [2, 24, 45]\n",
    "9. f_t = (1)/(e^-(W_fx_t + U_fh_{t-1} + b_f)\n",
    "10.  c_t = tanh(W_f x_t + U_f h_{t-1} + b_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78fef340b29ebcd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "10. $c_t - c_t$ = $tanh(W_fx_t + U_fh_{t-1} + b_f)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba98db5e33ab6e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### IC 26Sep23\n",
    "1. a GaussianNoise layer could be added to the model to add noise to the input and prevent overfitting\n",
    "2. simple dropout is used because it relies on removing a random subset of the input data to prevent overfitting rather than adding noise which could change the meaning of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb71fa1efe6398",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dimensionality Reduction\n",
    "- applies to all supervised ML models\n",
    "- why?\n",
    "    - reduces time complexity\n",
    "    - reduces space complexity\n",
    "    - saves cost of observing the features\n",
    "    - reduces the number of features\n",
    "- how?\n",
    "    - eliminate features that are highly correlated (i.e. redundant)\n",
    "    - eliminate features that are not correlated with the target (i.e. irrelevant)\n",
    "### Feature Transformation - PCA\n",
    "- PCA (principal component analysis) is a dimensionality reduction technique\n",
    "- project $x$ onto a lower dimensional subspace $z$\n",
    "    - $x$ is a vector of d input features\n",
    "    - $z$ is also a vector of d features\n",
    "    - minimize information loss by maximizing the variance of the projected data\n",
    "    - projection of $x$ is $z = w^Tx$\n",
    "        - $w$ maximizes the covariance of $z$\n",
    "    - $z$ is not a subset of $x$\n",
    "        - it is the same number of features as $x$ though\n",
    "    - a subset of $k$ features from $z$ is used as the input to the model\n",
    "- <img src=\"images/PCA.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a063b4f78de4a217",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- e.g. mean centered matrix\n",
    "    - mean of the column is subtracted from each element in the column\n",
    "    - <img src=\"images/mean_centered_matrix.png\" width=\"800\">\n",
    "    - next, calculate the covariance matrix\n",
    "        - <img src=\"images/covariance_matrix.png\" width=\"800\">\n",
    "    - next, calculate the eigenvectors and eigenvalues of the covariance matrix\n",
    "        - <img src=\"images/eigenvs.png\" width=\"800\">\n",
    "    - next, determine the transformed parameters\n",
    "        - $x$ is the x_centered matrix of original features\n",
    "        - $w$ is the matrix of eigenvectors\n",
    "        - $z$ is the transformed matrix of features\n",
    "            - $z = xw^T$\n",
    "                    - note that $x$ and $w^T$ are reversed from the previous equation\n",
    "    - finally, select the top $k$ features from $z$ to use as the input to the model\n",
    "        - selected from the eigenvector matrix\n",
    "        - the column in the eigenvector matrix with the highest eigenvalue is the most important feature\n",
    "            - this is the first principal component\n",
    "        - the column in the eigenvector matrix with the second highest eigenvalue is the second most important feature\n",
    "            - this is the second principal component\n",
    "        - and so on\n",
    "        - the top $k$ features are the first $k$ principal components\n",
    "        - if you want to reduce the number of features from $d$ to $k$, you select the first $k$ principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5a47e4b80dd52",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Choosing k\n",
    "-  Proportion of Variance (PoV)\n",
    "    - <img src=\"images/PoV.png\"> \n",
    "    - $\\lambda_i$ is the $i^{th}$ eigenvalue\n",
    "    - typically stop at PoV > 0.9\n",
    "- <img src=\"images/choosingk.png\">\n",
    "### What PCA Does\n",
    "- centers the data around the origin\n",
    "- <img src=\"images/PCAplot.png\">\n",
    "- the eigenvalues matrix is a covariance matrix with\n",
    "    - no covariance\n",
    "    - variance concentrated in the principal components\n",
    "        - more variance means more variance in features between classes\n",
    "            - should translate to better classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4795c2cab1c9b53",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Python Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741eb6ab21068881",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T21:27:46.044813300Z",
     "start_time": "2023-09-28T21:27:45.252376900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecomposition\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PCA\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Get data\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m dataset \u001B[38;5;241m=\u001B[39m read_csv(\u001B[43murl\u001B[49m, names\u001B[38;5;241m=\u001B[39mnames)\n\u001B[0;32m      7\u001B[0m array \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m      8\u001B[0m x \u001B[38;5;241m=\u001B[39m array[:, \u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m4\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "# Get data\n",
    "dataset = read_csv(url, names=names)\n",
    "array = dataset.values\n",
    "x = array[:, 0:4]\n",
    "y = array[:, 4]\n",
    "# Create PCA instance\n",
    "pca = PCA(n_components=4)\n",
    "# Perform PCA\n",
    "pca.fit(x)\n",
    "# Get eigenvectors and eigenvalues\n",
    "eigenvectors = pca.components_\n",
    "eigenvalues = pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e682d04b4dbad3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform data\n",
    "principleComponents = pca.transform(x)\n",
    "# Calculate PoVs\n",
    "sumvariance = np.cumsum(eigenvalues)\n",
    "sumvariance /= sumvariance[-1]\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = list(zip(eigenvalues, eigenvectors))\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "# eigen_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "# Transform data (x) to Z\n",
    "W = eigen_pairs[0][1].reshape(4, 1)\n",
    "Z = principleComponents.dot(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77f584ba7ce7ee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### IC 28Sep23\n",
    "- 1) [[-2.33, -2.33, -2.33], [-0.33, -0.33, -0.33], [2.66, 2.66, 2.66]]\n",
    "- 2) a. 6.33\n",
    "- 2) b. 4.33\n",
    "- 3) 2, using the first two principal components yields PoV of 97.8% which is greater than 97%"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PCA Continued\n",
    "- a key weakness is that it assumes a linear relationship between the features\n",
    "    - i.e. straight line relationship\n",
    "- correlation can range from -1 to + 1 (perfect negative correlation to perfect positive correlation)\n",
    "    - 0 represents no correlation at all\n",
    "- typical ML applications do not have linear relationships\n",
    "    - e.g. a car's price is not linearly related to its mileage\n",
    "- one solution or mitigation strategy is to apply a feature selection technique to reduce the number of features d to k"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "836565e7a0e05c82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Selection\n",
    "- brute force\n",
    "    - try all possible combinations of features\n",
    "    - select the combination that yields the best results\n",
    "    - this is not feasible for large d\n",
    "    - $2^d - 1$ iterations are required to test all combinations of features\n",
    "- sequential search strategies\n",
    "    - greedy algorithms - i.e. they find sub-optimal solutions\n",
    "    - forward selection\n",
    "        - search forward and add the best feature at each step\n",
    "            1. start with no features\n",
    "            2. find the feature that yields the best results\n",
    "            3. add that feature to the model\n",
    "            4. stop when adding more features does not improve the results\n",
    "        - max iterations = d\n",
    "        - max k-fold cross validation = $\\frac{d(d+1)}{2}$\n",
    "    - backward selection\n",
    "        - search backward and remove the worst feature at each step\n",
    "            1. start with all features\n",
    "            2. find the feature that yields the worst results\n",
    "            3. remove that feature from the model\n",
    "            4. stop when removing more features does not improve the results\n",
    "        - max iterations = d - 1\n",
    "        - max k-fold cross validation = $\\frac{d(d+1)}{2} - 1$\n",
    "    - plus-l minus-r selection (LRS)\n",
    "        - generalized form of forward and backward selection\n",
    "        - if L > R, it is forward selection\n",
    "        - if L < R, it is backward selection\n",
    "        - LRS attempts to compensate for weaknesses in forward and backward selection\n",
    "            - involves some backtracking\n",
    "        - primary limitation is the lack of theory to predict the optimal values of L and R \n",
    "        - ```\n",
    "            If L>R then\n",
    "                F = âˆ…\n",
    "                Repeat L times\n",
    "                    Find the best feature and add it to F\n",
    "                Repeat R times\n",
    "                    Find the worst feature and delete it from F\n",
    "            Else\n",
    "                F = X\n",
    "                Repeat R times\n",
    "                    Find the worst feature and delete it from F\n",
    "                Repeat L times\n",
    "                    Find the best feature and add it to F\n",
    "            Endif\n",
    "            ```\n",
    "        - <img src=\"images/lrs_example.png\" width=\"800\">\n",
    "    - bidirectional search\n",
    "        - parallel forward and backward search\n",
    "        - to guarantee convergence, the forward and backward searches must be synchronized\n",
    "            - features added in forward cannot be removed in backward\n",
    "            - features removed in backward cannot be added in forward\n",
    "        - <img src=\"images/bds_example.png\" width=\"800\">\n",
    "    - sequential floating selection (SFS)\n",
    "        - forward selection with backtracking\n",
    "        - rather than fixing values of L and R, floating methods allow L and R to be determined from the data\n",
    "        - two methods\n",
    "            - sequential forward floating selection (SFFS)\n",
    "                - starts with no features\n",
    "                - after each forward step, it checks to see if removing any features would improve the results\n",
    "            - sequential backward floating selection (SBFS)\n",
    "                - starts with all features\n",
    "                - after each backward step, it checks to see if adding any features would improve the results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3be128253751865"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IC 03Oct23\n",
    "- 1) iteration time = 0.1 seconds\n",
    "        - $d$ = 8\n",
    "        - $n = 2^d -1$\n",
    "        - $2^8 - 1 = 255$  iterations\n",
    "        - $255 \\cdot 0.1 = 25.5$ seconds\n",
    "- 2) X = {a, b, c, d}, F = {a, b} \n",
    "        - a. the first feature to be evaluated is c\n",
    "        - b. the first feature to be evaluated is d\n",
    "- 3) X = {a, b, c, d}, after step 2: F = {a, b}, L = 1, R = 3\n",
    "        - a. the next features to be evaluated are c and d\n",
    "        - b. R > L the next step is to remove the worst feature from F\n",
    "- 4) Y = {a, b, f, g}, x = h, Acc({a, b, f, g}) = 0.89, and Acc({a, b, f, g, h}) = 0.91 in step 4\n",
    "        - a. the next Y to be evaluated is {a, b, f, g, h}\n",
    "        - b. the next step is to remove the worst feature from Y"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2443aa5e49946ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
