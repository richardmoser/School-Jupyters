{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4cb156",
   "metadata": {},
   "source": [
    "### SCC Stuff\n",
    "- 12s server ssh location login1.hpc.ku.edu or login2.hpc.ku.edu\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a5de6",
   "metadata": {},
   "source": [
    "### SLURM\n",
    "- Simple Linux Utility for Resource Management\n",
    "- how you talk to nodes\n",
    "- Commands\n",
    "    - srun\n",
    "        - synchronously submit a single command to run on new or existing allocation\n",
    "        - i.e. how you run things\n",
    "    - salloc\n",
    "        - give you nodes and a shell within the nodes\n",
    "            - on some clusters this is restricted\n",
    "        - `salloc -n5` will give you 5 nodes\n",
    "            - **make sure you cancel the allocation once you're done**\n",
    "        - `salloc -p gpu` will give you a node with a GPU\n",
    "        - doesn't free the resources once the job/command is done\n",
    "            - frees once you leave the session\n",
    "            - doesn't free up the nodes? not sure\n",
    "        - `man salloc` to read more about options\n",
    "    - sbatch\n",
    "        - shell script specifying node number & other parameters and SLURM will go run it\n",
    "    - squeue\n",
    "        - shows all the jobs running on the nodes\n",
    "    - scancel [JOBID]\n",
    "        - cancels the job with ID JOBID\n",
    "        - `scancel -u $USER` will cancel all of your jobs by user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2dd948",
   "metadata": {},
   "source": [
    "### MPI\n",
    "- You really need SLURM and MPI\n",
    "    - SLURM manages jobs across multiple users\n",
    "- e.g. matrix mutiplication\n",
    "    - have different nodes/processes/cores do each part of the multiplication and put it all back together at the end\n",
    "- all nodes must be able to access the same shared directory for read and write\n",
    "- `mpirun -np 10 -hostfile hostfile ./test_mpi`\n",
    "    - test_mpi is the code binary to be run\n",
    "- sometimes compute nodes don't have an OS\n",
    "\n",
    "- install the OS as you would normally\n",
    "    - use most defaults, probably bump the core number up\n",
    "    - install OpenSSH server\n",
    "    - Repeat process for compute nodes\n",
    "    \n",
    "### script\n",
    "- https://github.com/AbirHaque/KU-SCC-2023-Resources/blob/main/MPI-VM-Cluster-Tutorial/head.sh \n",
    "- line 13\n",
    "    - mpiuser does things\n",
    "        - you give it a bunch of permissions\n",
    "    - \n",
    "\n",
    "- For Compute node(s)\n",
    "    - follow the compute script\n",
    "    - or run it\n",
    "    \n",
    "- hostfile\n",
    "    - lists IPs and aliases\n",
    "- \n",
    "    \n",
    "    \n",
    "### Curtain\n",
    "- installs stuff fast or something\n",
    "- further googling required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e1bcd",
   "metadata": {},
   "source": [
    "### SCC App\n",
    "- Hardware\n",
    "    - pending... what hardware\n",
    "- Sponsor\n",
    "    - also pending\n",
    "- Diversity\n",
    "    - specifically not including skill diversity in this portion\n",
    "- Team Preparation\n",
    "    - talking to the I2S team\n",
    "    - talking to the Los Alamos ICP guy (Quincy?)\n",
    "    - VM cluster training\n",
    "    - weekly meetings with presentations on relevant hardware and software\n",
    "    - team is organized based on skillsets and interests (e.g. HPC, cloud, software, etc.) with flexibility to move around as requirements evolve. Yara is the team lead.\n",
    "    - advisor background:\n",
    "- Team Education Goals\n",
    "    - taking things from black box to understood\n",
    "    - understand how to implement and operate a cluster\n",
    "    - HPCs are the infrastructure behind AI\n",
    "- Strength of Approach for Software and Cloud Admin\n",
    "    - cloud pipelines\n",
    "    - \"AWS to summon a cluster up\" - James\n",
    "    - using containers\n",
    "    \n",
    "- Check boxes\n",
    "    - advisors\n",
    "        - Comp Scientists, HPC Staff, Faculty\n",
    "    - Heard about\n",
    "        - at conference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd912a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdf139e3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
