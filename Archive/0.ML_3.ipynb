{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffe549682c0500a2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Genetic Algorithm\n",
    "- a non greedy algorithm for finding the optimal feature set\n",
    "- good for when brute forcing is impractical\n",
    "- converge to an optimal solution given enough time\n",
    "- genetic algorithms rely on mutation, crossover, and selection\n",
    "- process:\n",
    "    - start with an initial population of individuals (features)\n",
    "    - each iteration is called a generation\n",
    "    - in each generation, the fitness of every individual is evaluated\n",
    "        - fitness is usually the value of the objective function in the optimization problem (in our case, the accuracy of the model)\n",
    "    - the individuals with the highest fitness are selected\n",
    "        - take the n most fit individuals\n",
    "    - the selected individuals are mutated and crossed over\n",
    "        - mutation: randomly change some of the features\n",
    "            -   defends against getting trapped in local optima\n",
    "        - crossover: combine features from two individuals to create a new individual\n",
    "    - terminate after a certain number of generations or when the fitness of the population reaches a certain threshold\n",
    "        - in our case, we will terminate after the maximum number of features is reached\n",
    "    - else, go back to step 2 and select the n most fit individuals\n",
    "- crossover\n",
    "    - can be done in many ways\n",
    "    - one way is union and intersection\n",
    "        - union: take all the best features from both individuals\n",
    "        - intersection: take only the best features that are in both individuals\n",
    "- mutation\n",
    "    - can be done in many ways\n",
    "    - one way is to randomly add, remove features, or swap features\n",
    "        - randomly choose whether to add, remove, or swap\n",
    "        - then randomly choose which feature to add, remove, or swap\n",
    "    - the add/swap pool should not include features that are already in the individual\n",
    "        - i.e. you should never have duplicate features in an individual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5bc3760926789b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- advantages\n",
    "    - easy to explain\n",
    "    - does not make assumptions about the data\n",
    "    - fast search technique\n",
    "    - \"close\" to optimal solutions in \"reasonable\" time\n",
    "    - suitable for parallel implementation\n",
    "        - can calculate fitness of individuals in parallel\n",
    "    - fairly simple to implement\n",
    "- disadvantages\n",
    "    - none, according to Dr. Johnson's experience\n",
    "- tips\n",
    "    - crossover rate should be high - 0.8 or 0.95\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3a60f3c5227b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### In Class 10Oct23\n",
    "- 1. A $\\cup$ B = ${z_1, SL, PL, PW, z_2}$\n",
    "- 2. D $\\cap$ E = ${z_1, z_2, z_3, z_4}$\n",
    "- 3. new C = ${z_1, z_2, z_3, SW, PL, SL}$\n",
    "- 4. new C = ${z_1, z_2, z_3, SW}$\n",
    "- 5. new C = ${z_1, z_2, z_3, SW, SL}$\n",
    "- 6. 50 sets k folded at 0.1 seconds each = 5 seconds\n",
    "- 7. 72 sets k folded at 0.1 seconds each = 7.2 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea813b3056e68d2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Unbalanced Datasets\n",
    "- not all datasets will be balanced\n",
    "    - i.e. the number of samples in each class will not be equal\n",
    "    - e.g. a child running into the street is rare but critical to autonomous vehicle training\n",
    "- methods for dealing with imbalanced data\n",
    "    - collect more data\n",
    "        - not always possible, but probably the best solution if it is\n",
    "        - concentrate on collecting more data for the minority class\n",
    "        - even if not balanced, more data is always better\n",
    "            - can be used in oversampling and undersampling (discussed below)\n",
    "    - change the performance metric\n",
    "        - accuracy is not a good metric for imbalanced data\n",
    "        - there is no consensus on what the best metric is\n",
    "            - Class Balance Accuracy (CBA)\n",
    "                - minimum between precision and recall is computed for each class\n",
    "                - then the average of the minimums is computed\n",
    "                - this is the CBA\n",
    "            - Balanced Accuracy (BA)\n",
    "                - average of the recall and specificity is computed for each class\n",
    "                -  then the average of the averages is computed\n",
    "                - this is the BA\n",
    "        - specificity is the true negative rate\n",
    "            - $specificity = \\frac{TN}{TN + FP}$\n",
    "            - <img src=\"images/specificity.png\" width=\"800\">\n",
    "            - TN is all of the non-fish instances that ***are not*** classified as fish\n",
    "                - $TN_{fish} = 4 + 3 + 1 + 6 = 14$\n",
    "            - FP is all of the non-fish instances that ***are*** classified as fish\n",
    "                - $FP_{fish} = 1 + 0 = 1$\n",
    "            - specificity = $\\frac{14}{14 + 1} = 0.933$\n",
    "        - `balanced_score_accuracy` is a scikit-learn function that is different than CBA an BA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9102367c126e3a10",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Oversampling\n",
    "- random oversampling\n",
    "    - randomly duplicate samples from the minority class until it is as large as the majority classes\n",
    "        - randomly chosen with replacement\n",
    "    - this is the simplest method\n",
    "    - can lead to overfitting\n",
    "- SMOTE oversampling\n",
    "    - Synthetic Minority Oversampling Technique\n",
    "    - instead of duplicating samples, create new samples that are similar to the minority class\n",
    "    - for each sample in the minority class, find the k nearest neighbors\n",
    "        - k is a hyperparameter\n",
    "    - randomly choose one of the k nearest neighbors\n",
    "    - create a new sample that is a random combination of the sample and the randomly chosen neighbor\n",
    "        - the sample will be the vector between the sample and the neighbor multiplied by a random number between 0 and 1\n",
    "        - i.e. the new sample will be somewhere on the line between the sample and the neighbor\n",
    "    - repeat until the minority class is as large as the majority classes\n",
    "- ADASYN oversampling\n",
    "    - Adaptive Synthetic Sampling\n",
    "    - similar to SMOTE\n",
    "    - finds nearest neighbors\n",
    "    - instead of randomly choosing one of the neighbors, it uses a probability distribution to choose the neighbor\n",
    "        - neighbors that are closer to the sample have a higher probability of being chosen\n",
    "        - neighbors can be chosen multiple times\n",
    "### Undersampling\n",
    "- random undersampling\n",
    "    - randomly remove samples from the majority class until it is as small as the minority classes\n",
    "    - this is the simplest method\n",
    "    - can increase variance\n",
    "    - can discard useful and important samples\n",
    "- cluster undersampling\n",
    "    - replaces a cluster of samples with a single sample at the centroid of the cluster\n",
    "    - centroid is calculated by taking the mean of each feature\n",
    "        - this is via k-means clustering\n",
    "- Tomek Links Undersampling\n",
    "    - finds a pair of samples that are nearest neighbors but are of different classes\n",
    "    - removes the majority class sample from the pair\n",
    "    - this is repeated until the desired balance is achieved\n",
    "    - i.e. it erases the boundary between the classes by removing majority samples only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c332e5baae7a1b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8547d7b4f837a070",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### IC 12Oct23\n",
    "- 1.- Cat:\n",
    "        - Precision: $\\frac{TP}{TP + FP} = \\frac{4}{4+9} = \\frac{4}{13} = 0.308$\n",
    "        - Recall: $\\frac{TP}{TP + FN} = \\frac{4}{4+2} = \\frac{2}{3} = 0.667$\n",
    "    - Fish:\n",
    "        - Precision: $\\frac{TP}{TP + FP} = \\frac{2}{2+1} = \\frac{2}{3} = 0.667$\n",
    "        - Recall: $\\frac{TP}{TP + FN} = \\frac{2}{2+8} = \\frac{1}{5} = 0.2$\n",
    "    - Hen:\n",
    "        - Precision: $\\frac{TP}{TP + FP} = \\frac{6}{6+3} = \\frac{2}{3} = 0.667$\n",
    "        - Recall: $\\frac{TP}{TP + FN} = \\frac{6}{6+3} = \\frac{2}{3} = 0.667$\n",
    "    - Class balance accuracy: $\\frac{0.308 + 0.2 + 0.667}{3} = 0.392$\n",
    "- 2. - Cat:\n",
    "        - Specificity: $\\frac{TN}{TN + FP} = \\frac{9}{9+2} = \\frac{9}{11} = 0.818$\n",
    "    - Fish:\n",
    "        -  \n",
    "            - \n",
    "- 3. $d = \\sqrt{(5.1-4.9)^2+(3.5-3)^2+(1.4-1.4)^2+(.2-.2)^2}$\n",
    "        - $d = \\sqrt{.04+.25+0+0}$\n",
    "        - $d = \\sqrt{.29}$\n",
    "        - $d = 0.539$\n",
    "- 4. RN = 0.4\n",
    "    synthetic sample $= (x_r + RN(x_n - x_r), y_r + RN(y_n-y_r), z_r + RN(z_n-z_r), a_r + RN(a_n-a_r)$\n",
    "        - $x = 5.1 + 0.4(4.9-5.1) = 5.1 + 0.4(-0.2) = 5.1 - 0.08 = 5.02$\n",
    "        - $y = 3.5 + 0.4(3-3.5) = 3.5 + 0.4(-0.5) = 3.5 - 0.2 = 3.3$\n",
    "        - $z = 1.4 + 0.4(1.4-1.4) = 1.4 + 0.4(0) = 1.4$\n",
    "        - $a = 0.2 + 0.4(0.2-0.2) = 0.2 + 0.4(0) = 0.2$\n",
    "        - $synthetic sample = (5.02, 3.3, 1.4, 0.2)$\n",
    "- 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1fb489a346a7c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Ensemble Learning\n",
    "- ensemble learning is the process of combining multiple models to solve a problem\n",
    "    - generally attain higher accuracy\n",
    "    - no single ML model is the best for all problems\n",
    "    - ensemble learning is a way to combine the strengths of multiple models\n",
    "    - questions\n",
    "        - how do you combine outputs of multiple models?\n",
    "        - how do you generate base learners that complement each other?\n",
    "- fusion\n",
    "    - combination of outputs\n",
    "    - categorical\n",
    "        - vote by most frequent output\n",
    "        - ties broken by random choice\n",
    "    - value\n",
    "        - average of outputs\n",
    "            - round if class is an integer\n",
    "            - classification = $\\frac{sum of outputs}{number of models}$\n",
    "        - median of outputs\n",
    "            - round if class is an integer\n",
    "            - if even number of models, take average of middle two values\n",
    "    - in python\n",
    "        - scikit-learn has a `VotingClassifier` method\n",
    "            - supports voting, weighted average, and mean\n",
    "### Generating Base-Learners\n",
    "- pairwise coupling method 1\n",
    "    - multiclassifiers are usually worse than binary classifiers\n",
    "    - break multiclass problem into multiple binary problems\n",
    "    - only works with models that output normalized probabilities\n",
    "    - <img src=\"images/pairwise.png\">\n",
    "    - basically, each model is trained to detect a single class\n",
    "        - in the e.g. above, C1 just says setosa or not setosa\n",
    "- pairwise coupling method 2\n",
    "    - each base learner compares a pair of classes and all possible pairs are combined\n",
    "    - <img src=\"images/pairwise2.png\">\n",
    "    - basically, each model is trained to detect two classes\n",
    "        - other classes are misclassified by that model as one of the two classes\n",
    "        - the assumption is that the output probability would be close to 50% for those misclassifications and 1 or 0 for the classes it is trained to detect\n",
    "        - in theory, the fusion step weeds out the misclassifications  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71117dc287eb88e9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bagging\n",
    "- bootstrap aggregating\n",
    "    - sometimes random subspace method of random forests is also called bagging\n",
    "- bootstrap aggregating is a method of generating multiple models from a single dataset\n",
    "- each model is trained on a slightly different section of the data\n",
    "- bootstrapping is a method of sampling with replacement\n",
    "    - i.e. each sample can be chosen multiple times\n",
    "    - each sample has an equal probability of being chosen\n",
    "- typically implemented with decision trees and neural networks\n",
    "    - can improve unstable models \n",
    "        - i.e. models that may give different answers with the same input\n",
    "    - can also degrade stable models\n",
    "        - i.e. models that will always give the same answer with the same input\n",
    "- in python\n",
    "    - scikit-learn has a `BaggingClassifier` method to perform bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dbc69dd2fe4a22",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### In Class 19Oct23\n",
    "- 1. a \n",
    "        - 2(.12) + 3(.18) + 7(.21) + 2(.18) + 9(.17) + 4(.14) = 4.7\n",
    "        - 4.7/6 = 0.783\n",
    "- 1. b \n",
    "        - median(2, 2, 3, 4, 7, 9) = 3.5\n",
    "- 2. \n",
    "        - AB, AC, AD, BC, BD, CD\n",
    "- 3. a\n",
    "        - $p_1(A) = 0.6, p_1(B) = 0.4$\n",
    "        - $p_2(A) = 0.7, p_2(C) = 0.3$ \n",
    "        - $p_3(B) = 0.5, p_3(D) = 0.5$\n",
    "        - $P(A) = 0.6 * 0.7 = 0.42$\n",
    "        - $P(B) = 0.4 * 0.5 = 0.2$\n",
    "        - $P(C) = 0.3 * 0.5 = 0.15$\n",
    "- 3. b\n",
    "        - class A\n",
    "- 4. \n",
    "        - net 1: 1, 4, 3, 4, 8 - replacement evident in the `4`\n",
    "        - net 2: 7, 2, 6, 3, 4\n",
    "        - net 3: 10, 6, 7, 2, 7\n",
    "        - net 4: 8, 4, 5 ,3, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Boosting\n",
    "- in bagging, generation of base learning is left to chance and instability, i.e. random \n",
    "- in boosting, generation of base learners is not random and each new base learner is generated to correct the errors of the previous base learners\n",
    "- training data is randomly divided into subsets\n",
    "    - train the next models on the subsets that the previous models misclassified\n",
    "        - i.e. the next model is better at the things that the previous models were bad at\n",
    "- e.g. if you have 3 sets, X1, X2, and X3\n",
    "    - train model d1 on X1\n",
    "    - then test d1 on X2\n",
    "    - take the misclassified samples from X2 and train d2 on those and an equal number of correctly classified samples from X2\n",
    "    - then test d1 and d2 on X3\n",
    "    - the samples on which d1 and d2 disagree are the ones that d3 will be trained on\n",
    "    - this process continues until all samples are classified correctly or the maximum number of models is reached\n",
    "    - in this example, d1 is trained on X1, d2 is trained on d1's mistakes on X2, and d3 is trained on d1 and d2's mistakes on X3\n",
    "- output\n",
    "    - if d1 and d2's outputs agree, that is the response\n",
    "    - else, d3's output is the response\n",
    "- Can also be done recursively\n",
    "- disadvantages\n",
    "    - sample is divided into 3 sets and results in 3 models\n",
    "    - each later model is trained on successively smaller data sets\n",
    "### AdaBoost - Advanced Boosting\n",
    "- designed to improve on boosting\n",
    "- many methods of AdaBoost, here we discuss AdaBoost.M1\n",
    "    - <img src=\"images/adaboostM1.png\">\n",
    "    - all training samples are available for each model\n",
    "    - probability of each sample being chosen is calculated\n",
    "        - later models are more likely to have samples that were misclassified by previous models\n",
    "    - the later models should be good at the hard situations\n",
    "    - fusion achieved by weighted average\n",
    "        - weights are proportional to the model's accuracies on the training data\n",
    "            - i.e. if a model is 90% accurate, its weights will be 90% of the total weights\n",
    "- all models share the whole data pool, not limited to 3 models "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50205d7c5a47cc83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "38895ce3986650b5"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "db5878c08ccb534e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2dd29ff48a4401b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IC 24Oct23\n",
    "- X1 = {a, b, c, d, e}\n",
    "- X2 = {f, g, h, i, j}\n",
    "- X3 = {k, l, m, n, o}\n",
    "- In Iteration 1, that d1 misclassified {i, j} of X2\n",
    "1. d2 training sample  = {i, j, g, h}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f1a5d3248e768ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7e7f7b1a28854528"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
