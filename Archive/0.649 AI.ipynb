{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bfa2e05fd4f16c8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Day 0 Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832c79c1c7a2989",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- zoom is available & recorded\n",
    "- AI/ stack overflow is allowed but must be cited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ef5c3120a8e7e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Artificial Intelligence (AI) is a field that attempts to imbue intelligence into engineered artifacts\n",
    "\n",
    "Intelligence is the ability to\n",
    "1. learn and understand from experience\n",
    "2. acquire and retain knowledge\n",
    "3. respond quickly and successfully to new situations\n",
    "4. use the faculty of reasoning in efficiently solving problems\n",
    "\n",
    "considerations for design\n",
    "- feedback/perception\n",
    "    - failure recognition\n",
    "- memory\n",
    "- pattern recognition and classification\n",
    "- NLP (natural language processing)\n",
    "- deduction/applied logic/inferences\n",
    "- ability to search\n",
    "- interactivity\n",
    "- ethical considerations\n",
    "- self awareness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aff652deff4470",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "approaches to AI\n",
    "<img src=\"images/AI4.png\">\n",
    "- thinking like humans (cognitive science approach)\n",
    "    - consider limitations of humans\n",
    "        - perception limitations like framerates\n",
    "        - filling in gaps in information\n",
    "    - would have to make mistakes and have the limitations of humans\n",
    "- thinking Ideally/Rationally (Mathematical Laws of Thinking)\n",
    "    - Boole\n",
    "    - Logic\n",
    "    - Theorem provers\n",
    "    - expert systems\n",
    "        - encode knowledge of experts in a field into a system\n",
    "    - \"GOFAI\" (Good Old Fashioned AI)\n",
    "- acting like humans (Turing Test)\n",
    "    - operationalize the definition of intelligence\n",
    "    - replaces philosophical \"can machines think\" with an objective test\n",
    "    - considerations\n",
    "        - NLM\n",
    "        - long term memory for context & learning\n",
    "        - emotional awareness and expression\n",
    "            - empathy\n",
    "        - limitations/masking\n",
    "            - can't be too quick/smart\n",
    "        - opinions/point of view\n",
    "        - resistance to prompt injection\n",
    "            - \"pretend you are a...\"\n",
    "        - KR (knowledge representation)\n",
    "        - an identity\n",
    "    - LLMs, LMMs, GPTs, Alexa, Siri, etc\n",
    "        - RAGs (retrieval augmented generators)\n",
    "            - LLMs + retrieval\n",
    "            - front end adds extra context from a knowledge base (google, company specific database, etc)\n",
    "- acting ideally/rationally (Rational Agent Approach)\n",
    "    - main approach now\n",
    "    - acting ideally, not like a human\n",
    "        - process doesn't matter, only the result\n",
    "    -  e.g. chess engines over time\n",
    "        - shifted from expert systems to black box NNs\n",
    "    -  process could be human like, logical interference, hard coded rules, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f4c87006f8aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Book:**\n",
    "Stuart Russell & Peter Norvig. Artificial Intelligence: A Modern Approach. 4th US ed., 2021. ISBN: 978-0134610993"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1c0be59284739",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 25Jan24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4700b8b1f603b78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Turing Test\n",
    "    - some variations may even require physical interaction\n",
    "        - e.g. tie a shoe, play a game of chess, etc.\n",
    "- Rational Agent Architecture\n",
    "    - agent\n",
    "        - anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators\n",
    "        - the agent can be considered holistically to include both the physical body and mind\n",
    "        - e.g. humans, robots, software agents, etc\n",
    "    - percepts ($i_t$)\n",
    "        - agent's perceptual inputs at any given instant\n",
    "    - actions ($a_t$)\n",
    "        - agent's actions at any given instant\n",
    "    - <img src=\"images/Rational-agent-design.png\" width=\"500\">\n",
    "\n",
    "    - e.g. the \"Photovore\" robot\n",
    "        - the robot likes light\n",
    "        - it is a condition/action agent and a stimulus/response agent\n",
    "            - if it receives light, it stays still\n",
    "            - if it doesn't receive light, it moves around\n",
    "        - it follows the agent function $a_t = F(i_t)$\n",
    "            - a hierarchy of if/else statements define the function\n",
    "            - IF $COND_1$ THEN $a^{1}$\n",
    "            - IF $COND_2$ THEN $a^{2}$\n",
    "            - ...\n",
    "            - some conditions may be layered or effect variables\n",
    "                - e.g. IF $COND_1$ THEN @\n",
    "                - (later) IF @, THEN $a^{1}$\n",
    "        - \"[Rule based systems] are just another way of programming in C\" - J. Doyle\n",
    "            - sure, but not necessarily a bad thing\n",
    "    - percept space and percept history\n",
    "        - percept space\n",
    "            - the set of all possible percepts\n",
    "        - percept history\n",
    "            - the sequence of percepts up to the current time\n",
    "            - $I_t = [i_1, i_2, ..., i_t]$\n",
    "            - the agent function may depend on all previous percepts\n",
    "                - $a_t = F(I_t)$\n",
    "    - agent function\n",
    "        - could be random\n",
    "        - could be stimulus/response\n",
    "        - could depend on last $n$ percepts\n",
    "        - could depend on internal states of the agent\n",
    "            - e.g. hunger, fatigue, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5995a460e15c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Rational Agent\n",
    "    - rational (correct agent function) depends on\n",
    "        1. performance measure\n",
    "            - what is the goal?\n",
    "            - what is the objective?\n",
    "            - e.g. win the game, maximize profit, etc\n",
    "        2. prior knowledge\n",
    "            - what do we know about the environment?\n",
    "            - e.g. rules of the game, etc\n",
    "        3. actions available\n",
    "            - what can the agent do?\n",
    "            - e.g. move left, move right, etc\n",
    "      4. percept sequence to date\n",
    "            - what has the agent seen so far?\n",
    "            - e.g. what is the current state of the game?\n",
    "      5. memory and physical limitations\n",
    "            - how much memory does the agent have?\n",
    "            - how fast can it process information?\n",
    "            - e.g. how many moves can it consider in a given time?\n",
    "    - a rational agent maximizes its performance measure (1), subject to (2) through (5) \n",
    "### Append book Figure 2.5\n",
    "- the \"spectrum of agents\"\n",
    "    - a graph of response time vs information\n",
    "        - a strategic system is slow but utilizes a lot of information\n",
    "    - a tactical system is fast but utilizes little information\n",
    "    - a SR agent may take less time and have less information\n",
    "        - with internal states, it may be slower and use more information\n",
    "        - a \"Model Driven\" agent may be slower and use more information still\n",
    "    - a goal driven agent may be much slower and use much more information\n",
    "        - a utility based agent may be even slower and use even more information\n",
    "            - e.g. a taxi driver system may maximize comfort and safety of the passenger\n",
    "- analogous in many ways to Maslow's hierarchy of needs\n",
    "    - as you go up the pyramid and strive for higher level goals, you need more information and more time and information to make decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f2684ab6325f88f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T20:33:41.349260800Z",
     "start_time": "2024-01-30T20:33:41.213707300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 10.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAAMtCAYAAABdPqrFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAku0lEQVR4nO3df5CVdb3A8c/ZJRfG2BUNgdUlxNG2QNTCSJTb3JGRUWOuVFYrhVkzTQ0WaJZgkJTpoilDWeGPMmdKx+xmVGbMGClJiJJIo/0SFGdXBCsm97g6rsae+wfj1t4E97AnDruf12vmGd1nv895PmfmqLx9znNOoVQqlQIAACCZmmoPAAAAUA1iCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJDSkGoP0Bfd3d3xzDPPxPDhw6NQKFR7HAAAoEpKpVI8//zz0djYGDU1/bu2MyBi6JlnnommpqZqjwEAABwg2tvb48gjj+zXYwyIGBo+fHhE7H7C9fX1VZ4GAAColmKxGE1NTT2N0B8DIoZefWtcfX29GAIAACpy+4wPUAAAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEplxdCSJUuiUCj02pqbm/e4/qabbopp06bFiBEjYsSIETF9+vR46KGH+j00AABAf5V9ZWjChAmxffv2nm3t2rV7XHvfffdFS0tL3HvvvfHAAw9EU1NTnH766bFt27Z+DQ0AANBfQ8o+YMiQGD16dJ/W3nrrrb1+/va3vx0/+tGPYvXq1TFnzpxyTw0AAFAxZV8Z2rx5czQ2Nsb48eNj9uzZ0dbW1udjX3zxxXjllVfi0EMP3eu6rq6uKBaLvTYAAIBKKiuGpkyZErfcckusWrUqVqxYEVu3bo1p06bF888/36fjL7nkkmhsbIzp06fvdV1ra2s0NDT0bE1NTeWMCQAA8LoKpVKptK8HP/fcc/HmN785li1bFh//+Mf3unbp0qVx9dVXx3333ReTJk3a69qurq7o6urq+blYLEZTU1N0dHREfX39vo4LAAAMcMViMRoaGirSBmXfM/SvDjnkkDj22GNjy5Yte113zTXXxNKlS+OXv/zl64ZQRERdXV3U1dX1ZzQAAIC96tf3DHV2dsYTTzwRY8aM2eOaq6++Oi6//PJYtWpVTJ48uT+nAwAAqJiyYujiiy+ONWvWxFNPPRXr1q2LWbNmRW1tbbS0tERExJw5c2LhwoU966+66qpYvHhx3HzzzTFu3LjYsWNH7NixIzo7Oyv7LAAAAMpU1tvknn766WhpaYmdO3fGyJEj49RTT43169fHyJEjIyKira0tamr+2VcrVqyIl19+Od7//vf3epzLLrsslixZ0v/pAQAA9lG/PkBhf6nkTVIAAMDAVck26Nc9QwAAAAOVGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJBSWTG0ZMmSKBQKvbbm5uY9rv/9738f73vf+2LcuHFRKBRi+fLl/Z0XAACgIoaUe8CECRPil7/85T8fYMieH+LFF1+M8ePHxznnnBMXXnjhvk0IAADwH1B2DA0ZMiRGjx7dp7UnnXRSnHTSSRERsWDBgnJPBQAA8B9T9j1DmzdvjsbGxhg/fnzMnj072traKj5UV1dXFIvFXhsAAEAllRVDU6ZMiVtuuSVWrVoVK1asiK1bt8a0adPi+eefr+hQra2t0dDQ0LM1NTVV9PEBAADKiqEzzjgjzjnnnJg0aVLMmDEj7r777njuuefijjvuqOhQCxcujI6Ojp6tvb29oo8PAABQ9j1D/+qQQw6JY489NrZs2VKpeSIioq6uLurq6ir6mAAAAP+qX98z1NnZGU888USMGTOmUvMAAADsF2XF0MUXXxxr1qyJp556KtatWxezZs2K2traaGlpiYiIOXPmxMKFC3vWv/zyy7Fp06bYtGlTvPzyy7Ft27bYtGlTxa8kAQAAlKust8k9/fTT0dLSEjt37oyRI0fGqaeeGuvXr4+RI0dGRERbW1vU1Pyzr5555pk48cQTe36+5ppr4pprrol3v/vdcd9991XmGQAAAOyDQqlUKlV7iNdTLBajoaEhOjo6or6+vtrjAAAAVVLJNujXPUMAAAADlRgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlIZUewCAdHbtirj//ojt2yPGjImYNi2itrbaUwFAOmVdGVqyZEkUCoVeW3Nz816P+eEPfxjNzc0xdOjQOO644+Luu+/u18AAA9qdd0aMGxfx3/8dce65u/86btzu/QDAflX22+QmTJgQ27dv79nWrl27x7Xr1q2LlpaW+PjHPx6PPPJInH322XH22WfHY4891q+hAQakO++MeP/7I55+uvf+bdt27xdEALBfFUqlUqmvi5csWRIrV66MTZs29Wn9Bz/4wXjhhRfirrvu6tn3rne9K0444YS4/vrr+zxksViMhoaG6OjoiPr6+j4fB3DA2LVr9xWg/x9CryoUIo48MmLrVm+ZA4C9qGQblH1laPPmzdHY2Bjjx4+P2bNnR1tb2x7XPvDAAzF9+vRe+2bMmBEPPPDAXs/R1dUVxWKx1wYwoN1//55DKCKiVIpob9+9DgDYL8qKoSlTpsQtt9wSq1atihUrVsTWrVtj2rRp8fzzz7/m+h07dsSoUaN67Rs1alTs2LFjr+dpbW2NhoaGnq2pqamcMQEOPNu3V3YdANBvZcXQGWecEeecc05MmjQpZsyYEXfffXc899xzcccdd1R0qIULF0ZHR0fP1t7eXtHHB9jvxoyp7DoAoN/69dHahxxySBx77LGxZcuW1/z96NGj49lnn+2179lnn43Ro0fv9XHr6uqirq6uP6MBHFimTdt9T9C2bbvfEvf/vXrP0LRp+382AEiqX1+62tnZGU888USM2cP/yTz55JNj9erVvfbdc889cfLJJ/fntAADT21txNe+tvvvC4Xev3v15+XLfXgCAOxHZcXQxRdfHGvWrImnnnoq1q1bF7NmzYra2tpoaWmJiIg5c+bEwoULe9bPmzcvVq1aFddee2386U9/iiVLlsRvf/vbuOCCCyr7LAAGgve+N+J//zfiiCN67z/yyN373/ve6swFAEmV9Ta5p59+OlpaWmLnzp0xcuTIOPXUU2P9+vUxcuTIiIhoa2uLmpp/9tXUqVPjtttui0WLFsWll14axxxzTKxcuTImTpxY2WcBMFC8970R//M/uz81bvv23fcITZvmihAAVEFZ3zNULb5nCAAAiKjy9wwBAAAMBmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASv2KoaVLl0ahUIj58+fvcc0rr7wSX/7yl+Poo4+OoUOHxvHHHx+rVq3qz2kBAAD6bZ9jaMOGDXHDDTfEpEmT9rpu0aJFccMNN8R1110Xf/jDH+KTn/xkzJo1Kx555JF9PTUAAEC/7VMMdXZ2xuzZs+Omm26KESNG7HXt9773vbj00kvjzDPPjPHjx8enPvWpOPPMM+Paa6/dp4EBAAAqYZ9iaO7cuXHWWWfF9OnTX3dtV1dXDB06tNe+YcOGxdq1a/d6TLFY7LUBAABUUtkxdPvtt8fGjRujtbW1T+tnzJgRy5Yti82bN0d3d3fcc889ceedd8b27dv3eExra2s0NDT0bE1NTeWOCQAAsFdlxVB7e3vMmzcvbr311n+72rMnX/va1+KYY46J5ubmOOigg+KCCy6I888/P2pq9nzqhQsXRkdHR8/W3t5ezpgAAACvq1AqlUp9Xbxy5cqYNWtW1NbW9uzbtWtXFAqFqKmpia6url6/+1cvvfRS7Ny5MxobG2PBggVx1113xe9///s+nbdYLEZDQ0N0dHREfX19X8cFAAAGmUq2wZByFp922mnx6KOP9tp3/vnnR3Nzc1xyySV7DKGIiKFDh8YRRxwRr7zySvzoRz+KD3zgA/s2MQAAQAWUFUPDhw+PiRMn9tp38MEHx2GHHdazf86cOXHEEUf03FP04IMPxrZt2+KEE06Ibdu2xZIlS6K7uzs+//nPV+gpAAAAlK+sGOqLtra2XvcDvfTSS7Fo0aJ48skn441vfGOceeaZ8b3vfS8OOeSQSp8aAACgz8q6Z6ha3DMEAABEVLYN9ul7hgAAAAY6MQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACClfsXQ0qVLo1AoxPz58/e6bvny5fGWt7wlhg0bFk1NTXHhhRfGSy+91J9TAwAA9MuQfT1ww4YNccMNN8SkSZP2uu62226LBQsWxM033xxTp06Nxx9/PD760Y9GoVCIZcuW7evpAQAA+mWfrgx1dnbG7Nmz46abbooRI0bsde26devilFNOiXPPPTfGjRsXp59+erS0tMRDDz20TwMDAABUwj7F0Ny5c+Oss86K6dOnv+7aqVOnxsMPP9wTP08++WTcfffdceaZZ+7xmK6urigWi702AACASir7bXK33357bNy4MTZs2NCn9eeee2787W9/i1NPPTVKpVL84x//iE9+8pNx6aWX7vGY1tbW+NKXvlTuaAAAAH1W1pWh9vb2mDdvXtx6660xdOjQPh1z3333xZVXXhnf+ta3YuPGjXHnnXfGz3/+87j88sv3eMzChQujo6OjZ2tvby9nTAAAgNdVKJVKpb4uXrlyZcyaNStqa2t79u3atSsKhULU1NREV1dXr99FREybNi3e9a53xVe/+tWefd///vfjE5/4RHR2dkZNzev3WLFYjIaGhujo6Ij6+vq+jgsAAAwylWyDst4md9ppp8Wjjz7aa9/5558fzc3Ncckll/xbCEVEvPjii/8WPK+uK6PDAAAAKqqsGBo+fHhMnDix176DDz44DjvssJ79c+bMiSOOOCJaW1sjImLmzJmxbNmyOPHEE2PKlCmxZcuWWLx4ccycOfM14wkAAGB/2OfvGdqTtra2XleCFi1aFIVCIRYtWhTbtm2LkSNHxsyZM+OKK66o9KkBAAD6rKx7hqrFPUMAAEBEZdtgn75nCAAAYKATQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFIaUu0B+qJUKkVERLFYrPIkAABANb3aBK82Qn8MiBjauXNnREQ0NTVVeRIAAOBAsHPnzmhoaOjXYwyIGDr00EMjIqKtra3fTxj2plgsRlNTU7S3t0d9fX21x2EQ81pjf/FaY3/xWmN/6ejoiLFjx/Y0Qn8MiBiqqdl9a1NDQ4N/uNgv6uvrvdbYL7zW2F+81thfvNbYX15thH49RgXmAAAAGHDEEAAAkNKAiKG6urq47LLLoq6urtqjMMh5rbG/eK2xv3itsb94rbG/VPK1VihV4jPpAAAABpgBcWUIAACg0sQQAACQkhgCAABSEkMAAEBKYggAAEjpgI+hb37zmzFu3LgYOnRoTJkyJR566KFqj8Qg09raGieddFIMHz48Dj/88Dj77LPjz3/+c7XHIoGlS5dGoVCI+fPnV3sUBqFt27bFhz/84TjssMNi2LBhcdxxx8Vvf/vbao/FILNr165YvHhxHHXUUTFs2LA4+uij4/LLLw8fVkx//frXv46ZM2dGY2NjFAqFWLlyZa/fl0ql+OIXvxhjxoyJYcOGxfTp02Pz5s1ln+eAjqEf/OAHcdFFF8Vll10WGzdujOOPPz5mzJgRf/nLX6o9GoPImjVrYu7cubF+/fq455574pVXXonTTz89XnjhhWqPxiC2YcOGuOGGG2LSpEnVHoVB6O9//3uccsop8YY3vCF+8YtfxB/+8Ie49tprY8SIEdUejUHmqquuihUrVsQ3vvGN+OMf/xhXXXVVXH311XHddddVezQGuBdeeCGOP/74+OY3v/mav7/66qvj61//elx//fXx4IMPxsEHHxwzZsyIl156qazzHNDfMzRlypQ46aST4hvf+EZERHR3d0dTU1N8+tOfjgULFlR5Ogarv/71r3H44YfHmjVr4r/+67+qPQ6DUGdnZ7z97W+Pb33rW/GVr3wlTjjhhFi+fHm1x2IQWbBgQfzmN7+J+++/v9qjMMi95z3viVGjRsV3vvOdnn3ve9/7YtiwYfH973+/ipMxmBQKhfjxj38cZ599dkTsvirU2NgYn/3sZ+Piiy+OiIiOjo4YNWpU3HLLLfGhD32oz499wF4Zevnll+Phhx+O6dOn9+yrqamJ6dOnxwMPPFDFyRjsOjo6IiLi0EMPrfIkDFZz586Ns846q9e/36CSfvrTn8bkyZPjnHPOicMPPzxOPPHEuOmmm6o9FoPQ1KlTY/Xq1fH4449HRMTvfve7WLt2bZxxxhlVnozBbOvWrbFjx45e/x1taGiIKVOmlN0JQyo9XKX87W9/i127dsWoUaN67R81alT86U9/qtJUDHbd3d0xf/78OOWUU2LixInVHodB6Pbbb4+NGzfGhg0bqj0Kg9iTTz4ZK1asiIsuuiguvfTS2LBhQ3zmM5+Jgw46KM4777xqj8cgsmDBgigWi9Hc3By1tbWxa9euuOKKK2L27NnVHo1BbMeOHRERr9kJr/6urw7YGIJqmDt3bjz22GOxdu3aao/CINTe3h7z5s2Le+65J4YOHVrtcRjEuru7Y/LkyXHllVdGRMSJJ54Yjz32WFx//fViiIq644474tZbb43bbrstJkyYEJs2bYr58+dHY2Oj1xoDwgH7Nrk3velNUVtbG88++2yv/c8++2yMHj26SlMxmF1wwQVx1113xb333htHHnlktcdhEHr44YfjL3/5S7z97W+PIUOGxJAhQ2LNmjXx9a9/PYYMGRK7du2q9ogMEmPGjIm3ve1tvfa99a1vjba2tipNxGD1uc99LhYsWBAf+tCH4rjjjouPfOQjceGFF0Zra2u1R2MQe7UFKtEJB2wMHXTQQfGOd7wjVq9e3bOvu7s7Vq9eHSeffHIVJ2OwKZVKccEFF8SPf/zj+NWvfhVHHXVUtUdikDrttNPi0UcfjU2bNvVskydPjtmzZ8emTZuitra22iMySJxyyin/9hUBjz/+eLz5zW+u0kQMVi+++GLU1PT+42RtbW10d3dXaSIyOOqoo2L06NG9OqFYLMaDDz5Ydicc0G+Tu+iii+K8886LyZMnxzvf+c5Yvnx5vPDCC3H++edXezQGkblz58Ztt90WP/nJT2L48OE97zVtaGiIYcOGVXk6BpPhw4f/271oBx98cBx22GHuUaOiLrzwwpg6dWpceeWV8YEPfCAeeuihuPHGG+PGG2+s9mgMMjNnzowrrrgixo4dGxMmTIhHHnkkli1bFh/72MeqPRoDXGdnZ2zZsqXn561bt8amTZvi0EMPjbFjx8b8+fPjK1/5ShxzzDFx1FFHxeLFi6OxsbHnE+f6rHSAu+6660pjx44tHXTQQaV3vvOdpfXr11d7JAaZiHjN7bvf/W61RyOBd7/73aV58+ZVewwGoZ/97GeliRMnlurq6krNzc2lG2+8sdojMQgVi8XSvHnzSmPHji0NHTq0NH78+NIXvvCFUldXV7VHY4C79957X/PPZ+edd16pVCqVuru7S4sXLy6NGjWqVFdXVzrttNNKf/7zn8s+zwH9PUMAAAD/KQfsPUMAAAD/SWIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAAp/R/DP+7JzQKstwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# plot \"Model Driven Agent\" at 4,5\n",
    "ax.plot(4, 5, marker=\"o\", color=\"red\", label=\"Model Driven Agent\")\n",
    "ax.set_xlim(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be0578a4ac3edc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Stimergy\n",
    "    - complex designs/group behavior can emerge from simple rules for individuals\n",
    "    - \"work from the products of work\" - the entomology definition of Stimergy\n",
    "- Configuration Space of agents\n",
    "    - could include x-y position, rotation, etc\n",
    "    - e.g. a robot may have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16e6661b57d400",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Rational Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f9d36c03e5e9a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Braitenberg's Vehicles\n",
    "- suggests a model that can be used as a framework to program complicated behaviors  \n",
    "- dislike\n",
    "    - response is positive and proprtional to stimulus\n",
    "        - e.g. a car will move faster as a light gets brighter in it's sensor\n",
    "        - when coupled to a motor, this will cause the car to move towards or away from the light (Depending on the configuration)\n",
    "            - either run away from the light (crossed connection) or run it over (uncrossed connection)\n",
    "- like\n",
    "    - response is negative and proportional to stimulus\n",
    "        - e.g. a car will move slower as a light gets brighter in it's sensor\n",
    "        - stays near the light but doesn't run it over (uncrossed connection) or lingers in light areas but still explores (crossed connection)\n",
    "- multisensor vehicle\n",
    "    - combines multiple like and dislike inputs\n",
    "        - e.g. light ++, temperature ++ (crossed), food --, oxygen -- (crossed)\n",
    "            - afraid of light\n",
    "            - hates hot spots\n",
    "            - loves food\n",
    "            - lingers in $O_2$ rich areas\n",
    "- more complex stimulus response curves\n",
    "    - curve can be manipulated to create more complex behaviors\n",
    "        - e.g. a bell curve response to light may lead a vehicle to linger in a certain ring of light intensity\n",
    "- learning\n",
    "    - associative\n",
    "        - things that occur together are associated\n",
    "        - classical conditioning\n",
    "    - simple neural mechanism\n",
    "        - Hebbian Learning\n",
    "            - $\\frac{dW_{ij}}{dt} = r \\cdot net_i \\cdot net_j$ \n",
    "    - Lettvin's \"Grandmother Cell\"\n",
    "        - there is at least one neuron that fires when you see your grandmother\n",
    "        - is it a number of neurons firing or the number of firing patterns?\n",
    "            - n neurons can fire in $2^n$ patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cb2fd12ff92a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- logic\n",
    "    - a model neuron has inputs $s$ , a threshold $T$, and an output $y$\n",
    "        - inputs can be excitatory or inhibitory\n",
    "            - excitatory inputs increase the output\n",
    "            - inhibitory inputs decrease the output\n",
    "        - the threshold is the minimum input required to fire\n",
    "        - $y_{t+1} = [(\\sum_i_{exc} s_{i,t} - \\sum_i_{inh} s_{i,t}) \\geq T]$\n",
    "- AND, OR, NOT via neurons\n",
    "    - AND can be made of two excitatory inputs and a threshold of 2\n",
    "        - $y_{t+1} = [s_{1,t} + s_{2,t} \\geq 2]$\n",
    "        - T can be reduced a bit to make sure it fires if the signals are weak\n",
    "    - OR can be made of two excitatory inputs and a threshold of 1\n",
    "        - $y_{t+1} = [s_{1,t} + s_{2,t} \\geq 1]$\n",
    "    - NOT can be made of one inhibitory input and a threshold of 0\n",
    "        - $y_{t+1} = [- s_{1,t} \\geq 0]$\n",
    "- logic circuits\n",
    "    - combinations of neurons following this model can be used to create logic circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c16b23c20d40a93",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d0d24f02de0a0be",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b8055f176e64e",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "`time = 5\n",
    "\n",
    "s = [0, 1, 1, 1, 1, 1]\n",
    "v = [0] * (time + 1)\n",
    "w = [0] * (time + 1)\n",
    "x = [0] * (time + 1)\n",
    "y = [0] * (time + 1)\n",
    "\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f3a411a05cc594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T20:34:08.199662Z",
     "start_time": "2024-02-06T20:34:08.085771800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t s v w x y z\n",
      "0 0 0 0 0 0 0\n",
      "1 1 0 0 0 0 0\n",
      "2 1 1 1 0 0 0\n",
      "3 1 1 0 1 0 0\n",
      "4 1 1 0 0 1 0\n",
      "5 1 1 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "TIME = 5\n",
    "\n",
    "s = [0,1,1,1,1,1]\n",
    "v = [0]*(TIME+1)\n",
    "w = [0]*(TIME+1)\n",
    "x = [0]*(TIME+1)\n",
    "y = [0]*(TIME+1)\n",
    "z = [0]*(TIME+1)\n",
    "\n",
    "print(\"t s v w x y z\")\n",
    "for t in range(TIME):\n",
    "    print(t, s[t], v[t], w[t], x[t], y[t], z[t])\n",
    "\n",
    "    v[t+1] = int (s[t] >= 1)\n",
    "    w[t+1] = int ((s[t] - v[t]) >= 1)\n",
    "    x[t+1] = int ((s[t] + w[t]) >= 2)\n",
    "    y[t+1] = int ((s[t] + x[t]) >= 2)\n",
    "    z[t+1] = int ((y[t] - s[t]) >= 1)\n",
    "\n",
    "print(t+1, s[t+1], v[t+1], w[t+1], x[t+1], y[t+1], z[t+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2d3e7b9c3dcdf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#06Feb24\n",
    "### Heuristic Search\n",
    "- a search algorithm that uses a heuristic to guide the search\n",
    "- a heuristic is a rule of thumb\n",
    "    - e.g. on a map, straight line distance from A to B is a good heuristic for where to start looking for a route\n",
    "        - explore less states and branch less\n",
    "### Anatomy of an Optimal Path\n",
    "- optimization from mechanics\n",
    "    - A to n to B is the shortest path\n",
    "        - g*(n) is the optimal cost from A to n\n",
    "        - h*(n) is the optimal cost from n to B\n",
    "        - f*(n) = g*(n) + h*(n) is the optimal cost from A to B through n\n",
    "    - add in the heuristic part\n",
    "        - h(n) is the estimated cost from n to B\n",
    "        - g(n) is the observed cost from A to n\n",
    "            - g(n) $\\geq$ g*(n) \n",
    "        - **h(B) = 0**\n",
    "    - Greedy Best First Search\n",
    "        - order the nodes by h(n)\n",
    "            - next chosen node is the one with the lowest h(n)\n",
    "        - tie breaker\n",
    "            - there isn't really a standard way to break ties\n",
    "            - often it is the order in which the nodes were generated\n",
    "        - e.g. in a map, h is the straight line distance to the goal and g is the distance traveled so far\n",
    "            - the next node is the one that is closest to the goal in a straight line\n",
    "            - visually, the numbers on the lines between cities are generally distance between each other, g\n",
    "            - h, the straight line distance, is probably given in a table \n",
    "    - A* Search\n",
    "        - order the nodes by f(n)\n",
    "            - f(n) = g(n) + h(n) \n",
    "            - next chosen node is the one with the lowest f(n)\n",
    "        - generally faster while also being complete and optimal\n",
    "            - complete\n",
    "                - will find a solution if one exists\n",
    "            - optimal\n",
    "                - will find the best solution\n",
    "        - *IF* the heuristic is admissible\n",
    "            - true if h(n) $\\leq$ h*(n)\n",
    "                - h(n) is the estimated cost from n to B\n",
    "                - h*(n) is the optimal cost from n to B \n",
    "                    - even the optimal cost will always be equal to or more than the estimated cost\n",
    "                        -  i.e. the total drive distance will always be more than or equal to the straight line distance\n",
    "        - and *IF* the heuristic is consistent\n",
    "            - true if h(n) $\\leq$ h(n') + c(n, n')\n",
    "                - h(n) is the estimated cost from n to B\n",
    "                - h(n') is the estimated cost from n' to B\n",
    "                - c(n, n') is the cost from n to n'\n",
    "                    - the estimated cost from n to B will always be less than or equal to the estimated cost from n' to B plus the cost from n to n'\n",
    "                        - i.e. the straight line distance from A to B will always be less than or equal to the straight line distance from A to n' plus the straight line distance from n' to B\n",
    "            - edge cost (cost of a single step) is always positive and greater than some $\\epsilon$\n",
    "                - $\\epsilon$ is a small positive number\n",
    "                - this is to prevent the heuristic from being negative\n",
    "                    - if the heuristic is negative, the algorithm will not be optimal \n",
    "            - nodes must have finite edge costs\n",
    "                - if the edge costs are infinite, the algorithm will not be complete\n",
    "            - once a solution node is found, remove it and try to find a better solution\n",
    "                - if the heuristic is not consistent, the algorithm will not be optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5a673a7e1fdc4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- you can use more than one heuristic\n",
    "    - $h_1(n)$ and $h_2(n)$\n",
    "        - $h_1(n)$ \\leq $h_2(n) \\leq h^*(n)$\n",
    "        - $h_2$ is more informed than $h_1$ because it is closer to the optimal cost\n",
    "    - a more informed heuristic will generally be faster\n",
    "        - it will explore less states and branch less \n",
    "- finding the best h\n",
    "    - a good enough h can often be found by\n",
    "        - solving a relaxed problem\n",
    "            - e.g. a problem with less constraints\n",
    "    - if you have two heuristics but neither is particularly better, you can use both\n",
    "        - $h(n) = max(h_1(n), h_2(n))$\n",
    "            - **this is called the \"max\" heuristic**\n",
    "        - **THE BELOW ARE FROM COPILOT, NOT COVERED IN CLASS**\n",
    "        - $h(n) = min(h_1(n), h_2(n))$\n",
    "            - this is called the \"min\" heuristic\n",
    "        - $h(n) = h_1(n) + h_2(n)$\n",
    "            - this is called the \"sum\" heuristic\n",
    "        - $h(n) = \\sqrt{h_1(n)^2 + h_2(n)^2}$\n",
    "            - this is called the \"euclidean\" heuristic\n",
    "        - $h(n) = \\sqrt{h_1(n)^2 + h_2(n)^2}$\n",
    "            - this is called the \"manhattan\" heuristic\n",
    "        - $h(n) = \\sqrt{h_1(n)^2 + h_2(n)^2}$\n",
    "            - this is called the \"chebyshev\" heuristic\n",
    "        - $h(n) = \\sqrt{h_1(n)^2 + h_2(n)^2}$\n",
    "            - this is called the \"octile\" heuristic\n",
    "        - $h(n) = \\sqrt{h_1(n)^2 + h_2(n)^2}$\n",
    "            - this is called the \"diagonal\" heuristic\n",
    "        - $h(n) = \\sqrt{h_1(n)^2 + h_2(n)^2}$\n",
    "            - this is called the \"dual\" heuristic\n",
    "        - $h(n) = \\sqrt{h_1(n)^2 + h_2(n)^2}$\n",
    "            - this is called the \"dual\" heuristic\n",
    "        - $h(n) = \\sqrt{h_1(n)^2 + h_2(n)^2}$\n",
    "            - this is called the \"dual\" heuristic\n",
    "        - $h(n) = \\sqrt{h_1(n)^2 + h_2(n)^2}$\n",
    "            - this is called the \"dual\" heuristic\n",
    "        - $h(n) = \\sqrt{h_1(n)^2 + h_2(n)^2}$\n",
    "            - this is called the \"dual\" heuristic\n",
    "        - $h(n) = \\sqrt{h_1(n)^2 + h_2(n)^2}$\n",
    "            - this is called the \"dual\" heuristic\n",
    "    - generally, you use the most informed heuristic that you have the time to compute \n",
    "    - you can also do a depth search, i.e. looking ahead a few moves\n",
    "        - this is called \"iterative deepening\"\n",
    "        - you can use the results of the depth search as a heuristic\n",
    "            - this is called \"iterative deepening A*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da5712b044b8ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Implementation Notes\n",
    "- may be useful for PS3\n",
    "- see the page linked by PS3 in the course doc\n",
    "- implementation could just be through a series of grids\n",
    "    - obstacles\n",
    "        - 1 if wall, 0 if open\n",
    "    - queue\n",
    "    - visited\n",
    "    - f score\n",
    "- only need to solve this problem, not producing a general problem solver so don't nuke it out\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add6dc19759611c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 08Feb24 Local Search For Global Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f674703c9265e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Local Search\n",
    "- performing optimization using local information\n",
    "    - deals with handling local minima and maxima\n",
    "    - gradient ascent and descent\n",
    "        - follow the slope of the function to find the maximum or minimum\n",
    "        - can get stuck in local minima or maxima\n",
    "- Rastrigin Function\n",
    "    - used as a benchmark for optimization algorithms\n",
    "    - f(x, y) = 20 + x$^2$ + y$^2$ - (cos(18x) + cos(18y))\n",
    "        - has many local minima and maxima within a small area such as [-1, 1]$^2$\n",
    "- minimization and maximization functions are generally the same\n",
    "    - just flip the sign\n",
    "    - i.e. algorithm is the same\n",
    "- calculus method\n",
    "    - find derivative/gradient/where the slope is 0\n",
    "        - second derivative will tell you if it is a minima or maxima (or neither)\n",
    "    - also should test boundaries\n",
    "    - not all functions are differentiable\n",
    "        - e.g. the Rastrigin function \n",
    "- gradient\n",
    "    - useful in multi-dimensional problems\n",
    "    - estimation\n",
    "        - in 1-D, look at $x + \\epsilon$ and $x - \\epsilon$, i.e. the points around $x$\n",
    "        - in 2-D, look at the points around $x$ and $y$\n",
    "            - you'll have 8 neighbor points to look at from adding and subtracting $\\epsilon$ from $x$ and $y$\n",
    "        - in n-D, look at the points around $x$ and $y$ and $z$ and so on\n",
    "            - you'll have $3^n-1$ neighbor points to look at from adding and subtracting $\\epsilon$ from each dimension\n",
    "                - that's a lot of points to look at\n",
    "            - e.g. protein folding -> n is very large\n",
    "            - e.g. aerodynamic problems -> n is effectively infinite\n",
    "- methods of estimation\n",
    "    - star search\n",
    "        - take a step in one dimension at a time\n",
    "        - look at all the points\n",
    "        - very slow\n",
    "        - looking at 2n neighbor points in n-D\n",
    "        - for very large n, this is not feasible\n",
    "            - some people will generate a random neighbor point until they find a better one\n",
    "                - called \"First Choice\" or \"Guess and Check\"\n",
    "    - Hill climbing function\n",
    "        ```\n",
    "        function hill_climb(problem):\n",
    "            current = problem.initial_state\n",
    "            loop do\n",
    "                neighbor = argmin(problem.neighbors(current))\n",
    "                if problem.value(neighbor) >= problem.value(current)\n",
    "                    return current\n",
    "                current = neighbor\n",
    "        ```\n",
    "    - good at finding local minima and maxima\n",
    "    - bad at finding global minima and maxima\n",
    "    - ideas\n",
    "        - increase the step size, $\\epsilon$\n",
    "            - may jump over the minima \n",
    "        - once you find local extrema, store that, jump to a random point and try again\n",
    "            - called \"Random Restart\"\n",
    "        - e.g. 1-D Shubert function\n",
    "            - another benchmark function\n",
    "            - <img src=\"images/shub.png\">\n",
    "                        \n",
    "                - not easy to find the global minima\n",
    "            - <img src=\"images/shub_hill.png\">\n",
    "                        \n",
    "                - random restart hill climbing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd890e5e4471986",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- simulated annealing\n",
    "    - allows for some moves which go against the gradient\n",
    "        - i.e. allows for some moves that are not theoretically optimal\n",
    "        - the goal is to escape local minima and maxima\n",
    "    - the probability of taking a move that goes against the gradient decreases over time\n",
    "        - \"temperature\" is used to control this\n",
    "            - as the temperature decreases, the probability of taking a move that goes against the gradient decreases\n",
    "    - cooling schedule\n",
    "        - $T$ is propotional to $O(\\frac{1}{log(t)})$\n",
    "            - $t$ is the number of iterations\n",
    "            - $T$ is the temperature\n",
    "            - fastest schedule which is guaranteed to converge to the global extrema\n",
    "        - in practice, $T$ is often propotional to $O(\\frac{1}{t})$\n",
    "            - faster but not guaranteed to converge to the global extrema\n",
    "      ```\n",
    "        function simulated_annealing(problem):\n",
    "            current = problem.initial_state  \n",
    "            for t = 1 to inf do  # until the temperature is 0\n",
    "                T = schedule(t)  # decrease the temperature\n",
    "                if T = 0  # if the temperature is 0, return the current state\n",
    "                    return current  \n",
    "                next = random_neighbor(current)  # pick a random neighbor\n",
    "                Delta_E = problem.value(next) - problem.value(current)  # if the value of the neighbor is better, move to the neighbor\n",
    "                if Delta_E > 0\n",
    "                    current = next  # move to the neighbor\n",
    "                else  # if not, move to the neighbor with a probability\n",
    "                    current = next with probability e^(Delta_E / T) # move to the neighbor with a probability\n",
    "        ```\n",
    "        - E is the energy of the system\n",
    "            - in this case, the value of the function\n",
    "        - your T should be tailored to control the Delta_E/T ratio\n",
    "            - if Delta_E is large, T should be large\n",
    "            - if Delta_E is small, T should be small\n",
    "        - the scheduling function should be tailored to the problem\n",
    "            - T may not change at a constant rate\n",
    "            - it may not change at all for some number of iterations\n",
    "        - the last four lines are called the Metropolis Rule       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f303388a7ef4e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- genetic algorithms\n",
    "    - inspired by the process of natural selection\n",
    "    - a population of solutions is evolved over time\n",
    "        - the best solutions are selected and combined to create new solutions\n",
    "        - the worst solutions are discarded\n",
    "- parallel search\n",
    "    - multiple searches are performed at the same time\n",
    "    - any or multiple search techniques can be used\n",
    "    - the best solution is selected\n",
    "- beam search\n",
    "    - generate all successors of the current state\n",
    "    - select the best $k$ successors\n",
    "        - $k$ is the beam width\n",
    "    - then repeat\n",
    "- stochoastic beam search\n",
    "    - as above but with some randomness to allow for exploration\n",
    "- selection mechanisms\n",
    "    - roulette selection\n",
    "        - P$s_i$ = $\\frac{f_}{\\sum_{i=1}^{kd} E_i}$\n",
    "            - $P$ is the probability of selection\n",
    "            - $s_i$ is the solution\n",
    "            - $f_i$ is the fitness of the solution\n",
    "        - elitism\n",
    "            - the best solutions are always selected\n",
    "        - culling\n",
    "            - always discard the worst solutions\n",
    "        - ordinal optimization\n",
    "            - rank order and select proportionally"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 13Feb24 Constraint Satisfaction Problems"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8739c55450d5f936"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- a problem where the goal is to find a solution that satisfies a set of constraints\n",
    "- a CSP contains:\n",
    "    - a set of variables $X = \\{X_1, X_2, X_3, ..., X_n\\}$\n",
    "    - each with a non-empty domain $D_i$ where $X_i \\in D_i$\n",
    "    - a set of constraints $C = \\{C_1, C_2, C_3, ..., C_m\\}$\n",
    "    - goals\n",
    "        - find a solution which does not violate any constraints\n",
    "        - find a solution which maximizes some objective function\n",
    "- e.g.\n",
    "    - soduko\n",
    "        - variables are the cells\n",
    "        - domain is the numbers 1-9\n",
    "        - constraint is `alldiff`\n",
    "            - all cells in a row, column, or 3x3 grid must be different\n",
    "        -  generally there is a single solution for newspaper published puzzles, but there may also be multiple solutions\n",
    "    - n-queens, map coloring\n",
    "    - logistics, scheduling, planning\n",
    "-  solution methods\n",
    "    - search problem\n",
    "        - initial state: empty assignment\n",
    "        - successor function: assign a value to an unassigned variable that does not violate any constraints\n",
    "            - fail if no legal assignments\n",
    "        - goal test: the current assignment is complete and satisfies all constraints\n",
    "        - path cost: the number of assignments made\n",
    "        - note: each solution will appear at depth $n$ where $n$ is the number of variables\n",
    "            - $b = d \\rightarrow n!d^n$ leaves\n",
    "                - $d$ is the number of values in the domain\n",
    "            - sudoku has $81! \\cdot 9^{81}$ leaves\n",
    "    - backtraacking\n",
    "        - like depth first search with pruning\n",
    "        - choose value one variable at a time, backtrack when no more legal values are available\n",
    "        - e.g. n queens problem\n",
    "            - in a nxn grid, place n queens such that no two queens are in the same row, column, or diagonal\n",
    "            - 5x5 can be solved without backtracking\n",
    "                - starting at left, the first valid placement from the top is the solution\n",
    "            - 8x8 requires backtracking\n",
    "                - the \"first valid\" method will stop working pretty quickly \n",
    "    - forward checking\n",
    "        - how most people actually solve sudoku irl\n",
    "        - wordle's pruning of letters is similar\n",
    "        - like backtracking but with a look ahead\n",
    "        - when a value is assigned to a variable, remove that value from the domain of all related variables\n",
    "        - if a domain becomes empty, backtrack \n",
    "            - e.g. 5 queens\n",
    "                - when a queen is placed, remove the row, column, and diagonals from the domain of the other queens\n",
    "                - the subsequent queens will have fewer options for placement\n",
    "    - min-conflicts\n",
    "        - randomly assign least bad values to variables\n",
    "        - process:\n",
    "            - choose a variable \n",
    "            - evaluate each possible value that the variable can be assigned for the number of conflicts at that value\n",
    "                - if the current value has the fewest conflicts, keep it and move to the next variable\n",
    "            - change the variable to the value with the fewest conflicts\n",
    "            - repeat for another variable\n",
    "        - for a randomly assigned (not solved) 5 queens problem\n",
    "            - choose a queen\n",
    "            - denote the number of conflicts for each possible move\n",
    "                - i.e. list the number of possible attacks if the queen is moved to each square\n",
    "            - move the queen to the square with the fewest conflicts\n",
    "            - choose another queen randomly and repeat\n",
    "- variable selection heuristics\n",
    "    - minimum remaining values\n",
    "        - choose the variable with the fewest legal values\n",
    "        - e.g. soduko: if a cell has only one possible value, choose that cell and assign it that value\n",
    "            - then move to the next most restricted cell\n",
    "    - degree heuristic\n",
    "        - choose the variable with the most constraints on remaining variables\n",
    "        - the way that humans naturally look at many problems\n",
    "        - immediately reduces the number of options for the next variable as much as possible\n",
    "    - least constraining value\n",
    "        - choose the value that rules out the fewest values in the remaining variables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d10ba4568a5a8694"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 15Feb24 Adversarial Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fc39348711c3793"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2 Player Games\n",
    "- why games?\n",
    "    - exist as a useful benchmark for AI \n",
    "    - closed environments\n",
    "    - easy to compare algorithms to each other and to humans\n",
    "    - many are still challenging\n",
    "    - some solutions and strategies are transferable to other problems\n",
    "- framework\n",
    "    - state\n",
    "        - the current configuration of the game\n",
    "    - initial state\n",
    "        - the starting configuration of the game\n",
    "    - player\n",
    "    - action\n",
    "        - a legal move\n",
    "    - result function\n",
    "        - returns the state that results from a given action\n",
    "    - terminal test\n",
    "        - checks if the game is over and returns the winner\n",
    "    - e.g. tic-tac-toe\n",
    "        - state\n",
    "            - the current configuration of the board\n",
    "        - initial state\n",
    "                - the empty board\n",
    "        - player\n",
    "            - X or O\n",
    "        - action\n",
    "            - a legal move\n",
    "        - result function\n",
    "            - returns the state that results from a given action\n",
    "        - terminal test\n",
    "            - checks if the game is over and returns the winner or a draw if the game is over\n",
    "        - tic-tac-toe is actually small enough to make it practical to determine an actual optimal strategy from any given state\n",
    "    - game of stones\n",
    "        - two players\n",
    "        - both players start with a pile of n stones\n",
    "        - on each turn, a player can take 1, or 2 stones\n",
    "        - the player who takes the last stone wins\n",
    "        - state\n",
    "            - the number of stones left\n",
    "        - initial state\n",
    "            - the number of stones\n",
    "        - player\n",
    "            - player 1 or player 2\n",
    "        - action\n",
    "            - 1 or 2\n",
    "        - result function\n",
    "            - returns the state that results from a given action\n",
    "        - terminal test\n",
    "            - checks if the game is over and returns the winner\n",
    "        - the game of stones is also small enough to make it practical to determine an actual optimal strategy from any given state\n",
    "    - minimax algorithm\n",
    "        - useful for many games\n",
    "        - a recursive algorithm that computes the best move for a player\n",
    "        - assumes that the opponent will play optimally and that the game is zero sum\n",
    "            - i.e. the gain of one player is the loss of the other\n",
    "        - min and max are the two players\n",
    "            - max is the player who is trying to maximize the score\n",
    "            - min is the player who is trying to minimize the score\n",
    "        - recursively descend the game tree alternating between max and min\n",
    "            - when a terminal state is reached, the score is returned \n",
    "                - e.g. 1 for a win, 0 for a draw, -1 for a loss\n",
    "        - minimax(s) =\n",
    "            - utility(s) if s is terminal\n",
    "                - the score of the terminal state\n",
    "            - max$_{a \\in actions(s)}$ minimax(result(s, a)) if player(s) = max\n",
    "                - the best move for max is the move that results in the highest score\n",
    "            - min$_{a \\in actions(s)}$ minimax(result(s, a)) if player(s) = min\n",
    "                - the best move for min is the move that results in the lowest score\n",
    "        - for complex games, the game tree is too large to search completely\n",
    "            - e.g. chess or checkers\n",
    "            - in these cases, consider a depth limited search\n",
    "                - e.g. 3 moves ahead\n",
    "                - this is called \"depth limited minimax\"\n",
    "                - then use an evaluation function to estimate the value of the state\n",
    "                    - e.g. the number of pieces on the board\n",
    "            - this way, you look ahead a few moves and make an educated guess as to which move leads to the best outcome\n",
    "        - **Something like this^^^ will be on the midterm**\n",
    "            - i.e. descending a game tree with the minimax algorithm\n",
    "            - the layers of the tree alternate between max and min\n",
    "                - max will choose the highest value and min will choose the lowest value\n",
    "                - so the algorithm is looking for the largest end value that min will allow max to have"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "354d727c82e5d8b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- minimax pseudocode\n",
    "    ```\n",
    "    function minimax_decision(state):\n",
    "        return argmax(actions(state), min_value(result(state, a)))\n",
    "  \n",
    "    function max_value(state):\n",
    "      if terminal_test(state):\n",
    "            return utility(state)\n",
    "        v = -infinity\n",
    "        for a in actions(state):\n",
    "            v = max(v, min_value(result(state, a)))\n",
    "        return v\n",
    "  \n",
    "    function min_value(state):\n",
    "      if terminal_test(state):\n",
    "            return utility(state)\n",
    "        v = infinity\n",
    "        for a in actions(state):\n",
    "            v = min(v, max_value(result(state, a)))\n",
    "        return v\n",
    "    ```\n",
    "- complexity of minimax\n",
    "    - time: $O(b^m)$\n",
    "        - $b$ is the branching factor\n",
    "        - $m$ is the maximum depth of the tree\n",
    "    - space: $O(bm)$\n",
    "        - the number of nodes in the tree\n",
    "- chess is not markov"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a62b2ef8fc69a458"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- alpha-beta search\n",
    "    - don't explore branches that\n",
    "        - can't win\n",
    "        - won't change the outcome\n",
    "        - won't change the backed up values\n",
    "    - considered the optimal pruning strategy\n",
    "    - psuedocode\n",
    "        ```\n",
    "        function alpha_beta_search(state):\n",
    "            return argmax(actions(state), min_value(result(state, a), -infinity, infinity))\n",
    "        \n",
    "        function max_value(state, alpha, beta):\n",
    "            if terminal_test(state):\n",
    "                return utility(state)\n",
    "            v = -infinity\n",
    "            for a in actions(state):\n",
    "                v = max(v, min_value(result(state, a), alpha, beta))\n",
    "                if v >= beta:  # this is where it differs from minimax\n",
    "                    return v\n",
    "                alpha = max(alpha, v)\n",
    "            return v\n",
    "        \n",
    "        function min_value(state, alpha, beta):\n",
    "            if terminal_test(state):\n",
    "                return utility(state)\n",
    "            v = infinity\n",
    "            for a in actions(state):\n",
    "                v = min(v, max_value(result(state, a), alpha, beta))\n",
    "                if v <= alpha:  # this is where it differs from minimax\n",
    "                    return v\n",
    "                beta = min(beta, v)\n",
    "            return v\n",
    "        ```\n",
    "    - differs from minimax in that it will short circuit the search if it is clear that the branch will not change the outcome\n",
    "        - this is called \"pruning\"\n",
    "    - basically, search your first move option and look at the possible moves for the opponent\n",
    "        - assume the opponent will make the best move for them\n",
    "        - this is the value of your move which would lead to them making that move\n",
    "    - look at your next move option\n",
    "        - if the first option for the opponent after this is worse for you, then you don't need to look at the rest of the options because at best, they will still choose that first option\n",
    "        - else, look at the next option until you verify this is a better move than the first option or you find a worse (for you) move they could make\n",
    "            - if there isn't a worse move they could make, update the value of your move to the value of the worst outcome of this move\n",
    "        - repeat for all of your move options\n",
    "    - **see book examples**\n",
    "    - notes\n",
    "        - keeps track of bounds on backed up values\n",
    "            - alpha is the best value found so far at any choice point along the path for max\n",
    "            - beta is the best value found so far at any choice point along the path for min\n",
    "        - you can search farther with the same amount of time\n",
    "        - can use heuristics to order nodes\n",
    "        - notably used by Stockfish chess engine"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4387d8d11f09490d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 20Feb24\n",
    "- Adversarial Search Continued"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aed74385b9a9b991"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- complexity reminder \n",
    "    - minimax: $O(b^m)$\n",
    "-   alpha-beta pruning: $O(b^{m/2})$ = $O(\\sqrt{b^m})$\n",
    "    - this is because it can prune half of the tree\n",
    "- strategies for simplifying problems\n",
    "    - reduce b, reduce m, convert the game tree to a graph\n",
    "    - note symmetries\n",
    "        - reduces b (branching factor)\n",
    "        - e.g. in tic-tac-toe, the board can be rotated and reflected\n",
    "            - X in the top left is the same as X in the top right and X in the bottom left and X in the bottom right\n",
    "    - opening books (literally)\n",
    "        - reduce m (depth)\n",
    "        - find a list of known good moves and starting strategies\n",
    "        - e.g. in chess, the Sicilian Defense   \n",
    "    - end game databases\n",
    "        - if you can reach a certain state, the game may be effectively solved\n",
    "            - e.g. king and rook vs king is a known win based on established strategies, so if you can reach that state, you can play optimally\n",
    "    - meta strategies for pruning\n",
    "        - have some heuristics for when to stop searching\n",
    "            - e.g. if a move will lose the queen in the first 3 moves, you can stop searching because a win is far less likely\n",
    "    - search to a fixed depth and use an evaluation function\n",
    "        - if you can't find a solution in a reasonable amount of time, just stop\n",
    "        - continue to search if one of the points where it bottoms out may be promising\n",
    "        - anytime algorithms\n",
    "            - return the best move found so far when time runs out\n",
    "            - e.g. iterative deepening\n",
    "                - search to a depth of 1, then 2, then 3, etc\n",
    "                - if you run out of time, return the best move found so far\n",
    "            - receding horizon control\n",
    "                - **look this up**\n",
    "    - transposition tables\n",
    "        - caching or memorization\n",
    "        - store the results of previous searches \n",
    "        - if you reach a state that has been searched before, use the results of that search\n",
    "            - this is where symmetry awareness can be useful\n",
    "        - tree to graph because you can look at it as multiple paths to the same state\n",
    "        - e.g. chess engines"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35f95065f3936cc8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- games with chance\n",
    "    - can be constructed as a game tree of Max - Chance - Min - Chance - Max - Chance - Min - Chance - ...\n",
    "    - use expectiminimax function\n",
    "        - like minimax but with chance nodes\n",
    "        - the expected value of the chance nodes is used\n",
    "- games with imperfect information\n",
    "    - e.g. Battleship\n",
    "        - if not a hit, you don't know where to look next\n",
    "        - if a hit, you have a very good idea of where to look next\n",
    "    - use a game tree with chance nodes\n",
    "    - use a game tree with information sets\n",
    "        - a set of states that are indistinguishable to the player\n",
    "        - e.g. in poker, the player doesn't know the other player's hand\n",
    "    - e.g. tic-tac-toe with a blindfold\n",
    "        - i.e. you can't see the board\n",
    "        - can be optimally solved with a game tree with information sets\n",
    "    - a key concept is the \"belief state\"\n",
    "        - the set of states that are possible given the information available\n",
    "        - e.g. in blindfolded tic-tac-toe, the belief state is the set of all places the O's could be\n",
    "- multi-player games\n",
    "    - can use minimax, alpha-beta pruning, Q-sum, nash, etc\n",
    "    - can represent as a game tree except each node will have n values for n players instead of just 2\n",
    "        - each player will act as a max player in their turn and a min player in the other players' turns\n",
    "    - other possible considerations\n",
    "        - collusion and shifting alliances\n",
    "        - other coordination\n",
    "            - dupes (aka ringers)\n",
    "                - i.e. a player who is working with another player and will throw the game to them\n",
    "                - if you will lose by a little, maybe you can cheat just a little bit to win\n",
    "            - enemy of my enemy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "247114c20cf10725"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5c56520bccebdd3c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ebe325d3e2b68ca2"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ff03c3717403ae3"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bb7a57af6731e01a"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d8b11f53c0c60902"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
