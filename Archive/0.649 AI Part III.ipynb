{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592201ee6c905e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T17:32:48.534370Z",
     "start_time": "2024-05-05T17:32:48.526032Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b785c7e9fa3d6b4e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Making Complex Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f3f660c590e05",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Markov Decision Processes (MDPs)\n",
    "    - states, s ∈ S\n",
    "    - actions, a ∈ A\n",
    "        - sometimes A(s)\n",
    "    - rewards, R(s)\n",
    "        - sometimes r(s, a)\n",
    "    - transition model, P(s' | s, a)\n",
    "        - P(s' | s, a) is the probability that the next state s' is reached when action a is taken in state s\n",
    "            - environmental model for how agent's actions update the world\n",
    "    - Element 1: Markov attributes\n",
    "        - because it is a Markov process, the probability of reaching a state s' depends only on the current state s and the action a\n",
    "            - P(s' | s, a) = P(s' | s, a, s1, a1, s2, a2, ..., st, at)\n",
    "                - i.e. the future does not depend on the past, only the present\n",
    "        - time that you reach a state is not important, only the state itself\n",
    "            - i.e. the process is memoryless, the first time you land on Go is the same as the 70th time you land on Go\n",
    "        - sort of like a deterministic process, but with probabilities\n",
    "            - e.g. in gridworld there is a chance that when you step_forward() you will actually step_backward() instead or stay in the same place\n",
    "            - the probabilities are the only thing that is not deterministic\n",
    "    - Element 2: Maximized Reward\n",
    "        - the goal is to maximize the sum of rewards over a sequence of actions\n",
    "            - the reward is the only thing that matters\n",
    "            - the agent is trying to maximize the sum of rewards\n",
    "        - $max_{a1, a2, ..., an} \\sum_{t=0}^{n} R_t$\n",
    "        \n",
    "            - deterministic, finite horizon\n",
    "            - where n is the number of steps in the sequence\n",
    "        - $max_{a1, a2, ..., an} \\sum_{t=0}^{\\infty} \\gamma^tR_t$\n",
    "        \n",
    "            - stochastic, infinite horizon\n",
    "            - where $\\gamma$ is the discount factor\n",
    "                - $\\gamma \\in [0, 1]$\n",
    "                - $\\gamma = 0$ means that the agent only cares about the immediate reward\n",
    "                - $\\gamma = 1$ means that the agent cares about all future rewards equally\n",
    "                - $\\gamma > 1$ causes an infinite sum\n",
    "                - $\\gamma = 0.9$ is a common value\n",
    "    - Element 3: Policy\n",
    "        - map from state to action\n",
    "        - $\\pi(s) = a$\n",
    "            - when in state s, take action a\n",
    "    - Element 4: Value/Utility function\n",
    "        - usually called V(s), sometimes U(s)\n",
    "        - maps from state to value if you follow the policy\n",
    "    - Element 5: Optimal Value Function\n",
    "        - the value function that maximizes the sum of rewards\n",
    "        - denoted with a star, $V^*(s)$\n",
    "        - $V^*(s) = max_{\\pi} V^{\\pi}(s)$\n",
    "            - the best reward you can get from state s is by making all the best decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a77b977bde970d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Note: long term vs short term rewards\n",
    "    - the agent may have to make a decision that sacrifices short term rewards for long term rewards\n",
    "    - e.g. in chess, sacrificing a piece to gain a positional advantage\n",
    "- R(s) can be used to tailor the agent's behavior\n",
    "    - e.g. in gridworld with two terminal states: one with reward 1 and one with -1\n",
    "        - a reward of < -1 for each step can be used to encourage the agent to reach the goal as quickly as possible\n",
    "            - i.e. end the game as fast as possible even if the terminal state has a negative (but higher) reward\n",
    "        - a reward of > 1 encourages the agent to never reach the terminal state\n",
    "            - i.e. the agent will try to avoid the terminal state at all costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4f247460ce5335",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- acting with a value function\n",
    "    - use MEU (maximize expected utility) to choose the best action\n",
    "        - $MEU = max_{a} \\sum_{s'} P(s' | s, a) V(s')$\n",
    "        - the agent will choose the action that maximizes the expected utility **of the next state**\n",
    "    - hill climbing behavior, i.e. follow the gradient of the value function\n",
    "        - $a = argmax_{a} V(s')$\n",
    "        - the agent will choose the action that maximizes the value of the next state\n",
    "        - if it is a probabilistic environment, the agent will choose the action that maximizes the expected value of the next state\n",
    "            - i.e. account for the possibility of the action failing\n",
    "        - if the environment is deterministic, the agent will choose the action that maximizes the value of the next state\n",
    "            - i.e. the chosen action will always succeed\n",
    "- Optimality Principle\n",
    "    - Bellman Equation\n",
    "        - $V^*(s) = max_{a} (R(s) + \\sum_{s'}  \\gamma P(s' | s, a) [V^*(s')])$\n",
    "        - if it is deterministic, the equation simplifies to\n",
    "            - $V^*(s) = max_{a} (R(s) + \\gamma V^*(s'))$\n",
    "    - each state has an associated $V^*(s)$\n",
    "        - N states, N equations to solve completely\n",
    "    - algorithms\n",
    "        - value iteration\n",
    "            - evaluate every state's value and then update them all at once\n",
    "            1. initialize V(s) with 0 or some guess, k = 0\n",
    "            2. for each state s\n",
    "                - $V_{k+1}(s) = max_{a} (R(s) + \\gamma \\sum_{s'} P(s' | s, a) V_k(s'))$\n",
    "                - repeat until $V_{k+1}(s)$ and $V_k(s)$ are close enough**\n",
    "                    - close enough defined by $max_s |V_{k+1}(s) - V_k(s)| < \\epsilon\\frac{1 - \\gamma}{\\gamma}$\n",
    "                        - where $\\epsilon$ is the desired error\n",
    "            - e.g. in gridworld, the value of each state is the maximum of the values of the states that can be reached from the current state\n",
    "        - in place value iteration\n",
    "            - same as value iteration, but update the value of each state as you go\n",
    "            - i.e. don't wait until the end of the loop to update the values\n",
    "            - use half as much memory because you only need to store the current and previous values\n",
    "            - it will probably converge in fewer iterations\n",
    "        - policy evaluation\n",
    "            - given a policy, compute its value function\n",
    "            - $V^{\\pi}(s) = R(s) + \\gamma \\sum_{s'} P(s' | s, \\pi(s)) V^{\\pi}(s')$ for s in S\n",
    "                - simpler than the Bellman equation because it doesn't have the max operator and the policy is fixed\n",
    "                    - i.e. we know what $\\pi(s)$ is\n",
    "                - a linear system of equations that can be solved with linear algebra\n",
    "                    - often develops a triangular matrix that can be solved in O(n^2) time where n is the number of states\n",
    "        - policy improvement\n",
    "            - given a policy, find a better policy\n",
    "            - $\\pi'(s) = argmax_{a} (R(s) + \\gamma \\sum_{s'} P(s' | s, a) V^{\\pi}(s'))$\n",
    "                - i.e. choose the action that maximizes the expected value of the next state\n",
    "        - policy iteration\n",
    "            1. initialize V(s) to any admissible policy, k = 0\n",
    "            2. for each state s\n",
    "                - $V_{k+1}(s) = R(s) + \\gamma \\sum_{s'} P(s' | s, \\pi(s)) V_k(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a95e8f0b623ed1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Learning from Examples I\n",
    "- 04Apr2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641e2a3dab0e7a8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- moving from utility based agents to learning agents\n",
    "    - in some cases, you may have to learn the utility function\n",
    "- learning from experience\n",
    "    - gain experience (E) with respect to a task (T) and a performance measure (P)\n",
    "        - e.g. Samuel's checkers player\n",
    "            - T: playing checkers\n",
    "            - P: % of games won\n",
    "            - E: playing games against itself\n",
    "    - learning system\n",
    "        - a loop\n",
    "            - experiment generator starts a new problem ->\n",
    "            - performance system does the task ->\n",
    "            - critic evaluates the performance ->\n",
    "            - generalizer modifies the model or hypothesis -> (back to experiment generator)\n",
    "                - finding $\\hat V$, an approximation of the true value function\n",
    "    - representation\n",
    "        - how to represent the value function\n",
    "            - e.g. Samuel's 6 features of the checkers board\n",
    "                - the true value function is some $V^*(board)$\n",
    "                - his estimated value function is $\\hat V(board) = \\sum_{i=1}^{6} w_i f_i(board)$\n",
    "                    - $w_i$ is the weight of feature i\n",
    "                    - $f_i(board)$ is the value of feature i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02f4a9fa2a89cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- forms of learning\n",
    "    - supervised\n",
    "        - learn a function from labeled examples\n",
    "        - given instances and their correct outputs (labels)\n",
    "        - e.g. recognizing handwritten digits after training on a dataset of labeled images\n",
    "    - unsupervised\n",
    "        - recognize and learn patters in the data\n",
    "        - given instances without labels\n",
    "        - e.g. clustering\n",
    "    - reinforcement\n",
    "        - learn from rewards\n",
    "        - given instances and rewards\n",
    "        - generally the reward is given at the end of the sequence of actions, not immediately\n",
    "            - e.g. it is postponed until the end of the game and may not be explicitly tied to a specific action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e3260d1fd9d9f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- supervised learning\n",
    "    - given a vector of samples and their labels\n",
    "        - $D = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$\n",
    "    - goal: learn a function that maps from x to y\n",
    "        - $f: X \\rightarrow Y$\n",
    "        - assumption: there is some true function $f^*$ that maps from x to y\n",
    "            - $y = f^*(x)$\n",
    "        - the goal is to find a function $\\hat f$ that approximates $f^*$\n",
    "            - $\\hat f$ is the hypothesis\n",
    "    - output may be a category (classification), a real number (regression), or a sequence (sequence prediction)\n",
    "    - issues:\n",
    "        - consistency in data\n",
    "        - generalization (fitting)\n",
    "            - will h match f on new data?\n",
    "        - noisy data\n",
    "            - error in the data may affect the hypothesis\n",
    "        - model complexity (hypothesis space)\n",
    "            - how complex should the model be?\n",
    "            - too simple and it may not capture the true function\n",
    "            - too complex and it may overfit the data\n",
    "                - i.e. it may capture the noise in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5d178478f7aca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- metrics\n",
    "    - accuracy\n",
    "        - $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "        \n",
    "            - where TP is true positive, TN is true negative, FP is false positive, and FN is false negative\n",
    "        - the proportion of correct predictions\n",
    "    - precision\n",
    "        - $\\frac{TP}{TP + FP}$\n",
    "        - the proportion of positive identifications that were actually correct\n",
    "    - recall\n",
    "        - $\\frac{TP}{TP + FN}$\n",
    "        - the proportion of actual positives that were identified correctly\n",
    "    - least squares fit:\n",
    "        - Loss = $\\sum_{i=1}^{n} (y_i - (ax_i + b))^2$\n",
    "            - y_i is the true value\n",
    "            - where a and b are the parameters of the model\n",
    "            - the goal is to minimize the loss\n",
    "    - exact polynomial fit:\n",
    "        - Loss = $\\sum_{i=1}^{n} (y_i - (a_0 + a_1x_i + a_2x_i^2 + ... + a_kx_i^k))^2$\n",
    "            - where a_0, a_1, ..., a_k are the parameters of the model\n",
    "            - the goal is to minimize the loss\n",
    "            - probably a poor model because it will overfit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133c21e736c79a5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- overfitting\n",
    "    - the model fits the training data too well and does not generalize to new data\n",
    "        - e.g. the exact polynomial fit will work well on the training data, but probably poorly on new data\n",
    "    - Occam's Razor\n",
    "        - \"Entities should not be multiplied without necessity\"\n",
    "        - the simplest explanation/model that fits the data is probably the best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe180b345d6c2ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- decision trees (nothing new compared to ML with Johnson)\n",
    "    - structural details\n",
    "        - input: a set of input attributes\n",
    "        - decision: predicted output\n",
    "        - e.g. boolean classification\n",
    "        - a decision is reached by following a path from the root to a leaf\n",
    "        - each internal node is a branching decision based on an attribute's value\n",
    "    - popular when\n",
    "        - there are discrete target outputs\n",
    "        - the data is noisy\n",
    "        - it is valuable to understand the decision making process\n",
    "            - e.g. need to be able to explain the decision to a human who is not a data scientist\n",
    "    - inputs can be categorical or numerical (discrete or continuous)\n",
    "    - outputs can be categorical or numerical\n",
    "        - e.g. CART and regression trees\n",
    "- decision stumps\n",
    "    - a decision tree with only one split (only one attribute is considered)\n",
    "    - not really anything wild but Branicky thinks the're neat/funny\n",
    "    - e.g. a decision stump for classifying fruit with color as the only attribute\n",
    "        - if x == yellow, then lemon, if x == green, then lime, else orange "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6384166c5dc5f2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- building a decision tree system\n",
    "    - start with the root node\n",
    "        - choose the attribute that explains the largest section of the data\n",
    "    - recursively build the tree down the branches\n",
    "        - choose the attribute that explains the largest section of the data\n",
    "        - repeat until the data is perfectly classified or the tree is sufficiently deep\n",
    "- more on this Tuesday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43e9c065f22b54",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Learning from Examples II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33949e93d057cf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Learning a Decision Tree\n",
    "    - start with the training data\n",
    "    - begin with the root node\n",
    "        - choose the attribute that best splits the data\n",
    "        - split the data into subsets based on the attribute\n",
    "    - repeat for each subset\n",
    "    - stop when the data is perfectly classified or the tree is sufficiently deep\n",
    "- ID3 algorithm\n",
    "    - psuedocode\n",
    "    ```\n",
    "    if examples is empty\n",
    "        return plurality-value(parent-examples)\n",
    "    else if all examples have the same classification\n",
    "          return the classification\n",
    "        else if attributes is empty\n",
    "            return plurality-value(examples)\n",
    "        else\n",
    "            A <- the attribute that best classifies examples\n",
    "            tree <- a new decision tree with root A\n",
    "            for each value v of A\n",
    "                add a branch to tree with label v\n",
    "                exs <- {e : e in examples and e.A = v}\n",
    "                subtree <- ID3(exs, examples)\n",
    "                add a branch to tree with label A=v and subtree subtree\n",
    "            return tree\n",
    "    ```\n",
    "    - in practice, depth can also be a stopping condition\n",
    "        - e.g. stop when the tree is 5 levels deep \n",
    "    - information gain is another potential stopping condition\n",
    "    - the algorithm is greedy\n",
    "        - it chooses the best attribute at each step\n",
    "        - it may not find the best tree\n",
    "- finding the \"best\" attribute\n",
    "    - entropy is a measure of disorder\n",
    "        - for random value V\n",
    "            - $H(V) = -\\sum_{k=1}^{n} P(v_k)log_2P(v_k)$\n",
    "        - binary-valued\n",
    "            - $B(q) = -(qlog_2q + (1 - q)log_2(1 - q))$\n",
    "                        \n",
    "                - where q is the proportion of positive examples\n",
    "                - e.g. if q = 0.5, then B(q) = 1\n",
    "                    - B(0) = B(1) = 0\n",
    "        - the entropy of a set S of boolean instances\n",
    "            - p = number of positive instances in S\n",
    "            - n = number of negative instances in S\n",
    "            - $B(\\frac{p}{p + n})$ is the entropy of S\n",
    "    - information gain\n",
    "        - the expected reduction in entropy from splitting the data on an attribute\n",
    "        - if no information remains, the split accounts for all the information in the set\n",
    "        - Gain(A) = $B\\frac{p}{p + n} - remainder(A)$\n",
    "            - where p and n are the number of positive and negative instances in the set \n",
    "            - where remainder(A) is the expected entropy of the subsets\n",
    "                - $remainder(A) = \\sum_k \\frac{p_k + n_k}{p + n}B(\\frac{p_k}{p_k + n_k})$ \n",
    "                \n",
    "                    - where p_k and n_k are the number of positive and negative instances in subset k\n",
    "        - e.g. tennis\n",
    "            - humidity and play tennis\n",
    "                - B(9/14) = 0.940\n",
    "                    - this is the entropy of the entire set (listed in the course notes)\n",
    "                        - 14 instances, 9 positive (play tennis), 5 negative (don't play tennis)\n",
    "                - humidity has 2 values: high and normal\n",
    "                - 14 instances, 7 positive, 7 negative\n",
    "                - $B(\\frac{7}{14}) = 1$\n",
    "                - high humidity\n",
    "                    - 7 instances, 3 positive, 4 negative\n",
    "                    - $B(\\frac{3}{3 + 4}) = 0.985$\n",
    "                - normal humidity\n",
    "                    - 7 instances, 6 positive, 1 negative\n",
    "                    - $B(\\frac{6}{6 + 1}) = 0.592$\n",
    "                - Gain(humidity) = 0.940 - $\\frac{7}{14}0.985 - \\frac{7}{14}0.592 = 0.151$\n",
    "            - e.g. wind and play tennis\n",
    "                - B(9/14) = 0.940\n",
    "                - wind has 2 values: strong and weak\n",
    "                - 14 instances, 8 positive, 6 negative\n",
    "                - $B(\\frac{8}{14}) = 0.985$\n",
    "                - strong wind\n",
    "                    - 8 instances, 6 positive, 2 negative\n",
    "                    - $B(\\frac{6}{6 + 2}) = 0.811$\n",
    "                - weak wind\n",
    "                    - 6 instances, 3 positive, 3 negative\n",
    "                    - $B(\\frac{3}{3 + 3}) = 1$\n",
    "                - Gain(wind) = 0.940 - $\\frac{8}{14}0.811 - \\frac{6}{14}1 = 0.048$\n",
    "            - humidity is the better attribute to split on because it has the higher information gain\n",
    "            - after splitting on humidity, start the process over with the subsets (high and normal) to find the next best attribute to split on in each subset\n",
    "                - for high humidity you are calculating only the entropy of the high humidity instances\n",
    "                - for normal humidity you are calculating only the entropy of the normal humidity instances\n",
    "                - both subsets may have different best attributes to split on\n",
    "            - **see the \"Split and Recurse\" slide from the 09Apr2024 lecture for a visual representation of this process and more examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301a127b06f8a23",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "ID3 hypothesis space\n",
    "- ID3 searches through the space of decision trees\n",
    "    - from simple to complex\n",
    "    - guided by information gain\n",
    "- no backtracking\n",
    "    - greedy with respect to information gain\n",
    "- can get caught in local optima\n",
    "    - e.g. if the best attribute to split on is not the best attribute to split on at the next level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4788b2c48357944d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Issues with decision trees\n",
    "- continuous attributes\n",
    "    - discretize the attribute\n",
    "        - i.e. choose thresholds and split the data into sets based on the thresholds\n",
    "        - e.g. if the attribute is temperature, discretize it into hot, warm, and cold\n",
    "- missing data\n",
    "    - some piece of data is missing for an instance\n",
    "    - common solutions:\n",
    "        - apply the most common value for the attribute in similar instances\n",
    "            - e.g. is it more common that it is sunny or rainy \n",
    "        - apply the most common value for the attribute in the entire dataset\n",
    "            - e.g. is it more common that it is sunny or rainy in the entire dataset\n",
    "        - split the value proportionally\n",
    "            - e.g. if 5/13 sunny, 4/13 rain, 4/13 overcast, then split the missing data 5/13 sunny, 4/13 rain, 4/13 overcast\n",
    "- weighting\n",
    "    - in the tennis example, there are 72 possible combinations of attributes\n",
    "    - if you have 1000 instances, it is not useful to train on all of the data\n",
    "    - count the number of instances that match each combination of attributes\n",
    "    - apply a weight to each of the 72 combinations based on the number of instances that match the combination\n",
    "- testing performance\n",
    "    - split into training and testing sets\n",
    "    - test is a disjoint set chosen at random from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e29143e39dde8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Ensemble Learning\n",
    "- combining multiple models to improve performance\n",
    "- plurality of k experts\n",
    "- can apply weights to the experts\n",
    "    - add to a weight when the expert is correct\n",
    "    - subtract when the expert is incorrect\n",
    "- random forest\n",
    "    - the experts are decision trees\n",
    "    - classification is determined by a majority vote\n",
    "    - trees must differ from each other\n",
    "        - e.g. by using different subsets of the data or different attributes\n",
    "- bagging\n",
    "    - training multiple models on different subsets of the data\n",
    "- boosting\n",
    "    - successively train models\n",
    "        - weight samples higher if they were misclassified by the previous model\n",
    "        - i.e. the next model will be good at classifying the samples that the previous model was bad at classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8da697d17d9a74",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a24c5d5acb5b697",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "P(pass(c1, t1)|q+(c1)) = 0.8\t<- pass given good shape \n",
    "P(pass(c1, t1)|q−(c1)) = 0.35\t<- pass given bad shape \n",
    "P(pass) = P(pass│q+)P(q+)+P(pass│q-)P(q-) \n",
    "    = P(pass | q+)P(q+) + P(pass | q-)P(q-) \n",
    "    = 0.8*0.7 + 0.35*0.3 \n",
    "    = 0.665 P(fail) \n",
    "    = 0.335\n",
    "P(q+|pass) = $\\frac{P(pass|q+)P(q+)}{P(pass)}$ \n",
    "    = $\\frac{0.8*0.7}{0.665}$ \n",
    "    = 0.842\n",
    "P(q−|pass) = $\\frac{P(pass|q-)P(q-)}{P(pass)}$\n",
    "    = $\\frac{0.35*0.3}{0.665}$\n",
    "    = 0.158\n",
    "P(q+|fail) = $\\frac{P(fail|q+)P(q+)}{P(fail)}$\n",
    "    = $\\frac{0.2*0.7}{0.335}$\n",
    "    = 0.416\n",
    "P(q−|fail) = $\\frac{P(fail|q-)P(q-)}{P(fail)}$\n",
    "    = $\\frac{0.65*0.3}{0.335}$\n",
    "    = 0.584"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca50a31eecb25ab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "U(q+,buy) = 500\n",
    "U(q−,buy) = -200\n",
    "\n",
    "EU(buy|pass) = P(q+|pass)U(q+,buy) + P(q−|pass)U(q−,buy)\n",
    "= 0.842*500 + 0.158*-200\n",
    "= 421 - 31.6\n",
    "= 389.4\n",
    "    \n",
    "EU(buy|fail) = P(q+|fail)U(q+,buy) + P(q−|fail)U(q−,buy)\n",
    "= 0.416*500 + 0.584*-200\n",
    "= 208 - 116.8\n",
    "= 91.2\n",
    "EU(buy-|pass) = 0.842*0 + 0.158*0\n",
    "= 0\n",
    "EU(buy-|fail) = 0.416*0 + 0.584*0\n",
    "= 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985418d9aa9310a5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# video lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ce911354358d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Bayesian Learning\n",
    "-16Apr2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba9bcf99dc75ad",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Bayesian Learning\n",
    "    - statistical learning\n",
    "    - based on Bayes Rule\n",
    "        - $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "        - \"posterior probability of A given B is equal to the likelihood of B given A times the prior probability of A divided by the prior probability that B is observed\"\n",
    "- defining the \"best\" hypothesis in the hypothesis space\n",
    "    - MAP\n",
    "        - best = most probable hypothesis given the data\n",
    "        - Maximum A Posteriori hypothesis\n",
    "        - $h_{MAP} = argmax_{h \\in H} P(h|D)$ = $argmax_{h \\in H} P(D|h)P(h)$\n",
    "            - where h is the hypothesis and D is the data\n",
    "            - e.g. $D$ = head pain, $h_1$ = migraine, $h_2$ = brain tumor, $H$ = $\\{h_1, h_2\\}$\n",
    "                - if $P(D|h_1)P(h_1) > P(D|h_2)P(h_2)$, then $h_{MAP} = h_1$\n",
    "    - ML\n",
    "        - best = most likely to have produced the data\n",
    "        - Maximum Likelihood hypothesis\n",
    "            - \"without any other specific knowledge, assume that each $h_i$ is equally likely\"\n",
    "                - $P(h_i) = P(h_j)$ for all $h_i, h_j \\in H$ \n",
    "        - $h_{ML} = argmax_{h \\in H} P(D|h)$ = $argmax_{h \\in H} log(P(D|h))$\n",
    "            - e.g. $D$ = \\{heads, heads\\}, h_1 = P(heads) = 0.5, h_2 = P(heads) = 0.6\n",
    "                - $P(D|h_1) = 0.5^2 = 0.25$\n",
    "                - $P(D|h_2) = 0.6^2 = 0.36$\n",
    "                - $P(D|h_1) < P(D|h_2)$, so $h_{ML} = h_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b5804a47a6a6f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- further examples\n",
    "    - MAP\n",
    "        - hypothesis: $h_1$ = +cancer, $h_2$ = -cancer\n",
    "        - priors: $P(+c) = 0.008$, $P(-c) = 0.992$\n",
    "        - test gives correct positive answer 98% of the time\n",
    "            - $P(+t|+c) = 0.98$\n",
    "        - test gives correct negative answer 97% of the time\n",
    "            - $P(-t|-c) = 0.97$\n",
    "        - what is the diagnosis if the test is positive?\n",
    "            - $h_{MAP} = argmax_{h \\in H} \\{P(D|h)P(h)\\}$\n",
    "                        - $= argmax \\{P(+t|+c)P(+c), P(+t|-c)P(-c)\\}$\n",
    "                        - $= argmax \\{0.98*0.008, 0.03*0.992\\}$\n",
    "                        - $= argmax \\{0.00784, 0.02976\\}$\n",
    "        - what is the chance of cancer?\n",
    "            - need to do the normalization\n",
    "                - the argmax above must sum to 1\n",
    "            - $P(+c|+t) = \\frac{P(+t|+c)P(+c)}{P(+t)}$\n",
    "                - $= \\frac{0.00784}{0.00784 + 0.02976}$\n",
    "                - $= \\frac{0.00784}{0.0376}$\n",
    "                - $= 0.2085$\n",
    "                                \n",
    "                - 20.85% > 0.8% so the probability that it is cancer decreased \n",
    "                    - A Priori probability of not cancer is 0.992%\n",
    "                    - A Posteriori probability of not cancer is 0.79%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782eee1bc0396b4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Learning Bayes Networks\n",
    "    - problem: given a network structure and data, calculate the CPTs (conditional probability tables)\n",
    "    - solution: just estimate\n",
    "        - $\\hat{P}\\{V_i = v_i | P_i = p_i\\} = \\frac{count(V_i = v_i \\land P_i = p_i)}{count(P_i = p_i)}$\n",
    "        \n",
    "            - where $V_i$ is the variable, $v_i$ is the value, $P_i$ is the parent variable, and $p_i$ is the parent value\n",
    "        - this is the ML estimate\n",
    "        - children\n",
    "            - e.g. table in book example in slides (robot with battery, move, arm, liftable\n",
    "                - $\\hat{P}(+b) = 94/100 = 0.94$\n",
    "                                \n",
    "                    - count instances of +b and divide by the total number of instances\n",
    "                - $\\hat{P}(+g|-b) = 0/6 = 0$\n",
    "                                \n",
    "                    - count instances of +g and -b and divide by the total number of instances of -b\n",
    "                - $\\hat{P}(+m|+b, -l) = 1/30$\n",
    "                                \n",
    "                    - count instances of +m and +b and -l and divide by the total number of instances of +b and -l\n",
    "            - **it's just counting from the table**\n",
    "            - what if you don't have data for the parents?\n",
    "                - some people initialize all numerators to 1 and all denominators to |V|\n",
    "                    - V is the number of labels for the variable V\n",
    "                    - e.g. boolean case, |V| = 2 $\\implies \\frac{1}{2}$ \n",
    "    - issues\n",
    "        - unknown BN structure\n",
    "            - usually supplied by a human expert\n",
    "                - even then, there may be \"hidden\" intermediate variables\n",
    "                    - e.g. meat spontaneously creates flies\n",
    "                        - the hidden variable is the presence of flies laying eggs in the meat\n",
    "                    - cholera spread near water sources\n",
    "                        - germs in the water are the hidden variable\n",
    "            - solution: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad64c8cd42a98f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Naive Bayes Classifiers\n",
    "    - assume structure\n",
    "        - all attributes are independent given the class\n",
    "        - the class is the parent of all the attributes\n",
    "        - estimate CPTs from training examples\n",
    "            - if the class is binary, each attribute has 2 CPTs\n",
    "                - one for each class\n",
    "    - e.g. tennis & weather\n",
    "        - <img src=\"images/ML_Tennis.png\">\n",
    "        - class = play tennis\n",
    "            - P(play tennis) = $\\frac{9}{14}$\n",
    "            - P(don't play tennis) = $\\frac{5}{14}$\n",
    "            - P(strong wind | play tennis) = $\\frac{\\sum strong | yes}{\\sum yes} = \\frac{3}{9} = 0.333$\n",
    "            - P(strong wind | don't play tennis) = $\\frac{\\sum strong | no}{\\sum no} = \\frac{3}{5} = 0.6$ \n",
    "            - P(weak wind | play tennis) = $1 - \\frac{3}{9} = 0.667$\n",
    "                - don't have to calculate this, it is just 1 - P(strong wind | play tennis)\n",
    "        - MAP defined best\n",
    "            - we can use the non normalized probabilities to determine the best hypothesis\n",
    "            - P(play| sunny, cool, high, strong) ~ P(play)P(sunny|play)P(cool|play)P(high|play)P(strong|play)\n",
    "                - $= \\frac{9}{14} * \\frac{2}{9} * \\frac{3}{9} * \\frac{3}{9} * \\frac{3}{9}$\n",
    "                - $= 0.0053$\n",
    "            - P(don't play| sunny, cool, high, strong) ~ P(don't play)P(sunny|don't play)P(cool|don't play)P(high|don't play)P(strong|don't play)\n",
    "                - $= \\frac{5}{14} * \\frac{3}{5} * \\frac{1}{5} * \\frac{2}{5} * \\frac{3}{5}$\n",
    "                - $= 0.0096$\n",
    "            - argmax $\\implies$ don't play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f8be2ead13eac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- issues continued\n",
    "    - hidden variables\n",
    "    - incomplete or missing data\n",
    "        - missing attribute values\n",
    "    - patterns that don't appear at all\n",
    "    - solution ideas\n",
    "        - $h_ml = argmax_{h \\in H} P(D|h)$, where $D = [x, Z]$\n",
    "        - Z is unobserved so take the expectation over all values of Z\n",
    "        - $h_ml = argmax_{h \\in H} E[log(P(x,Z|h))]$\n",
    "                - $= argmax_{h \\in H} \\sum_{z} P(Z=z|x,h)log(P(x,z|h))$\n",
    "    - implementation\n",
    "        - EM algorithm\n",
    "            - Expectation Maximization\n",
    "            - iterative\n",
    "            - $\\theta$ = set of known and unknown parameters for h\n",
    "                - e.g. the CPTs\n",
    "            - start with a random guess for unknown parameters of $\\theta$\n",
    "            - repeat until convergence\n",
    "                - E-step: estimate the hidden variables\n",
    "                - M-step: estimate the parameters\n",
    "                - repeat until convergence\n",
    "            - $\\theta^{i+1} = argmax_\\theta \\sum_z P(Z=z|x,h,\\theta^i)log(P(x,Z=z|\\theta))$\n",
    "                        \n",
    "                - where $\\theta^i$ is the current estimate of the parameters\n",
    "                - $\\theta^{i+1}$ is the next estimate of the parameters\n",
    "                - $P(Z=z|x,h,\\theta^i)$ is the probability of the hidden variable given the data and the current estimate of the parameters\n",
    "                - $P(x,Z=z|\\theta)$ is the probability of the data and the hidden variable given the parameters\n",
    "            - e.g. robot except we don't know the value of L for the 2nd to last row\n",
    "                - $\\theta_1$ = proportion of L=True\n",
    "                - pick a random value for $\\theta_1 \\in [0,1]$\n",
    "                - repeat until convergence\n",
    "                    - $c_1 = P(-g, -m, -b, +l | \\theta_1^i)$\n",
    "                        - = $P(-g|-b)P(-m|-b, +l, \\theta_1^i)P(-b)P(+l|\\theta_1^i)$\n",
    "                        - = $(6/6)(1)(6/100)(64+\\theta_1^i*4)/100$\n",
    "                    - $c_2 = P(-g, -m, -b, -l | \\theta_1^i)$\n",
    "                        - = $P(-g|-b)P(-m|-b, -l, \\theta_1^i)P(-b)P(-l|\\theta_1^i)$\n",
    "                        - = $(6/6)(1)(6/100)(32+(1-\\theta_1^i*4)/100$\n",
    "                - M-step\n",
    "                    - $\\theta_1^{i+1} = \\frac{c_1}{c_1 + c_2}$\n",
    "                - code version below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a7b5d202233538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T20:41:27.186619Z",
     "start_time": "2024-04-16T20:41:27.169482Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.225044, 0.649002, 0.66596, 0.666638, 0.666666, 0.666667, 0.666667]\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "th1 = random()\n",
    "allth1 = [th1]\n",
    "\n",
    "for i in range(6):\n",
    "    c1 = 1 * 1 * (6/100) * (64 + th1*4) / 100\n",
    "    c2 = 1 * 1 * (6/100) * (32 + (1-th1)*4) / 100\n",
    "    th1 = c1 / (c1 + c2)\n",
    "    allth1.append(th1)\n",
    "print([round(t1, 6) for t1 in allth1])\n",
    "\"\"\"\n",
    "converges pretty dang well within just a few iterations\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341472f40d5e87ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- EM algorithm can actually be used to infer the entire structure of a network\n",
    "    - downside is that there may be many possible structures\n",
    "        - can use hill climbing to estimate the best structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151831f82ff7a483",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Reinforcement Learning\n",
    "- 18Apr2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce425b14623fcfca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Reinforcement Learning\n",
    "    - learning from rewards\n",
    "    - the agent interacts with the environment\n",
    "        - the agent takes an action\n",
    "        - the environment responds with a new state and a reward\n",
    "        - the agent uses the reward to update its policy\n",
    "    - issues with RL\n",
    "        - Thorndike's Law of Effect\n",
    "            - \"responses that produce a satisfying effect in a particular situation become more likely to occur again in that situation, and responses that produce a discomforting effect become less likely to occur again in that situation\"\n",
    "            - conditioning\n",
    "                - can consider an animal to be an agent which increases weights given good outcomes and decreases weights given bad outcomes\n",
    "                - effect is often proportional to the magnitude of the reward/punishment\n",
    "        - min/max a performance measure of a system over time\n",
    "            - e.g. optimizing a system to maximize the number of cars that can pass through a toll booth in a given time\n",
    "        - combines search and memory\n",
    "            - search is selectional\n",
    "                - try alternatives by comparing consequences\n",
    "                - e.g. natural selection\n",
    "            - memory is associative\n",
    "                - remember what worked in the past and in what situations\n",
    "                - strengthen the weight of the action in the state if it leads to reward\n",
    "                - weaken the weight of the action in the state if it leads to punishment\n",
    "        - trial and error search\n",
    "            - not told, must discover\n",
    "        - credit assignment problem\n",
    "            - delayed reward means that the agent must figure out which actions led to the reward\n",
    "        - exploration vs exploitation tradeoff\n",
    "            - exploit what you know to get the reward\n",
    "                - me at the UD creamery\n",
    "            - explore to find a better reward\n",
    "                - me when Taco Bell has a new online exclusive\n",
    "                - risks lesser or no reward\n",
    "        - policy space\n",
    "            - the space of all possible policies\n",
    "            - $|A|^{|S|}$ where A is the set of actions and S is the set of states\n",
    "                        \n",
    "                - e.g. 4 actions and 10 states $\\implies 4^{10}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda49a223d0753b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- examples of RL\n",
    "    - chess player\n",
    "        - move informed by planning and judgments of desirability\n",
    "    - preparing breakfast\n",
    "        - behavior + interlocking goal/subgoal relationships\n",
    "        - e.g. walk to cupboard, open, select, reach, grasp, retrieve, etc\n",
    "            - can be represented as a heirarchy of actions\n",
    "- applications\n",
    "    - Samuel's checkers player\n",
    "    - Tessauro's backgammon player (1992 - 1995)\n",
    "        - trained by playing against itself\n",
    "        - multilayered neural network\n",
    "        - learned to play at a world class level\n",
    "        - utilized features for the board space\n",
    "    - acrobot\n",
    "        - 2 jointed pendulum\n",
    "            - shoulder is motorized, elbow is free to move\n",
    "        - goal is to swing the pendulum \"hand\" above a certain height\n",
    "        - given a -1 reward for each time step until the goal is reached\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75588ce3ffdab9a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "http://incompleteideas.net/book/the-book-2nd.html\n",
    "- Sutton and Barto's book on RL\n",
    "    - free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6fe85c7e1c1df4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- learning to *do*\n",
    "    - state $\\implies$ action $\\implies$ reward\n",
    "    - goal is encoded as a reward passed from environment to agent\n",
    "    - agent must learn to maximize the cumulative reward\n",
    "    - e.g. pick and place robot\n",
    "        - states: arm angles and velocities\n",
    "        - actions: voltages to the motors\n",
    "        - rewards: +1 for successful pick and place, -$\\epsilon$ jerkiness of path\n",
    "    - e.g. learning to walk\n",
    "        - use reward proportional to distance traveled per unit time\n",
    "    - e.g. Unity Puppo\n",
    "        - dog simulation\n",
    "        - reward = $0.01 \\times \\vec{v} \\cdot \\vec{d} - 0.001 - 0.001 \\times \\theta + \\{ \\begin{array}{ll} 1 & \\text{if at target} \\\\ 0 & \\text{if not} \\end{array}$\n",
    "        - $\\vec{v}$ is the velocity of the dog\n",
    "        - \n",
    "    - discounting\n",
    "        - rewards received now are more valuable than rewards received in the future (at a discount)\n",
    "    - value function\n",
    "        - as previously discussed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba5e1a3533a835d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Q-Learning\n",
    "    - when you don't have a model, you can still identify action-value pairs\n",
    "    - Q(s,a) = expected reward of taking action a in state s\n",
    "        - V(s) = max_a Q(s,a)\n",
    "            - the value of a state is the maximum value of the actions that can be taken in that state\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f9dcd2b2db9b3",
   "metadata": {},
   "source": [
    "### Game Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5baa697fd41403",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- models of conflict and cooperation between rational decision makers\n",
    "    - created by von Neumann and Morgenstern in 1944\n",
    "    - has lead to 10 Nobel Prizes (mostly in economics)\n",
    "    - setup\n",
    "        - multi-player games with simultaneous actions\n",
    "        - main aspects\n",
    "            - agent design\n",
    "                - what is the bets strategy against a rational player?\n",
    "            - mechanism design\n",
    "                - define rules/protocols so collective good is achieved\n",
    "                    - e.g. TCP/IP\n",
    "        - elements\n",
    "            - players\n",
    "            - actions\n",
    "                - refers to an entire strategy or policy\n",
    "            - payoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ae546c9d36f5c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- e.g. Rock-Paper-Scissors\n",
    "    - a zero sum game\n",
    "        - the sum of the payoffs is zero for all possible outcomes\n",
    "        - for 2 player games, payoff for player 1 is the negative of the payoff for player 2\n",
    "        - can be generalized to just player 1's payoff as with previous examples\n",
    "            - e.g. player 1 gets 1, player 2 gets -1\n",
    "- e.g. Cake-Slicing \"Game\"\n",
    "    - to fairly split a dessert between 2 people\n",
    "    - one person cuts, the other chooses\n",
    "        - maximin strategy\n",
    "            - maximize the minimum payoff\n",
    "        - minimax strategy\n",
    "            - minimize the maximum payoff\n",
    "        - both strategies lead to a 50/50 split if both players are rational\n",
    "    - not a zero sum game\n",
    "        - there is one cake and there is no negative cake\n",
    "    - assumes that a bigger piece is preferred\n",
    "        - \"take bigger\" is then a dominant strategy\n",
    "    - there is a saddle point at 50/50\n",
    "        - a saddle point is a point where the best strategy for one player is the best strategy for the other player\n",
    "- e.g. Penny Matching Game\n",
    "    - do all 2 player games have a saddle point?\n",
    "    - the game is played as follows\n",
    "        - 2 players each place a penny on the table\n",
    "        - if pennies match, player 1 wins (+1) and player 2 loses (-1)\n",
    "        - if pennies don't match, player 2 wins (+1) and player 1 loses (-1)\n",
    "    - minimax\n",
    "        - min of the max is +1\n",
    "    - maximin\n",
    "        - max of the min is -1\n",
    "    - no saddle point\n",
    "        - minimax $\\ne$ maximin\n",
    "    - strategy\n",
    "        - play randomly\n",
    "            - both playing randomly does create a saddle point\n",
    "            - the expected value of the game is 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f14570c6eac2d",
   "metadata": {},
   "source": [
    "- solving a zero sum game\n",
    "    - using the penny matching game as an example\n",
    "    - assume players 1 and 2 use mixed strategies $R_p$ and $R_q$\n",
    "        - $R_p = {H, P(p), T(1-p)}$\n",
    "        - $R_q = {H, P(q), T(1-q)}$\n",
    "            - where H is heads, T is tails, and P is the probability of heads\n",
    "    - utility of player 1\n",
    "        - $U_1 = (p)(q) + (p)(1-q) + (1-p)(q) + (1-p)(1-q)$\n",
    "            - i.e. HH + HT + TH + TT\n",
    "    - find the saddle point\n",
    "        - find the partial derivatives of $U_1$ with respect to $p$ and $q$ and where they are equal to 0\n",
    "            - $\\frac{\\partial U_1}{\\partial p} = q - (1-q) + q + (q-1)$\n",
    "                    \n",
    "                - $= 4q - 2$\n",
    "                \n",
    "                - $= 0 \\implies q = 0.5$\n",
    "                            \n",
    "            - $\\frac{\\partial U_1}{\\partial q} = p + p - (1-p) - (1-p)$\n",
    "                    \n",
    "                - $= 4p - 2$\n",
    "                            \n",
    "                - $= 0 \\implies p = 0.5$            \n",
    "        - determine if the saddle point is a maximum or minimum\n",
    "            - $\\frac{\\partial^2 U_1}{\\partial p^2} = 4$\n",
    "            - $\\frac{\\partial^2 U_1}{\\partial q^2} = 4$\n",
    "            - $\\frac{\\partial^2 U_1}{\\partial p \\partial q} = \\frac{\\partial^2 U_1}{\\partial q \\partial p} = 0$\n",
    "            - determinant = 16 - 0 = 16\n",
    "            - since the determinant is positive and the second partial derivatives are positive, the saddle point is a minimum\n",
    "            - the Hessian matrix yields eigenvalues of -4 and 4\n",
    "                - since the eigenvalues are of opposing signs, this is a saddle point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4abb2e2532c59b",
   "metadata": {},
   "source": [
    "- Minimax Theorem (von Neummann, 1928)\n",
    "    - every finite, zero sum, two player game has a rational solution such that minimax = maximin\n",
    "        - i.e. the saddle point\n",
    "- Nash Equlibrium\n",
    "    - applies to non-zero sum games\n",
    "    - a point where no player can improve their payoff by changing their strategy\n",
    "        - assumes that the other player maintains their strategy\n",
    "    - a point where neither player has regret\n",
    "        - i.e. they are happy with their strategy\n",
    "        - no Monday morning quarterbacking\n",
    "    - tldr: neither player wants to change their strategy\n",
    "    - Existence Theorem (Nash, 1950)\n",
    "        - every finite game has at least one Nash equilibrium\n",
    "        - not necessarily unique, rational, or efficient\n",
    "        - equilibria may have \"strange or undesirable properties\" (Straffin)\n",
    "            - as defined by the game/society, not necessarily by the players\n",
    "    - positive example: the Driving Game\n",
    "        - 2 players driving towards each other on a 2 lane road\n",
    "        - each player can choose to drive on the left or right side of the road\n",
    "        - if both players choose the same side, they both get 1\n",
    "        - if they choose different sides, they both get 0\n",
    "        - the Nash equilibrium is for both players to choose the same side\n",
    "            - if one player changes their strategy, the other player will change their strategy to match\n",
    "            - the Nash equilibrium is for both players to drive on the same side of the road\n",
    "        - **$\\implies$ cooperation is the optimal strategy**\n",
    "    - negative example: the Prisoner's Dilemma\n",
    "        - 2 prisoners are arrested for a crime\n",
    "        - they are separated and offered a deal\n",
    "            - if one confesses and the other doesn't, the confessor goes free and the other gets 10 years\n",
    "            - if both confess, they both get 5 years\n",
    "            - if neither confess, they both get 1 year\n",
    "        - the Nash equilibrium is for both prisoners to betray\n",
    "        - **$\\implies$ betrayal is the optimal strategy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f6cec31554c78",
   "metadata": {},
   "source": [
    "- iterated games\n",
    "    - games that are played multiple times\n",
    "    - e.g. the Prisoner's Dilemma\n",
    "        - if the game is played once, the Nash equilibrium is to betray\n",
    "        - if the game is played multiple times, the Nash equilibrium may be to confess\n",
    "            - i.e. other factors and strategies may come into play\n",
    "        - meta strategies in iterated PD\n",
    "            - perpetual punishment\n",
    "                - cooperate until the other player betrays, then betray forever\n",
    "            - tit-for-tat\n",
    "                - cooperate until the other player betrays, then betray on the next move\n",
    "                - performs very well in competitions with large pools\n",
    "            - tit-for-two-tats\n",
    "                - cooperate until the other player betrays twice in a row, then defect on the next move\n",
    "    - e.g. Penny Game\n",
    "        - if the game is played once, the Nash equilibrium is to play randomly\n",
    "        - if you play over time you may find the other player is not perfectly random\n",
    "            - you can then exploit the other player's bias\n",
    "- Real world examples\n",
    "    - can be used to explain cooperation\n",
    "        - BitTorrent unchoking\n",
    "        - altruism in animal behavior\n",
    "        - WWI trench truces\n",
    "    - simulated societies\n",
    "        - \"Sugarscape\"\n",
    "    - issues\n",
    "        - limited (human) memory\n",
    "            - you can't remember everyone who ever cut you off\n",
    "        - adaptation over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b82ebb1c930c3",
   "metadata": {},
   "source": [
    "### 30Apr24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e6b4a8e0d32f1",
   "metadata": {},
   "source": [
    "- AI/ML/Data Science Pipeline\n",
    "    - data acquisition\n",
    "    - data processing\n",
    "    - data integration\n",
    "    - analytical modeling\n",
    "    - validation\n",
    "    - presentation\n",
    "- misc\n",
    "    - decision list\n",
    "        - decision tree with yes/no options at each layer\n",
    "            - if yes, classification\n",
    "            - if no, move to the next layer\n",
    "        - may require more data than a decision tree for training\n",
    "        - much more explainable than a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed600a40f3f6d6dd",
   "metadata": {},
   "source": [
    "### Simpler Machine Learning Models\n",
    "- Cynthia Rudin\n",
    "- CISE Distinguished Lecture Series video\n",
    "- possibly available on the NSF website   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4815c906704eccd",
   "metadata": {},
   "source": [
    "- goal is to increase the interpretability/explainability of machine learning models\n",
    "- many problems do not require or benefit from complex models\n",
    "    - e.g. 2HELPS2B\n",
    "        - an algorithm that can be calculated by hand or even in your head\n",
    "        - corresponds to 6 attributes in a healthcare scenario\n",
    "            - add up the values of the attributes\n",
    "            - score corresponds to a risk level\n",
    "    - for tabular data, neural networks generally are not better than simpler models\n",
    "        - e.g. decision trees, logistic regression, etc\n",
    "- why?\n",
    "    - one explanation is the Rashomon Set Theory\n",
    "        - there's no single best explanation but lots of good explanations\n",
    "        - when there are lots of \"almost optimal\" models, there will be simpler models in that set\n",
    "        - Roshomon Set: a set of models that are all about equally good for a given data set\n",
    "        - large Rashomon sets correlate with the existence of simpler models\n",
    "            - also correlate with many different methods being similarly effective\n",
    "        - extra features can end up being noise\n",
    "        - implication is that the urge to use complex models is not always necessary\n",
    "        - **simpler models are explainable and may be just as effective**\n",
    "        - e.g. FICO dataset of 10k loan applications\n",
    "            - loads of features\n",
    "            - FICO requested a black box model \n",
    "            - best black box accuracy was 73%\n",
    "                - boosted decision trees \n",
    "            - best UAC black box accuracy was 80%\n",
    "                - (2-layer neural network)\n",
    "            - Fast Sparse (not black box)\n",
    "                - uses 21 step features\n",
    "                - runs in ~ 3.85 seconds\n",
    "                - separates some variables into subgroups\n",
    "                    - e.g. age can be separated into groups of \"age > 25\" and \"age > 35\", etc\n",
    "                - model is explainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e501cfa2900e7",
   "metadata": {},
   "source": [
    "- Optimal Sparse Decision Trees\n",
    "    - NP hard problem to find the optimal sparse decision tree\n",
    "        - factorial of the number of features\n",
    "    - GOSDT\n",
    "        - Greedy Optimal Sparse Decision Tree\n",
    "        - the goal is to find the best decision tree with the fewest number of nodes\n",
    "        - the best feature to split on is based on the next best feature to split on which is based on the next best feature to split on, etc\n",
    "            - each subproblem is represented as a binary vector\n",
    "            - creates a giant dependency graph\n",
    "            - use theorems to prune the graph\n",
    "            - propagate bounds up the tree\n",
    "                - sort of like alpha-beta pruning\n",
    "        - when used on FICO dataset\n",
    "            - 10k data points\n",
    "            - > 1900 binary features\n",
    "            - 10 leaves\n",
    "            - accuracy of 71.7%\n",
    "            - ~8.1 seconds to run\n",
    "            - roughly on par with the black box models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d087d4704504579d",
   "metadata": {},
   "source": [
    "- why everything in the current ML paradigm is wrong\n",
    "    - recommendation: hand decision as to which model to use to the user\n",
    "        - i.e. give them the whole Roshamon Set of good enough models\n",
    "    - TreeFARMS\n",
    "        - returns all almost optimal trees\n",
    "        - TimberTrek\n",
    "            - a GUI that allows the user to explore the trees in the Roshamon Set returned by TreeFARMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0637364c35d07",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10e20900e0f3a4aa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
