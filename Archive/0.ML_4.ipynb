{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbde6cdebfe3fddd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Unsupervised Learning\n",
    "-  aim is to learn a mapping from the input space to an output space\n",
    "    - no pre-defined labels for the data\n",
    "        - i.e. just data, no \"correct\" answers to test against\n",
    "    - the model determines what features are important\n",
    "        - i.e. it is deciding how to group the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97970646fa6ee4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### k-Means Clustering\n",
    "- split data into k clusters\n",
    "    - k is a hyperparameter\n",
    "    - first, find the centroid of a group of points and determine the distance of each point from the centroid\n",
    "    - then group the points by the closest centroid\n",
    "    - i.e. find the k centroids that minimize the distance between the grouped points and their respective centroids \n",
    "- variety of algorithms for moving centroids to new locations\n",
    "    - basic idea is to monitor the average distance of points from their centroids\n",
    "    - move around until the average distance is minimized and stops decreasing\n",
    "#### Reconstruction Error\n",
    "- used to determine the optimal number of clusters (k)\n",
    "- it is the sum of squared distances $x^t$ between each point and its centroid $m_i$\n",
    "    - Reconstruction Error $= \\sum_{t} \\sum_{i} ||x^t - m_i||^2$ \n",
    "- the best k is usually found at the \"elbow\" of the graph of the reconstruction error\n",
    "    - i.e. the point where the error stops decreasing as much\n",
    "    - <img src=\"images/recoerror.png\">\n",
    "#### Non-Deterministic\n",
    "- k-means clustering is non-deterministic\n",
    "    - i.e. it can give different results each time it is run\n",
    "- mitigation\n",
    "    - run the algorithm multiple times and choose the best result\n",
    "        - about 100 times usually works\n",
    "    - alternatively, use a uniform distribution of initial centroids\n",
    "        - this may miss a more optimal solution\n",
    "#### k-Means in Python\n",
    "- scikit-learn supports k-Means with `sklearn.cluster.Kmeans`\n",
    "    - `init` specifies the method for initializing the centroids\n",
    "        - `random` randomly selects k points from the data\n",
    "        - `k-means++` is a more sophisticated method\n",
    "    - `n_init` specifies the number of times to run the algorithm\n",
    "        - the best result is returned\n",
    "    - `inertia` is the reconstruction error\n",
    "#### Summary\n",
    "- advantages\n",
    "    - simple\n",
    "    - available in most libraries\n",
    "    - scales well\n",
    "    - applicable to many fields and problems\n",
    "- disadvantages\n",
    "    - requires k to be specified\n",
    "    - assumes that clusters are spherical in shape\n",
    "    - sensitive to outliers\n",
    "    - \"curse of dimensionality\"\n",
    "        - i.e. the more dimensions, the more data is needed to get good results\n",
    "        - PCA can help with this\n",
    "    - will always converge, but may be to a local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24159bbe54424d99",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cb00cfebb07d37",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f23e6ea944bc1d7d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### IC 26Oct23\n",
    "- 1. \n",
    "        - distance((1, 4, 8) -> (3, 8, 2))\n",
    "        - $d = \\sqrt{(1-3)^2 + (4-8)^2 + (8-2)^2}$\n",
    "        - $d = \\sqrt{4 + 16 + 36}$\n",
    "        - $d = \\sqrt{56}$\n",
    "        - $d = 7.483$\n",
    "- 2. \n",
    "        - centroid_1((2, 3, 4), (1, 6, 3), (2 ,1 ,5))\n",
    "        - $c_1 = (\\frac{2+1+2}{3}, \\frac{3+6+1}{3}, \\frac{4+3+5}{3})$\n",
    "        - $c_1 = (\\frac{5}{3}, \\frac{10}{3}, \\frac{12}{3})$\n",
    "        - $c_1 = (1.667, 3.333, 4)$\n",
    "        - centroid_2((6, 3, 5), (9, 8, 7), (7, 2, 6)) \n",
    "        - $c_2 = (\\frac{6+9+7}{3}, \\frac{3+8+2}{3}, \\frac{5+7+6}{3})$\n",
    "        - $c_2 = (\\frac{22}{3}, \\frac{13}{3}, \\frac{18}{3})$\n",
    "        - $c_2 = (7.333, 4.333, 6)$\n",
    "- 3.\n",
    "        - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d003a0bf0bfef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Syllabification\n",
    "- common problem in applied linguistics\n",
    "- goal is to split a word into syllables\n",
    "    - i.e. \"syllabification\"\n",
    "- phones\n",
    "    - about 150 sounds which the human vocal tract can produce\n",
    "        - english uses about 44-62\n",
    "    - any word can be represented as a sequence of phones\n",
    "- sonority scale\n",
    "    - a ranking of speech phones by intensity from least to most sonorous\n",
    "        - here we will use a scale of 1-13\n",
    "        - i.e. how loud they are\n",
    "        - vowels are the most sonorous because they have the most vibration\n",
    "        - consonants are less sonorous because they have less vibration\n",
    "- sonority sequencing principle\n",
    "    - one of several theories on how words are split into syllables\n",
    "    - suggests that the middle of a syllable is the sonority apex (nucleus)\n",
    "        - usually a vowel\n",
    "    - can be applied to an audio file\n",
    "        - represent phones as a sequence of their intensities between 1 and 13\n",
    "        - how do you determine the exact place to separate the words based off of these numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32dfc439e7e5db8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Syllabification Problem Statement\n",
    "- need a set of rules to divide phones into syllables\n",
    "    - i.e. a syllabification algorithm\n",
    "    - rules are like look-up tables showing a pattern and the corresponding action\n",
    "        - e.g. if you have the pattern [13 12], split it into [13] & [12]\n",
    "        - e.g. if you have the pattern [13 5 10], split it into [13] & [5 10]\n",
    "- what if not in the table?\n",
    "    - find the closest matching pattern from the rules we do have\n",
    "        - uses euclidean distance\n",
    "        - <img src=\"images/syllabification.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44751be86c29a061",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Syllable Alignment Error\n",
    "- the metric used to evaluate the accuracy of the syllabification algorithm\n",
    "    - an ugly, ugly function\n",
    "- compare the machine classified syllables to the human classified syllables\n",
    "    - calculate how much time each machine classified syllable overlaps with each human classified syllable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a62e39aefde7d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Syllabification Objective Function\n",
    "- $min_{e(r)} S(R,X)$\n",
    "    - $R$ is the set of rules\n",
    "    - r is a rule\n",
    "    - $e(r)$ is the error of a rule\n",
    "    - X is a set of utterances\n",
    "    - $S(R,X)$ is a function which takes R and X and returns a set of utterances broken into variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8864d883c054055",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Syllabification Genetic Algorithm\n",
    "- often use the TIMIT corpus to train the algorithm\n",
    "    - 6300 utterances\n",
    "    - 8 different dialects\n",
    "    - 630 different speakers\n",
    "    - speach is made up of\n",
    "        - dialect sentences to identify dialect\n",
    "        - phonetically rich sentences to identify common phonemes\n",
    "        - phonetically compact sentences to include interesting and problematic phoneme combinations \n",
    "    - also includes manually classified beginnings and endings of syllables and phones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a7f088a6e77ca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### IC 09Nov23\n",
    "- a. euclidean distance from [9 8 9] to [10 2 10] and [10 7 10]\n",
    "    - $d_1 = \\sqrt{(9-10)^2 + (8-2)^2 + (9-10)^2}$\n",
    "    - $d_1 = \\sqrt{1 + 36 + 1}$\n",
    "    - $d_1 = \\sqrt{38}$\n",
    "    - $d_1 \\approx 6.164$\n",
    "    - $d_2 = \\sqrt{(9-10)^2 + (8-7)^2 + (9-10)^2}$\n",
    "    - $d_2 = \\sqrt{1 + 1 + 1}$\n",
    "    - $d_2 = \\sqrt{3}$\n",
    "    - $d_2 \\approx 1.732$\n",
    "- b. [9] [8 9] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b2f6b89fd36417",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Temporal Difference Learning\n",
    "- a form of reinforcement learning\n",
    "    - learning by trial and error\n",
    "    - i.e. learning by doing\n",
    "- Off-Policy\n",
    "    - trial and error\n",
    "- On-Policy\n",
    "    - learning from instruction\n",
    "    - follow the policy and learn about it at the same time\n",
    "- Q-Learning\n",
    "    - a form of off-policy learning\n",
    "    - uses a Q-table to store the value of each action in each state\n",
    "        - Q-table is initialized to 0\n",
    "        - Q-table is updated after each action\n",
    "        - Q-table is used to determine the next action\n",
    "    - e.g. robot in a house\n",
    "    <img src=\"images/qlearnstate.png\">\n",
    "\n",
    "    - use the state table to represent the options\n",
    "        - assign doors that lead outside a value of 100\n",
    "        - all other doors are assigned a value of 0\n",
    "    <img src=\"images/qlearnstaterewards.png\">\n",
    "\n",
    "    - create a rewards matrix and a Q matrix\n",
    "        - rewards matrix is the same as the state matrix\n",
    "        - Q matrix is initialized to 0 and is updated after each action\n",
    "        - Q matrix is like the brain or memory of the robot\n",
    "    - transition rule is dictated by Q(state, action) = R(state, action) + $\\gamma$ Max[Q(next state, all actions)]\n",
    "        - $\\gamma$ is the discount factor\n",
    "            - it is a value between 0 and 1\n",
    "            - it is used to determine how much to discount future rewards\n",
    "            - i.e. how much to discount the Max[Q(next state, all actions)]\n",
    "            - if $\\gamma$ is 0, then the robot will only consider the immediate reward\n",
    "            - if $\\gamma$ is 1, then the robot will consider all future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06af629d59d243",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Q-Learning Algorithm\n",
    "- initialize Q-table to 0\n",
    "- for each episode\n",
    "    - select random initial state\n",
    "    - while state is not terminal\n",
    "        - randomly select an action\n",
    "        - consider taking this action\n",
    "        - get maximum Q-value for this state based on all possible actions\n",
    "        - compute Q(state, action)\n",
    "        - set the next state to the current state\n",
    "- e.g. initial state of 1 & $\\gamma$ = 0.8\n",
    "    <img src=\"images/qlearnroom1.png\">\n",
    "\n",
    "    - options are go to state 3 or state 5\n",
    "        - random selection\n",
    "    - choose 5\n",
    "    - calculate Q(1,5)\n",
    "        - R(1,5) = 100\n",
    "        - max[Q(next state, all actions)] = max[Q(5,1), Q(5,4), Q(5,5)] = Max[0, 0, 0] = 0\n",
    "        - Q(1,5) = 100 + 0.8 * 0 = 100\n",
    "    - append Q(1,5) to Q-table\n",
    "- e.g. initial state of 3 & $\\gamma$ = 0.8\n",
    "    - options are go to state 1, 2, or state 4\n",
    "        - random selection\n",
    "    - choose 1\n",
    "    - calculate Q(3,1)\n",
    "        - R(3,1) = 0\n",
    "        - max[Q(next state, all actions)] = max[Q(1,3), Q(1,5)] = Max[0, 100] = 100\n",
    "        - Q(3,1) = 0 + 0.8 * 100 = 80\n",
    "    - append Q(3,1) to Q-table\n",
    "- after many iterations, the Q-table will start to converge\n",
    "    <img src=\"images/qlearntable.png\">\n",
    "\n",
    "    - convergence can be defined as when the Q-table stops changing\n",
    "        - because the algorithm is exploratory, it may be worth continuing after the first identical iteration\n",
    "            - two iterations may not differ, but the third may differ significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd784c238e7c1d4f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Q-Learning vs Monte Carlo\n",
    "- Q-Learning can learn ***before*** knowing the final outcome\n",
    "    - can learn every step\n",
    "    - monte carlo can only learn at the end of the episode\n",
    "- Q-Learning can learn ***without*** the final outcome\n",
    "    - Q-Learning can learn from incomplete episodes\n",
    "    - monte carlo can only learn from complete episodes\n",
    "    - Q-Learning works in continuous (non-terminating) environments\n",
    "        - e.g. a robot in a house\n",
    "    - monte carlo only works in episodic (terminating) environments\n",
    "        - e.g. a robot in a maze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808575301fe07f7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-\n",
    "<img src=\"images/ICP_28Nov.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce045efc1e2445",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### IC 28Nov23\n",
    "- initial state of 2 \n",
    "- options are go to state 3\n",
    "    - no random selection\n",
    "- calculate Q(2,3)\n",
    "    - R(2,3) = 0\n",
    "    - max[Q(next state, all actions)] = max[Q(3,1), Q(3,2), Q(3,4)] = Max[80, 0, 0] = 80\n",
    "    - Q(2,3) = 0 + 0.8 * 80 = 64\n",
    "- append Q(2,3) to Q-table\n",
    "- - - - - - - - \n",
    "- new state is 3\n",
    "- options are go to state 1 or state 2 or state 4\n",
    "    - random selection\n",
    "- go to state 1\n",
    "- calculate Q(3,1)\n",
    "    - R(3,1) = 0\n",
    "    - max[Q(next state, all actions)] = max[Q(1,3), Q(1,5)] = Max[0, 100] = 100\n",
    "    - Q(3,1) = 0 + 0.8 * 100 = 80\n",
    "- append Q(3,1) to Q-table\n",
    "- - - - - - - - \n",
    "- new state is 1\n",
    "- options are go to state 3 or state 5\n",
    "    - random selection\n",
    "- go to state 5\n",
    "- calculate Q(1,5)\n",
    "    - R(1,5) = 100\n",
    "    - max[Q(next state, all actions)] = max[Q(5,1), Q(5,4), Q(5,5)] = Max[0, 0, 0] = 0\n",
    "    - Q(1,5) = 100 + 0.8 * 0 = 100\n",
    "- append Q(1,5) to Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q-Learning vs SARSA\n",
    "- SARSA is an on-policy algorithm\n",
    "    - it learns from the policy it is following\n",
    "    - it learns ***after*** knowing the final outcome\n",
    "    - it learns ***with*** the final outcome\n",
    "    - it learns ***from*** the final outcome\n",
    "- in Q-Learning\n",
    "    - start in a random state\n",
    "    - update value estimate using a function\n",
    "    - choose next action randomly\n",
    "- in SARSA\n",
    "    - still start in a random state\n",
    "    - update value estimate using a function\n",
    "    - choose next action using the policy\n",
    "        - if the values are the same, then choose randomly"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2799559d437e7831"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Differences\n",
    "- Q-learning learns the optimal policy over time\n",
    "- SARSA learns a near-optimal policy over time\n",
    "- SARSA is mover conservative than Q-learning\n",
    "    - SARSA is less likely to take risks\n",
    "    - once it finds a good policy, it will stick with it\n",
    "        - may not be the best policy\n",
    "- avoids \"the cliff\"\n",
    "    - Q-learning may fall off the cliff\n",
    "    - SARSA will not fall off the cliff\n",
    "- SARSA is a greedy algorithm\n",
    "    - can get stuck in a local minimum\n",
    "- Q-learning tries to jump out of local minimums\n",
    "    - may fall off the cliff\n",
    "### Epson-Greedy\n",
    "- you can have both Greedy and Exploration\n",
    "- choose the best action with probability 1 - $\\epsilon$\n",
    "- choose a random action with probability $\\epsilon$\n",
    "- $\\epsilon$ is a hyperparameter\n",
    "    - usually between 0.1 and 0.3\n",
    "    - usually decreases over time\n",
    "        - i.e. start with a high $\\epsilon$ and decrease it over time\n",
    "        - this is called \"annealing\"\n",
    "    <img src=\"images/cliff_walking.png\">              \n",
    "\n",
    "    - blue path is the Episilon-Greedy path\n",
    "- sometimes you are on policy and sometimes off\n",
    "- the $\\epsilon$ variable is the probability of being off-policy\n",
    "    - i.e. the probability of being random\n",
    "- algorithm\n",
    "    <img src=\"images/epsilongreedy.png\">\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa77debe2a8932b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decaying Epsilon-Greedy\n",
    "- similar to regular epsilon-greedy\n",
    "- as you do more episodes, the epsilon value decreases\n",
    "    - become less exploratory and more conservative\n",
    "- a typical decay function is $\\epsilon = \\frac{t-1}{c}$\n",
    "    - t is the current episode\n",
    "    - c is a constant which determines how quickly epsilon decays"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba2eac591f607524"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploration vs Exploitation\n",
    "- the Multi-Armed Bandit Problem\n",
    "    - MAB is a theoretical slot machine with k levers\n",
    "    - action is chosen to pull a lever and get a reward\n",
    "    - reward is a random value from a normal distribution\n",
    "    - the goal is to maximize the reward\n",
    "    \n",
    "- <img src=\"images/MAB.png\">\n",
    "\n",
    "    - reward is maximized by choosing the right-most distribution, #7 in this case\n",
    "- simulation of choices\n",
    "    <img src=\"images/MAB_simulation.png\">\n",
    "    <img src=\"images/MAB_simulation_results.png\">\n",
    "\n",
    "    - greedy algorithm chooses the same lever every time\n",
    "    - epsilon keeps choosing random levers sometimes\n",
    "    - decaying epsilon chooses random levers less and less over time\n",
    "    - run to run results will likely differ\n",
    "        - there is always a chance that the greedy algorithm will choose the best lever first\n",
    "            - i.e. it will get lucky and the local minimum it finds will be the global minimum"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0bb910633b9cfb5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IC 30Nov23\n",
    "- 1: \n",
    "    - initial state of 2\n",
    "        - options are go to state 3\n",
    "            - no Q-table selection\n",
    "            - no random selection\n",
    "    - calculate Q(2,3)\n",
    "        - R(2,3) = 0\n",
    "        - max[Q(next state, all actions)] = max[Q(3,1), Q(3,2), Q(3,4)] = Max[80, 0, 0] = 80\n",
    "        - Q(2,3) = 0 + 0.8 * 80 = 64\n",
    "    - append Q(2,3) to Q-table\n",
    "- 2: episode = 8, c = 100, random number = 0.76\n",
    "    - a: randomly\n",
    "    - b: because 0.76 is greater than 0 and less than 1 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "341a7a6257547cf7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IC 05Dec23\n",
    "- 1: Random\n",
    "- 2: the random number is less than $\\epsilon$\n",
    "- 3: \n",
    "    - mean$(\\frac{1}{2}(8-9)^2 , \\frac{1}{2}(4-5)^2) =$ mean$( \\frac{1}{2} , \\frac{1}{2}) = 0.5$\n",
    "    - mean$(\\frac{1}{2}(7-6)^2 , |2-4| - \\frac{1}{2}1^2) =$ mean$( \\frac{1}{2} , 1.5) = 1$ \n",
    "    - mean$(\\frac{1}{2}(4-3)^2 , |3-6| - \\frac{1}{2}1^2) =$ mean$( \\frac{1}{2} , 2.5) = 1.5$\n",
    "    - mean$(\\frac{1}{2}(9-8)^2 , \\frac{1}{2}(8-8)^2) =$ mean$( \\frac{1}{2} , 0) = 0.25$\n",
    "    - mean$(\\frac{1}{2}(3-3)^2 , |3-9| - \\frac{1}{2}1^2) =$ mean$( 0 , 5.5) = 2.75$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cc3d592a8ec1d72"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c0a76a98c3e70fe3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
