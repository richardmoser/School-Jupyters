{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "769ead2f15d707aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Knowledge Representation & Propositional Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b7aeb91c0d463",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Logical Agents\n",
    "- things that happen in the world aka semantics\n",
    "    - data/sensing\n",
    "    - actions/new conditions\n",
    "    - it can be said that actions/conditions follow from data/sensing\n",
    "- representations aka syntax\n",
    "    - formally represented facts\n",
    "    - via reasoning, new facts can be derived or inferred\n",
    "- data sensing -> represented facts -> inferred facts -> actions/conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d270bcd2632973b4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Knowledge Bases\n",
    "- some examples\n",
    "    - general rules\n",
    "        - small AND flying -> edible (for a frog)\n",
    "    - common sense\n",
    "        - looks like a duck AND quacks like a duck -> it is a duck\n",
    "    - negative information\n",
    "        - NOT keys are on the counter (if I'm looking for my keys)\n",
    "    - incomplete information\n",
    "        - keys are on the counter OR keys are in the car\n",
    "### Propositional Logic\n",
    "- you might see this in a philosophy class\n",
    "- a formal language for representing knowledge\n",
    "- semantics associate elements/propositions with truth values in the world\n",
    "- inference rules\n",
    "- construction\n",
    "    - consistes of well-formed formulas (wffs)\n",
    "        - atoms\n",
    "            - True, False\n",
    "            - P, Q, R, ...\n",
    "                - \"propositional symbols\" which represent statements\n",
    "                - e.g. P = \"it is raining\"\n",
    "        - grouping\n",
    "            - (P AND Q)\n",
    "        - negation\n",
    "            - NOT P\n",
    "        - AND or OR\n",
    "            - (P AND Q) OR (R AND S)\n",
    "        - implication\n",
    "            - P => Q\n",
    "                - p is the antecedent and q is the consequent (or conclusion)\n",
    "        - biconditional, iff\n",
    "            - P <=> Q\n",
    "        - literal\n",
    "            - an atom or its negation\n",
    "            - e.g. P, NOT P\n",
    "- e.g. of wffs\n",
    "    - P\n",
    "    - NOT NOT P\n",
    "    - P AND Q\n",
    "    - (P AND Q) => R\n",
    "- e.g. of non-wffs\n",
    "    - P AND\n",
    "    - P NOT Q\n",
    "- models\n",
    "    - fixes the truth value of each proposition\n",
    "        - e.g. P = True, Q = False\n",
    "        - usually represented as a truth table\n",
    "        - each row of the table or defined arrangement is a model\n",
    "- partial models\n",
    "    - only some of the propositions are assigned truth values\n",
    "    - e.g. P = True, Q = ?\n",
    "    - can be used to represent incomplete information\n",
    "    - may be consistent with multiple defined models\n",
    "- semantics\n",
    "    - you can define the truth value of a wff in terms of the truth values of its subformulas\n",
    "- equivalence\n",
    "    - two wffs are equivalent if they have the same truth value in every model\n",
    "    - e.g. P AND Q is equivalent to Q AND P\n",
    "    - the symbol $\\equiv$ is used to denote equivalence\n",
    "- tautology\n",
    "    - a wff that is true in every model\n",
    "    - e.g. P OR NOT P\n",
    "- unsatisfiable\n",
    "    - a wff that is false in every model\n",
    "    - e.g. P AND NOT P\n",
    "- satisfiable\n",
    "    - a wff that is true in at least one model\n",
    "    - e.g. P OR Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5de111c034919",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- logical combinatorics\n",
    "    - given a sentence and a universe, in how many models is the sentence true?\n",
    "        - i.e. how many rows in the truth table have the sentence true?\n",
    "        - e.g. in U = {P, Q}\n",
    "            - P is true in 2 models\n",
    "            - P AND Q is true in 1 model\n",
    "        - e.g. in U = {P, Q, R, S}\n",
    "            - P AND Q is true in 4 models\n",
    "                - one combination of P and Q is true and this occurs in 2 models of R and 2 models of S\n",
    "                - 1 * 2 * 2 = 4\n",
    "            - P OR Q is true in 12 models\n",
    "                - 3 * 2 * 2 = 12\n",
    "                - or you can think of it as 16 - 4 = 12\n",
    "                    - 16 total models - 4 models where P OR Q is false\n",
    "        - e.g. |U| = n\n",
    "            - $2^n$ models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad393facee5bec71",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- inference\n",
    "    - $W_1 \\Rightarrow W_1 \\lor W_2$  \n",
    "    - $W_1 \\land W_2 \\Rightarrow W_1$\n",
    "    - $W_1, W_2 \\Rightarrow W_1 \\land W_2$\n",
    "        - the comma is used to denote a set of wffs that are known to be true\n",
    "    - $\\lnot \\lnot W_1 \\Rightarrow W_1$\n",
    "    - modus ponens\n",
    "        - $W_1, W_1 \\Rightarrow W_2 \\therefore W_2$\n",
    "- proofs\n",
    "    - using inference rules to derive new wffs from old ones\n",
    "    - knowledge base is represented as $\\Delta$\n",
    "        - $\\Delta = \\{P, R, P \\Rightarrow Q\\}$\n",
    "        \n",
    "            - the wffs in $\\Delta$ are known to be true\n",
    "    - $\\Delta = \\{BATTERY\\_OK \\land LIFTABLE \\Rightarrow ARM\\_MOVES\\}$\n",
    "    \n",
    "    - $\\Delta = \\{B, \\lnot M, B \\land L \\Rightarrow M\\}$\n",
    "        \n",
    "        1. $B$, given\n",
    "        2. $\\lnot M$, given\n",
    "        3. $B \\land L \\Rightarrow M$, given\n",
    "        4. $B \\land L$, given\n",
    "        5. ...\n",
    "- properties of sets of inference rules\n",
    "    - soundness: proofs are valid\n",
    "        - everything you can prove is actually true\n",
    "        - constructive definition:\n",
    "            - W is True under all models in which $\\Delta$ is true\n",
    "                - we will revisit this later\n",
    "    - completeness: everything that is true can be proven\n",
    "        - everything that is true is derivable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701a7a172ac3494",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Resolution\n",
    "    - $P \\lor Q, \\lnot P \\lor R \\therefore Q \\lor R$\n",
    "        \n",
    "        - resolution on P\n",
    "    - $R, \\lnot R \\lor P \\therefore P$\n",
    "        \n",
    "        - unit resolution on R\n",
    "            - unit resolution is a special case of resolution where one of the clauses is a unit clause\n",
    "                - a unit clause is a clause with only one literal\n",
    "    - $P \\lor Q \\lor R \\lor S, \\lnot P \\lor Q \\lor W \\therefore Q \\lor R \\lor S \\lor W$\n",
    "        \n",
    "        - i.e. you can combine the two clauses and remove the P literal\n",
    "    - $P \\lor Q \\lor \\lnot R, P \\lor W \\lor \\lnot Q \\lor R \\therefore P \\lor \\lnot R \\lor W \\lor R$\n",
    "        \n",
    "        - resolution on Q\n",
    "        - this statement is always true\n",
    "        - could also resolve on R\n",
    "            - $\\therefore P \\lor \\lnot Q \\lor W \\lor \\lnot Q$\n",
    "            \n",
    "            - this statement is also always true\n",
    "        - can you perform resolution on both simultaneously?\n",
    "            - no, you can only resolve on one literal at a time\n",
    "            - result is $P \\lor Q$ which is not always true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c794d8c13ee7d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 27FEB2024\n",
    "- resolution continued\n",
    "    -  $P \\lor Q, \\lnot P \\lor R \\therefore Q \\lor R$\n",
    "        \n",
    "        - resolution on P\n",
    "        - if P is true, then $\\lnot$ P is false\n",
    "            - thus R is true\n",
    "        - if P is false, then Q is true\n",
    "            - thus Q is true\n",
    "        - either way, Q $lor$ R is true\n",
    "        - this is a sound inference rule\n",
    "        - can be shown with a truth table\n",
    "- resolution in knowledge bases in conjunctive normal form (CNF)\n",
    "    - can be abbreviated\n",
    "        - $(R \\lor P) \\land (Q \\lor R)$ can be written as $R \\lor P, Q \\lor R$ \n",
    "- any wff can be converted to CNF\n",
    "    - e.g $\\lnot(P \\rightarrow Q) \\lor (R \\rightarrow P)$\n",
    "    1. eliminate biconditionals\n",
    "        - $ \\alpha \\Leftrightarrow \\beta$ is equivalent to $(\\alpha \\Rightarrow \\beta) \\land (\\beta \\Rightarrow \\alpha)$ \n",
    "        - e.g. $\\lnot(P \\rightarrow Q) \\lor (R \\rightarrow P)$ becomes\n",
    "            - $(\\lnot(\\lnot P \\lor Q) \\lor (\\lnot R \\lor P))$\n",
    "    2. eliminate implications\n",
    "        - $\\alpha \\Rightarrow \\beta$ is equivalent to $\\lnot \\alpha \\lor \\beta$\n",
    "        - e.g. $(\\lnot(\\lnot P \\lor Q) \\lor (\\lnot R \\lor P))$ becomes\n",
    "            - $(P \\land \\lnot Q) \\lor (\\lnot R \\lor P)$\n",
    "    3. move negations inwards with DeMorgan's laws\n",
    "        - $\\lnot(\\alpha \\land \\beta)$ is equivalent to $\\lnot \\alpha \\lor \\lnot \\beta$\n",
    "        - $\\lnot(\\alpha \\lor \\beta)$ is equivalent to $\\lnot \\alpha \\land \\lnot \\beta$\n",
    "        - e.g. $(P \\land \\lnot Q) \\lor (\\lnot R \\lor P)$ becomes\n",
    "            - $(P \\lor \\lnot \\lnot Q) \\land (\\lnot \\lnot R \\lor P)$\n",
    "            - $(P \\lor Q) \\land (R \\lor P)$\n",
    "    4. distribute OR over AND\n",
    "        - $\\alpha \\lor (\\beta \\land \\gamma)$ is equivalent to $(\\alpha \\lor \\beta) \\land (\\alpha \\lor \\gamma)$\n",
    "        - e.g. $(P \\lor Q) \\land (R \\lor P)$ becomes\n",
    "            - $((\\lnot R \\lor P) \\lor P) \\land ((\\lnot R \\lor P) \\lor \\lnot Q)$\n",
    "            - $(\\lnot R \\lor P \\lor P) \\land (\\lnot R \\lor P \\lor \\lnot Q)$\n",
    "            - $(\\lnot R \\lor P) \\land (\\lnot R \\lor P \\lor \\lnot Q)$\n",
    "            - in CNF: $\\lnot R \\lor P, \\lnot R \\lor P \\lor \\lnot Q$\n",
    "- resolution algorithm\n",
    "    - to probe $KB \\therefore \\alpha$, show that $\\lnot \\alpha \\lor KB$ is unsatisfiable\n",
    "    1. convert $KB \\land \\lnot \\alpha$ to CNF\n",
    "    2. successively resolve until\n",
    "        - no new clauses can be inferred\n",
    "            - $ KB \\not \\rightarrow \\alpha$\n",
    "        - the empty clause is inferred\n",
    "            - the empty clause is unsatisfiable\n",
    "            - $ KB \\rightarrow \\alpha$\n",
    "    - e.g. rover from past lecture\n",
    "        - KB: $B \\land L \\Rightarrow M, B, \\lnot M$\n",
    "        1. convert: \n",
    "            - $\\lnot (B \\land L) \\lor M, B, \\lnot M$\n",
    "            - $\\lnot B \\lor \\lnot L \\lor M, B, \\lnot M$\n",
    "            - set $\\alpha = \\lnot L$ and $\\lnot \\alpha = L$\n",
    "        2. resolve:\n",
    "            - $\\lnot M$ and $\\lnot B \\lor \\lnot L \\lor M$ can become $\\lnot B \\lor \\lnot L$\n",
    "            - $B$ and $\\lnot B \\lor \\lnot L$ can become $\\lnot L$\n",
    "            - $\\lnot L$ and $L$ can become the empty clause\n",
    "                - thus, $KB \\rightarrow \\alpha$\n",
    "                - the rock is not liftable\n",
    "        - could be written as a tree almost. Start with initial clauses and draw lines to new clauses that are inferred\n",
    "            - each successive line is a resolution step and the width reduces as you go down the tree\n",
    "            - eventually, you will reach the empty clause or no new clauses can be inferred\n",
    "                - i.e. either the empty clause is inferred and the statement is unsatisfiable or no new clauses can be inferred and the statement is satisfiable\n",
    "- since it is tree solveable, you can employ search algorithms\n",
    "    - resolving with unit clauses first is a good strategy\n",
    "        - dramatically reduces the search space and complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bf2ef3134e2bba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- DPLL (Davis-Putnam-Logemann-Loveland) algorithm\n",
    "    - uses techniques in CSPs\n",
    "        - degree heuristic\n",
    "        - backtracking\n",
    "        - random restarts\n",
    "        - clever indexing\n",
    "            - cache set of clauses in which X$_i$ appears\n",
    "- satisfiability problems\n",
    "    - e.g. WALKSAT\n",
    "        - pick an unsatisfied clause\n",
    "        - flip a random variable in the clause\n",
    "            - either at random or min conflict (minimizes the number of unsatisfied clauses after the flip)\n",
    "        - runs faster than DPLL on some problems\n",
    "        - may not always find a solution even though one may exist\n",
    "            - generally, run WALKSAT for a while and then run DPLL if WALKSAT fails in the time limit\n",
    "- restrict the type of problem\n",
    "    - Horn clauses\n",
    "        - at most one positive literal\n",
    "        - e.g. $P \\land Q \\Rightarrow R$\n",
    "        - if the knowledge base is made up of Horn clauses, then the problem can be solved in linear time\n",
    "            - e.g. $KB \\land \\lnot \\alpha$\n",
    "                - if $\\alpha$ is a Horn clause, then DPLL can be solved in linear time\n",
    "                - if $\\alpha$ is a Horn clause, then WALKSAT can be solved in linear time\n",
    "- forward and backward chaining\n",
    "    - efficient for Horn clauses\n",
    "    - forward chaining\n",
    "        - start with known facts\n",
    "        - repeatedly apply modus ponens\n",
    "        - stop when no new facts can be inferred\n",
    "    - backward chaining\n",
    "        - start with the goal\n",
    "        - repeatedly apply modus ponens\n",
    "        - stop when no new facts can be inferred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38740f2c929637e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- caching\n",
    "    - store the results of previous inferences in something"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770b956a171cd03",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c40553e28d0fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T21:39:23.322729100Z",
     "start_time": "2024-02-27T21:39:21.189532900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9227465\n",
      "CPU times: total: 1.7 s\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "fibnum = 35\n",
    "def fibonacci(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "%time print(fibonacci(fibnum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1619ed49091c7480",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T21:39:25.305875700Z",
     "start_time": "2024-02-27T21:39:23.322370200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9227465\n",
      "CPU times: total: 1.77 s\n",
      "Wall time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "fcache = []\n",
    "def man_cache_fibonacci(n):\n",
    "    if n < len(fcache):\n",
    "        return fcache[n]\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        fcache\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "%time print(fibonacci(fibnum))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9353674cf93211f8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "via function decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64510d176209e22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T21:39:56.245001700Z",
     "start_time": "2024-02-27T21:39:56.235871600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9227465\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 999 Âµs\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "@lru_cache(maxsize=None)\n",
    "def fib_cache(n):\n",
    "    if n == 0: \n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return fib_cache(n-1) + fib_cache(n-2)\n",
    "    \n",
    "%time print(fib_cache(fibnum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf205d772cb8f30",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ae8377ea0f69e93",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Covid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f0e0144a18dcc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Probabilistic Reasoning I Review\n",
    "- Bayes Networks\n",
    "    - follow Bayes' theorem\n",
    "        - $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "    - a directed acyclic graph (DAG)\n",
    "        - nodes represent random variables\n",
    "        - edges represent dependencies\n",
    "        - each node has a conditional probability table (CPT)\n",
    "            - the probability of the node given its parents\n",
    "    - given k parents, the CPT has $2^k$ entries\n",
    "        - normalization is done by dividing by the sum of the entries\n",
    "            - this makes the sum of the entries equal to 1\n",
    "    - simplified chain rule\n",
    "        - $P(X, Y, Z) = P(X|Y, Z)P(Y|Z)P(Z)$\n",
    "        - allows you to expand and simplify sums of probabilities in a Bayes network\n",
    "        - it can be useful to draw the network to see when some things can be simplified\n",
    "            - e.g. if X is independent of Z given Y, then $P(X|Y, Z) = P(X|Y)$\n",
    "            - e.g. relationships that are evident in the network involving parents and children\n",
    "                        - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f2cbf54b46741",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Probabilistic Reasoning II\n",
    "- inference\n",
    "    - given evidence, what is the probability of a query?\n",
    "- exact inference\n",
    "    - enumeration\n",
    "        - enumerate all possible values of all variables\n",
    "        - sum over all values that are consistent with the evidence\n",
    "        - divide by the sum of all values\n",
    "        - complexity is $O(2^n)$ for a binary variable\n",
    "            - else $O(d^n)$ where d is the number of values a variable can take on and n is the number of variables\n",
    "            - computed in a Bayesian network with n variables by summing over the CPTs\n",
    "            - i.e. the number of parent entries you need to know or compute to find the probability of a variable\n",
    "    - variable elimination\n",
    "        - eliminate variables that are not in the query or evidence\n",
    "        - sum out variables that are not in the query\n",
    "        - multiply and normalize\n",
    "        - complexity is $O(n^2)$ where n is the number of variables\n",
    "            - regroup terms and share partial results\n",
    "    - junction tree\n",
    "        - convert the Bayes network to a junction tree\n",
    "        - sum out variables that are not in the query\n",
    "        - multiply and normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b821ec6e619b74",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- e.g. Polytrees\n",
    "    - singly connected Bayes networks\n",
    "        - sort of like a tree or several connected trees\n",
    "        - only one path from any node to any other node    \n",
    "        - it is easier because as you go down the tree, it gets simpler\n",
    "    - NP-hard to find the optimal junction tree\n",
    "    - time and space complexity is $O(n)$\n",
    "    - the process is the same as above, but each "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7b947cea18ae8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- approximate inference\n",
    "    - sampling\n",
    "        - sample values of the variables\n",
    "        - keep track of the values that are consistent with the evidence\n",
    "        - divide by the sum of all values\n",
    "    - MCMC\n",
    "        - sample values of the variables\n",
    "        - keep track of the values that are consistent with the evidence\n",
    "        - divide by the sum of all values\n",
    "        - use the Markov chain to sample values\n",
    "- e.g.\n",
    "    - B = battery charged\n",
    "    - G = gauge shows full charge\n",
    "    - L = liftable rock\n",
    "    - M = arm moves\n",
    "        - B and L are independent\n",
    "        - G is independent of L but dependent on B\n",
    "        - M is dependent on B and L\n",
    "    - probabilities\n",
    "        - p(B) = 0.95\n",
    "        - p(L) = 0.7\n",
    "        - p(G|B) = 0.95, gauge shows full charge if battery is charged\n",
    "        - p(G|$\\lnot$B) = 0.1, gauge shows full charge if battery is not charged\n",
    "        - p(M|B, L) = 0.9, arm moves if battery is charged and rock is liftable\n",
    "        - p(M|B, $\\lnot$L) = 0.05, arm moves if battery is charged and rock is not liftable\n",
    "        - p(M|$\\lnot$B, L) = 0.0, arm does not move if battery is not charged and rock is liftable\n",
    "        - p(M|$\\lnot$B, $\\lnot$L) = 0.0, arm does not move if battery is not charged and rock is not liftable\n",
    "    - direct sampling\n",
    "        - sample in topological order (parents before children)\n",
    "        - repeat N times\n",
    "            - sample p(B)\n",
    "            - sample p(L)\n",
    "            - sample p(G|B)\n",
    "            - sample p(M|B, L) and p(M|B, $\\lnot$L)\n",
    "            - record the values and add to a count of the possible sets\n",
    "                - e.g. if B = True, L = True, G = False, M = True, then add to the count for (T,T,F,T)\n",
    "        - divide by N and you have the probability of each atomic event\n",
    "            - an atomic event is a set of values for all the variables at a given time\n",
    "    - rejection sampling\n",
    "        - evidence refers to something \"given\"\n",
    "            - e.g. p(B|M) uses M as evidence and B as the query\n",
    "        - if you have evidence, sample only the variables meeting the evidence\n",
    "            - sample p(B)\n",
    "            - sample p(L)\n",
    "            - sample p(G|B)\n",
    "            - sample p(M|B, L) and p(M|B, $\\lnot$L)\n",
    "            - if it does not meet the evidence, discard the sample\n",
    "            - else, increase the count for the atomic event\n",
    "                - if query, then increase the count for the query\n",
    "                    - i.e. P(Q|E) = P(Q and E) / P(E) where Q is the query and E is the evidence\n",
    "        - as N approaches infinity, the probability of the query approaches the true probability\n",
    "    - both rely on the law of large numbers\n",
    "        - as N approaches infinity, the sample mean approaches the true mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14f477e27106f7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- fuzzy logic\n",
    "    - alternative framework for dealing with uncertainty\n",
    "    - deals with the \"thruthiness\" of linguistic concepts or variables\n",
    "        - e.g. \n",
    "            - 6' is \"tall\" is true to degree 0.5\n",
    "            - 6'6\" is \"tall\" to degree 1.0\n",
    "            - degree likely comes from measurements/polling/observed data\n",
    "    - Matlab has a good guide to this, Foundations of Fuzzy Logic\n",
    "    - fuzzy sets\n",
    "        - a **crisp** set is a collection of elements\n",
    "            - an object is a member or it is not a member\n",
    "                - there is certainty\n",
    "            - e.g. if the set of \"tall\" starts at 6', it is [6', +$\\infty$)\n",
    "                - 6' is a member, 5'11\" is not a member\n",
    "        - a **fuzzy** set is a collection of elements with degrees of membership\n",
    "            - an object is a member to a certain degree\n",
    "                - there is uncertainty\n",
    "            - membership degree may be defined by some characteristic/membership function\n",
    "                - e.g. a person is \"tall\" to a degree of 0.5 as defined by a membership function\n",
    "                - it is still possible to have a crisp value in a fuzzy set\n",
    "                    - e.g. a person is \"tall\" to a degree of 1.0 if they are 6'6\"\n",
    "        - membership functions\n",
    "            - a function that maps an element to a degree of membership\n",
    "            - e.g. \"close to zero\" \n",
    "                - it could be a gaussian function centered at zero with height 1\n",
    "                - it could be a two linear functions with a slope of 1 and -1 centered at zero, else 0\n",
    "        - logic rule conversion\n",
    "            - Crisp\n",
    "                - A = 0 or 1\n",
    "            - Fuzzy\n",
    "                - 0 $\\leq$ A $\\leq$ 1\n",
    "            -  Crisp\n",
    "                - $\\lnot$ A\n",
    "            - Fuzy\n",
    "                - 1 - $\\mu$(A)\n",
    "            - Crisp\n",
    "                - A OR B\n",
    "            - Fuzzy\n",
    "                - max($\\mu$(A), $\\mu$(B))\n",
    "            - Crisp\n",
    "                - A AND B\n",
    "            - Fuzzy\n",
    "                - min($\\mu$(A), $\\mu$(B))\n",
    "            - Crisp\n",
    "                - A $\\Rightarrow$ B\n",
    "            - Fuzzy\n",
    "                - max(min($\\mu$(A), $\\mu$(B)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dbf2721cd05e4b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Fuzzy If-Then rules\n",
    "    - can be used to define a fuzzy expert system or fuzzy controller\n",
    "    - e.g. car controller\n",
    "        - S is speed\n",
    "        - A is acceleration\n",
    "        - P is pedal position\n",
    "            - if S is \"fast\" and A is \"accelerating\" then P is \"pressed\"\n",
    "            - if A is not \"accelerating\" and P is \"pressed\" then S is fast\n",
    "    - combine results and create a crisp output by some metric\n",
    "        - e.g. defuzzification\n",
    "            - centroid\n",
    "                - find the center of mass of the fuzzy set\n",
    "                - the center of mass is the average of the values weighted by the membership function\n",
    "            - max membership\n",
    "                - find the value with the highest membership\n",
    "            - mean of maxima\n",
    "                - find the value with the highest membership and take the average of the values\n",
    "            - weighted average\n",
    "                - find the average of the values weighted by the membership function\n",
    "            - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39157aaa55cf008",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Making Simple Decisions\n",
    "- if you figure this out bottle it and sell it (I'll buy)\n",
    "- preferences\n",
    "    - A > B means A is preferred to B\n",
    "    - A $\\gtrsim$ B means A is at least as good as B\n",
    "    - A $\\sim$ B means A is indifferent to B\n",
    "    - everything $>$ [$U_\\bot$]\n",
    "    - everything $<$ [$U_\\top$]\n",
    "- lottery notation\n",
    "    - [p1, S1; p2, S2; ...; pn, Sn]\n",
    "        - p is the probability of the outcome\n",
    "        - S is the outcome\n",
    "        - e.g. [0.5, 100; 0.5, 0]\n",
    "            - 50% chance of winning 100, 50% chance of winning 0\n",
    "    - $\\sum_{i=1}^{n} p_i = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f27c39c31804f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Utility Theory\n",
    "- Orderability\n",
    "    - if A > B xor A < B, then A $\\sim$ B\n",
    "- Transitivity\n",
    "    - if A > B and B > C, then A > C\n",
    "- Continuity\n",
    "    - if A > B > C, then there exists a p such that [p, A; 1-p, C] $\\sim$ B\n",
    "        - i.e. there is some probability p where the lottery is indifferent to B\n",
    "        - e.g. A = $\\textdollar100$, B = $\\textdollar10$, C = $\\textdollar1$\n",
    "            - there is some probability p where you would pay $\\textdollar10$ for a chance at $\\textdollar100$\n",
    "- Substitution\n",
    "    - if A $\\sim$ B, then [p, A; 1-p, C] $\\sim$ [p, B; 1-p, C]\n",
    "        - i.e. if A is indifferent to B, then a lottery with A is indifferent to a lottery with B\n",
    "- Monotonicity\n",
    "    - if A > B, then [p, A; 1-p, C] > [p, B; 1-p, C]\n",
    "        - i.e. if A is preferred to B, then a lottery with A is preferred to a lottery with B\n",
    "- Decomposability\n",
    "    - you can decompose a lottery into simpler lotteries\n",
    "        - e.g. [p, A; 1-p, B] = [p, A; 1-p, C] + [p, C; 1-p, B]\n",
    "- Dominance\n",
    "    - strict dominance\n",
    "        - plotted on x and y, if B > A in x and y, then A is strictly dominated by B\n",
    "            - anything in the $\\gtrsim$ x AND $\\gtrsim$ y from A \n",
    "        - if one lottery's outcomes are always better than another, then the first lottery is strictly dominant\n",
    "    - stochastic dominance\n",
    "        - complementary cumulative distribution strictly dominates\n",
    "            - P[A $\\geq$ x] $\\geq$ P[B $\\geq$ x] **and** for some x, P[A$\\geq$x] > P[B$\\geq$x]\n",
    "        - e.g. \n",
    "            - B is uniform chances between 3 and 8\n",
    "                - height of the curve is 1/5\n",
    "            - A is uniform chances between 4 and 10\n",
    "                - height of the curve is 1/6\n",
    "            - A stochastically dominates B\n",
    "            - survival function\n",
    "                - P[X $\\geq$ x]\n",
    "                - P[B $\\geq$ 3] = 1, then decreases to 0 at 8\n",
    "                - P[A $\\geq$ 4] = 1, then decreases to 0 at 10\n",
    "                - A's curve is **always** $\\geq$ B's curve\n",
    "                - if A was from 4 to 5, neither would dominate because A is higher in some places and B is higher in others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d324e5cff34e7d91",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Utility Functions\n",
    "    - give you a number for how much you like something\n",
    "    - more useful than preferences\n",
    "        - you can do exact computations with utility functions\n",
    "    1. U(A) > U(B) iff A > B AND U(A) = U(B) iff A $\\sim$ B\n",
    "    2. U([p1, S1; p2, S2; ...; pn, Sn]) \n",
    "        - = $\\sum_{i=1}^{n} p_i U(S_i)$\n",
    "            - $\\neq$ $\\sum_{i=1}^{n} p_i S_i$\n",
    "        - i.e. the utility of a lottery is the sum of the utilities of the outcomes weighted by the probabilities\n",
    "    - if you have U(s), you have everything\n",
    "    - relation to rational agents\n",
    "        - U(s) represents the agent's preferences\n",
    "        - the expected utility of an action given evidence EU(a|e) is the average utility value of the outcomes of the action weighted by the probabilities of the outcomes\n",
    "            - EU(a|e) = $\\sum_{s'}^{n} P(Result(a) = s'|a,e) U(s')$\n",
    "        - the rational agent chooses the action with the highest expected utility\n",
    "            - argmax$_a$ EU(a|e)\n",
    "            - Maximum Expected Utility (MEU) principle says that the rational agent chooses the action with the highest expected utility\n",
    "- the hard part: operationalizing the utility function\n",
    "    - you don't know the probabilities of all moves or contingencies an adversary might make\n",
    "    - i.e. you can't really fill out the decision tree\n",
    "        - e.g. one armed bandit (slot machine)\n",
    "            - you have to pull the lever to see the outcomes\n",
    "        - e.g. where should NIH spend its money?\n",
    "            - you can't know the probabilities of all the outcomes\n",
    "    - you learn the probabilities as you go\n",
    "        - e.g. you can learn the probabilities of the slot machine as you play\n",
    "- there may be special situations that throw everything off\n",
    "    - e.g. wind disturbances on a plane, nonstandard power grid utilization, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de673fc12870631",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Multiple Attributes\n",
    "    - ordered/priorities\n",
    "        - tuple ordering or heirarchy\n",
    "            - <safety, mission, stealth>\n",
    "            - it may become a 2D or larger nested system\n",
    "                - e.g. for a plane, there may be a polygon of safety, within that there be a fuel polygon\n",
    "                    - when you meet safety, then you can care about fuel\n",
    "    - additive\n",
    "        - V(x1, x2, ..., xn) = $\\sum_{i=1}^{n} c_i V_i(x_i)$\n",
    "            - V(noise, cost, deaths) for an airport\n",
    "- decision network\n",
    "    - decision nodes\n",
    "        - actions\n",
    "    - chance nodes\n",
    "        - uncertain events\n",
    "    - utility nodes\n",
    "        - utility of the outcome\n",
    "    - e.g.\n",
    "        - decision node: car wash?\n",
    "        - dependent chance nodes: cost, time \n",
    "        - independent chance nodes: forget\n",
    "        - utility node: U(1-$\\gamma^d$)f(-cost - 10, time)\n",
    "            - $\\gamma$ is the discount factor\n",
    "            - d is the depth of the decision tree\n",
    "            - f is the utility function\n",
    "                - e.g. f(x, y) = x + y\n",
    "            - 0 < $\\gamma$ < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d552b8b9524a312",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a21ea00d9affb1a1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5b53bbfbd298f01",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd4bab82001b3b03",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1c06dcae8086682",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e855302877817023",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caf8b01a795b9e7b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
