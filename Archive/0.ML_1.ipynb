{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T21:57:06.578432600Z",
     "start_time": "2023-10-03T21:57:04.586176600Z"
    }
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ML Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Supervised Machine Learning\n",
    "- Training, Validation, Testing datasets\n",
    "    - Training: used to train the model\n",
    "    - Validation: used to tune the hyperparameters\n",
    "        - Hyperparameters: parameters that are not learned by the model\n",
    "        - modern models often handle this automatically\n",
    "        - terminology has evolved so older sources may say validation but mean testing data\n",
    "    - Testing: used to evaluate the model\n",
    "- Cross Validation\n",
    "    - iteratively train and test the model on different subsets of the data\n",
    "    - allows you to use all of the data for training and testing without overfitting (hopefully)\n",
    "    - Leave-One-Out Cross Validation (LOOCV)\n",
    "        - train on all but one data point\n",
    "        - test on the one data point\n",
    "        - repeat for all data points\n",
    "        - pros: uses all data for training and testing\n",
    "        - cons: computationally expensive and can lead to overfitting\n",
    "        - **Should not be used**\n",
    "    - k-fold Cross Validation\n",
    "        - split data into k subsets\n",
    "        - train on k-1 subsets\n",
    "        - test on the remaining subset\n",
    "        - repeat for all subsets\n",
    "        - pros: computationally efficient\n",
    "        - cons: uses less data for training and testing\n",
    "  ### We will use 2-fold cross validation for this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### USeful Python Libraries\n",
    "- NumPy\n",
    "    - good for linear algebra\n",
    "- scikit-learn\n",
    "    - good for machine learning\n",
    "- pandas\n",
    "    - good for data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T02:14:06.033723Z",
     "start_time": "2023-09-15T02:14:05.588716200Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'files/iris.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfiles/iris.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      3\u001B[0m names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msepal-length\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msepal-width\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpetal-length\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpetal-width\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclass\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m----> 5\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    944\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    945\u001B[0m )\n\u001B[0;32m    946\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    608\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    610\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 611\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    614\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1448\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1704\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1707\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1708\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1711\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1712\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1714\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1715\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1716\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    866\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    867\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    868\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    869\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'files/iris.csv'"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "url = \"files/iris.csv\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length',\n",
    "'petal-width', 'class']\n",
    "dataset = read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.032722300Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Most scikit-learn library functions use the following convention:\n",
    "- X is an array containing all the features in the first columns and the class in the last column.\n",
    "- y is an array containing only the classes.\n",
    "- Note: Test_size must be set to 0.50 for 2-fold cross-validation which we will be using in this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T02:14:06.034723100Z",
     "start_time": "2023-09-15T02:14:06.033723Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create Arrays for Features and Classes\n",
    "array = dataset.values\n",
    "X = array[:,0:4] #contains flower features (petal length, etc..)\n",
    "y = array[:,4] #contains flower names\n",
    "#Split Data into 2 Folds for Training and Test\n",
    "X_Fold1, X_Fold2, y_Fold1, y_Fold2 = train_test_split(X, y, test_size=0.50, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.033723Z"
    }
   },
   "outputs": [],
   "source": [
    "model = GaussianNB() #create model of type Gaussian Naive Bayes\n",
    "model.fit(X_Fold1, y_Fold1)  #train model on Fold1\n",
    "pred1 = model.predict(X_Fold2)  #test model on Fold2\n",
    "model.fit(X_Fold2, y_Fold2)  #train model on Fold2\n",
    "pred2 = model.predict(X_Fold1)  #test model on Fold1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluating the Model\n",
    "- used to quantify\n",
    "    - desired performance vs actual performance\n",
    "    - desired vs baseline performance\n",
    "    - progress over time\n",
    "- Accuracy\n",
    "    - number of correct predictions / total number of predictions\n",
    "    - good for balanced datasets\n",
    "    - bad for unbalanced datasets\n",
    "- Confusion Matrix\n",
    "    - shows the number of correct and incorrect predictions\n",
    "    - good for unbalanced datasets\n",
    "    - at it's most basic, made up of 4 values\n",
    "        - true positives (TP)\n",
    "        - true negatives (TN)\n",
    "        - false positives (FP)\n",
    "        - false negatives (FN)\n",
    "        - FP and FN are often called Type I and Type II errors\n",
    "        - <img src=\"images/FP_and_FN.png\" alt=\"drawing\" width=\"500\"/>\n",
    "    - accuracy, precision, recall, and F1 score can be calculated from the confusion matrix\n",
    "        - accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "                - how often the model is correct\n",
    "        - precision = TP / (TP + FP)\n",
    "                - how often the model is correct when it predicts positive\n",
    "        - recall = TP / (TP + FN)\n",
    "                - how often the model predicts positive when it is correct\n",
    "        - F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "                - harmonic mean of precision and recall\n",
    "                - good for unbalanced datasets\n",
    "    - F-Score\n",
    "        - F-Score or F-measure is used in statistical analysis of binary classification\n",
    "        - F-Score is the harmonic mean of precision and recall\n",
    "        - highest possible value is 1.0\n",
    "        - lowest possible value is 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Multiclass Confusion Matrices\n",
    "- confusion matrices can be extended to multiclass problems\n",
    "    - e.g.\n",
    "        - <img src=\"images/multiclass.png\" alt=\"drawing\" width=\"500\"/>\n",
    "        - precisoin of cat is from the horizontal cat row, 4/13\n",
    "        - recall of cat is from the vertical cat column, 4/6\n",
    "- there is no standard orientation of the matrix\n",
    "    - i.e. the predicted and true labels can be on the rows or columns\n",
    "    - so always read the labels\n",
    "    - the diagonal is always the true positives\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.034723100Z"
    }
   },
   "outputs": [],
   "source": [
    "actual = np.concatenate([y_Fold2, y_Fold1])  #combine the actual labels from both folds\n",
    "predicted = np.concatenate([pred1, pred2])   #combine the predicted labels from both folds\n",
    "print(f\"Accuracy: {accuracy_score(actual, predicted)}\")   #print the accuracy\n",
    "print(\"Confusion Matrix:\")   #print the confusion matrix\n",
    "print(confusion_matrix(actual, predicted))   #print the confusion matrix\n",
    "print(\"Classification Report:\")   #print the classification report\n",
    "print(classification_report(actual, predicted))   #print the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Regression Classifiers\n",
    "- linear regression\n",
    "    - single input variable\n",
    "    - $y = b_0 + b_1x$\n",
    "        - $y$ is the response\n",
    "        - $b_0$ is the bias coefficient\n",
    "        - $b_1$ is the coefficient for the input variable\n",
    "    - training data is used to find the values of the coefficients\n",
    "        - finding the best fit line\n",
    "        - many different algorithms can be used to find the best fit line\n",
    "            - ordinary least squares\n",
    "            - gradient descent\n",
    "            - stochastic gradient descent\n",
    "            - etc...\n",
    "    - once the coefficients are found, the model can be used to make predictions\n",
    "        - $y = 0.5 + 0.8x$\n",
    "        - $y = 0.5 + 0.8(5)$\n",
    "        - $y = 4.5$ \n",
    "- polynomial regression\n",
    "    - nonlinear relationship between the input and response\n",
    "        - $y = b_0 + b_1x + b_2x^2 +$ ...    \n",
    "    - formulas are typically represented as matrices\n",
    "- multiple linear regression\n",
    "    - multiple input variables\n",
    "    - $y = b_0 + b_1x_1 + b_2x_2 +$ ...\n",
    "    - formulas are typically represented as matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T02:14:06.035722900Z",
     "start_time": "2023-09-15T02:14:06.034723100Z"
    }
   },
   "outputs": [],
   "source": [
    "x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x, y = np.array(x), np.array(y)\n",
    "# print(x)\n",
    "# print(y)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression() #create model of type Linear Regression\n",
    "model.fit(x, y)  #train model on data\n",
    "# model = LinearRegression().fit(x, y) # oneliner for the above 2 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluating Regression Models\n",
    "- $R^2$ is a measure of the fit\n",
    "- can be obtained with `.score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.034723100Z"
    }
   },
   "outputs": [],
   "source": [
    "model.score(x, y)\n",
    "print('B_0:', model.intercept_)\n",
    "print('[B_1 B_2]:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Regression\n",
    "- Strengths\n",
    "    - straightforward to understand and explain\n",
    "    - can be regularized to avoid overfitting\n",
    "    - easily updated with new data via gradient descent\n",
    "- Weaknesses\n",
    "    - assumes a linear relationship between the input and response\n",
    "        - performs poorly with nonlinear relationships\n",
    "    - not flexible enough to capture more complex relationships\n",
    "        - e.g. polynomial regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### In Class 29Aug23\n",
    "\n",
    "1. a) Polynomial regression, the data clearly does not follow a straight line\n",
    "1. b) Linear regression, the data follows a straight line\n",
    "2.  \n",
    "- $y = 0.2 + 0.1x_1 + 0.05x_2$\n",
    "- $x_1 = 5.1$\n",
    "- $x_2 = 1.8$\n",
    "- $y = 0.2 + 0.1(5.1) + 0.05(1.8)$\n",
    "- $y = 0.2 + 0.51 + 0.09$\n",
    "- $y = 0.8$\n",
    "- The model predicts Iris-setosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### scikit-learn Algorithm for Regression\n",
    "- needed for assignment 2\n",
    "- doesn't work without other code (as of 29Aug23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.034723100Z"
    }
   },
   "outputs": [],
   "source": [
    "def regModel(name, model):\n",
    "    #Fit and transform data sets according to the regression degree\n",
    "    poly_reg = None\n",
    "    if (name == \"Linear Regression\"):\n",
    "        poly_reg = PolynomialFeatures(degree=1)\n",
    "    elif(name == \"2 Degree Polynomial Regression\"):\n",
    "        poly_reg = PolynomialFeatures(degree=2)\n",
    "    elif(name == \"3 Degree Polynomial Regression\"):\n",
    "        poly_reg = PolynomialFeatures(degree=3)\n",
    "    #create 2 folds\n",
    "    X_Poly1 = poly_reg.fit_transform(X_Fold1)\n",
    "    X_Poly2 = poly_reg.fit_transform(X_Fold2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.035722900Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_Poly1, y_Fold1) #first fold training\n",
    "pred1 = model.predict(X_Poly2).round() #first fold testing\n",
    "#regression may produce values < 1 or > 3.\n",
    "pred1 = np.where(pred1 >= 3.0, 2.0, pred1) #map all values > 3 to 2\n",
    "pred1 = np.where(pred1 <= -1.0, 0.0, pred1) #map all values < 0 to 0\n",
    "model.fit(X_Poly2, y_Fold2) #second fold training\n",
    "pred2 = model.predict(X_Poly1).round() #second fold testing\n",
    "pred2 = np.where(pred2 >= 3.0, 2.0, pred2)\n",
    "pred2 = np.where(pred2 <= -1.0, 0.0, pred2)\n",
    "actual = np.concatenate([y_Fold2, y_Fold1])\n",
    "predicted = np.concatenate([pred1, pred2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Naive Bayesian Classifiers\n",
    "- simplest ML classifier\n",
    "- gold standard for comparing other classifiers\n",
    "    - if a new classifier is not better than a naive bayesian classifier, it is not worth using \n",
    "- based on Bayes' Theorem of conditional probability\n",
    "    - $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "    - $P(A|B)$ is the probability of A given B\n",
    "    - $P(B|A)$ is the probability of B given A\n",
    "    - $P(A)$ is the probability of A\n",
    "    - $P(B)$ is the probability of B\n",
    "- mean and variance are used to summarize the data\n",
    "    - mean  $\\mu$\n",
    "         - the average\n",
    "        - $\\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i $\n",
    "    - variance $\\sigma^2$\n",
    "        - how much the data varies from the mean\n",
    "        - $\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2 $\n",
    "- NB Classifiers are conditional probability models\n",
    "    - a sample to be classified is represented as a vector of features\n",
    "        - $x = (x_1, x_2, x_3, ..., x_n)$\n",
    "    - calculates the conditional probability of each class given the features\n",
    "        - $P(C_k|x_1, x_2, x_3, ..., x_n)$\n",
    "    - the class with the highest probability is the predicted class\n",
    "- problem\n",
    "    - if the number of features is large, classification by conditional probability is infeasible\n",
    "    - thus the model is reformulated to be more tractable\n",
    "        - the denominator is removed because it is effectively a constant\n",
    "- reduced form\n",
    "    - posterior numerator\n",
    "        - posterior numerator = prior * likelihood\n",
    "        - can estimate $p(x_k|C_i)$ from the training data\n",
    "            - $p(x_k|C_i) = \\frac{1}{\\sqrt{2\\pi\\sigma_{ik}^2}}e^{-\\frac{(x_k - \\mu_{ik})^2}{2\\sigma_{ik}^2}}$\n",
    "            - $x_k$ is the value of feature k in the sample\n",
    "            - $\\mu_{ik}$ is the mean of feature k for the entire training set\n",
    "            - $\\sigma_{ik}^2$ is the variance of feature k for the entire training set\n",
    "            - $C_i$ is the class\n",
    "            - $e$ is Euler's number (2.71828...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Summary of Naive Bayes\n",
    "- Strengths\n",
    "    - simple and easy to implement\n",
    "    - fast\n",
    "    - good for high dimensional data\n",
    "    - good for categorical data\n",
    "    - good for text classification\n",
    "- Weaknesses\n",
    "    - assumes independence of features\n",
    "    - assumes a gaussian distribution of features\n",
    "    - based on probability theory\n",
    "        - real world data is often more complex\n",
    "    - can be outperformed by other classifiers\n",
    "- Training\n",
    "    - calculate one probability for each class\n",
    "    - calculate n * m conditional probabilities\n",
    "        - n is the number of class\n",
    "        - m is the number of features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### In Class 31Aug23\n",
    "- Given:\n",
    "    - The iris data set contains 150 samples of data, 50 for each variety of iris: Iris-setosa, Irisversicolor & Iris-virginica\n",
    "    - We will use 149 samples of the data to train the classifier, and test it with one sample of Irisvirginica which has the following features:\n",
    "        - sepal-length = 5.9\n",
    "        - sepal-width = 3\n",
    "        - petal-length = 5.1\n",
    "        - petal-width = 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. posterior numerator each class\n",
    "    - posterior numerator(Iris-setosa) = $p(Iris-setosa) \\cdot p(sepal-length|Iris-setosa) \\cdot p(sepal-width|Iris-setosa) \\cdot p(petal-length|Iris-setosa) \\cdot p(petal-width|Iris-setosa)$\n",
    "    - posterior numerator(Iris-versicolor) = $p(Iris-versicolor) \\cdot p(sepal-length|Iris-versicolor) \\cdot p(sepal-width|Iris-versicolor) \\cdot p(petal-length|Iris-versicolor) \\cdot p(petal-width|Iris-versicolor)$\n",
    "    - posterior numerator(Iris-virginica) = $p(Iris-virginica) \\cdot p(sepal-length|Iris-virginica) \\cdot p(sepal-width|Iris-virginica) \\cdot p(petal-length|Iris-virginica) \\cdot p(petal-width|Iris-virginica)$\n",
    "2. P for each class\n",
    "    - P(Iris-setosa) = 50/150 = 0.333\n",
    "    - P(Iris-versicolor) = 50/150 = 0.333\n",
    "    - P(Iris-virginica) = 50/150 = 0.333\n",
    "3. given mean 5.0 and variance 0.12 for p(sepal-length } Iris-setosa) fill in the formula\n",
    "    - $p(x_k|C_i) = \\frac{1}{\\sqrt{2\\pi\\sigma_{ik}^2}}e^{-\\frac{(x_k - \\mu_{ik})^2}{2\\sigma_{ik}^2}}$ \n",
    "    - $p(sepal-length|Iris-setosa) = \\frac{1}{\\sqrt{2\\pi(0.12)^2}}e^{-\\frac{(5.9 - 5.0)^2}{2(0.12)^2}}$\n",
    "    - $p(sepal-length|Iris-setosa) = 2.0286 \\cdot 10^{-12}$\n",
    "4. how many conditional probabilities are there?\n",
    "    - 4 features * 3 classes = 12 conditional probabilities\n",
    "5. $posterior numerator(Iris-setosa) = 0.005, posterior numerator(Iris-versicolor) = 0.002, posterior numerator(Iris-virginica) = 0.003$, which variety of iris is the sample most likely to be?\n",
    "    - Iris-setosa because it has the highest posterior numerator"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discriminant Analysis Classifiers\n",
    "- <img src=\"images/discrimination_analysis.png\" alt=\"drawing\" width=\"500\"/>\n",
    "- linear and quadratic discriminant analysis\n",
    "    - two classic classifiers\n",
    "    - provide closed-form solutions which are easy to compute\n",
    "    - inherently multiclass\n",
    "    - proven to work well in practice\n",
    "    - do not have hyperparameters to tune\n",
    "- Variance \n",
    "    - how far a set of random numbers are spread from their average value\n",
    "- Covariance\n",
    "    - joint variability of two random variables (two features)\n",
    "    - measures interdependence between two features\n",
    "    - if the covariance is positive, the two features increase together\n",
    "    - if the covariance is negative, the two features move in opposite directions\n",
    "        - e.g. as one feature increases, the other decreases\n",
    "    - magnitude of the covariance is not normalized so it is hard to interpret\n",
    "        - the normalized version is called the correlation coefficient\n",
    "            - correlation coefficient is always between 0 and 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### shrinkage function\n",
    "- from scikit-learn\n",
    "- a tool to improve the covariance matrix estimate when the number of samples is small compared to the number of features\n",
    "- set shrinkage parameter of discriminant_analysis.LinearDiscriminantAnalysis to 'auto'\n",
    "    - automatically determines the optimal shrinkage parameter\n",
    "- "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LDA and QDA\n",
    "- predictions can be derived from Bayes' Theorem\n",
    "- LDA\n",
    "    - assumes the covariance of the features is the same for each class\n",
    "    - assumes the distribution of each class is normal\n",
    "- QDA\n",
    "    - assumes the covariance of the features is different for each class\n",
    "    - assumes the distribution of each class is normal\n",
    "- Both\n",
    "    - assume the features are statistically independent of each other\n",
    "    - use Bayes' Theorem to calculate the probability of each class\n",
    "    - use the class with the highest probability as the predicted class\n",
    "- Naive Bayes is a simplified version of the LDA formula\n",
    "    - covariance matrix is assumed to be diagonal\n",
    "        - i.e. the features are independent of each other\n",
    "#### Model Comparison\n",
    "- LDA\n",
    "    - advantages\n",
    "        - closed-form solution\n",
    "        - inherently multiclass\n",
    "        - proven to work well in practice\n",
    "        - no hyperparameters to tune\n",
    "        - use covariance to account for correlation between features\n",
    "    - disadvantages\n",
    "        - assumes the distribution of each class is gaussian\n",
    "        - assumes the covariance of the features is the same for each class\n",
    "        - can only learn linear boundaries\n",
    "- QDA\n",
    "    - advantages\n",
    "        - closed-form solution\n",
    "        - inherently multiclass\n",
    "        - proven to work well in practice\n",
    "        - no hyperparameters to tune\n",
    "        - use covariance to account for correlation between features\n",
    "    - disadvantages\n",
    "        - assumes the distribution of each class is gaussian\n",
    "        - assumes the covariance of the features is different for each class\n",
    "        - can only learn linear boundaries  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### k-Nearst Neighbors (kNN) Classifiers\n",
    "- model representation is the entire training dataset on a graph\n",
    "- no  model required, just store the training data\n",
    "- easy to update by adding new training data or removing old bad data\n",
    "- lazy learner\n",
    "    - does not learn a discriminative function from the training data\n",
    "    - makes predictions based on the entire training dataset\n",
    "    - computationally expensive\n",
    "- works by finding the k closest training samples in the feature space\n",
    "    - the predicted class is the most common class among the k nearest neighbors\n",
    "        - e.g. if k = 3 and the 3 nearest neighbors are 2 Iris-setosa and 1 Iris-versicolor, the predicted class is Iris-setosa\n",
    "    - the distance between samples is calculated using a distance metric\n",
    "        - e.g. Euclidean distance, Hamming, Manhattan distance, Minkowski distance, etc...\n",
    "- Euclidean distance\n",
    "    - distance between two points\n",
    "    - good if the input variables are similar in type\n",
    "        - e.g. all measured widths and heights\n",
    "- Hamming distance\n",
    "    - distance between binary vectors\n",
    "- Manhattan distance\n",
    "    - distance between two points measured along axes at right angles\n",
    "    - good if the input variables are not similar in type\n",
    "        - e.g. age, gender, height, etc...\n",
    "- Minkowski distance\n",
    "    - generalization of Euclidean and Manhattan distance\n",
    "- varying the k value can affect the performance of the model\n",
    "    - small k values can lead to overfitting\n",
    "    - large k values can lead to underfitting\n",
    "    - k should be odd if there are an even number of classes or even if there are an odd number of classes\n",
    "        - e.g. if there are 3 classes, k should be odd\n",
    "        - e.g. if there are 4 classes, k should be even\n",
    "        - ties can be broken by expanding k by 1 and looking at the class of the next nearest neighbor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Curse of Dimensionality\n",
    "- kNN works well for small numbers of features (dimensions) but not for large numbers of features\n",
    "    - each feature adds a dimension to the feature space\n",
    "    - as the number of features increases, the number of samples required to maintain accuracy increases exponentially\n",
    "    - in high dimensions, points that may be similar may have very large distances between them\n",
    "#### General\n",
    "- rescaling data\n",
    "    - kNN works well for data that is in the same scale\n",
    "- address missing data \n",
    "    - missing data increases the distance between samples\n",
    "- lower dimensionality\n",
    "    - remove features that are not important\n",
    "    - reduce the number of features using PCA\n",
    "- advantages\n",
    "    - simple and easy to implement\n",
    "    - easy to update\n",
    "    - makes no assumptions about the distribution or independence of the data\n",
    "- disadvantages\n",
    "    - memory intensive\n",
    "    - performs poorly on high dimensional data\n",
    "    - requires a meaningful distance function to calculate similarity "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In Class 05Sep23\n",
    "1. Matrix B because the covariances of all features are 0 except with themselves, and C because the covariances are all nearly 0 except with themselves\n",
    "2. Matrix A because the covariance of feature c with a and b are nearly 0 but the covariance of a and b are slightly higher\n",
    "3. Matrix A because the covariance of feature a and b are the highest of the 3 matrices\n",
    "4. $d = \\sqrt{(3-1)^2+(8-4)^2+(2-8)^2} \\approx 7.48$\n",
    "5. cat\n",
    "6. because the 3 closest training data points are two cats and a dog"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Support Vector Machines (SVM) Classifiers\n",
    "- SVMs are a set of supervised learning methods used for classification, regression, and outliers detection\n",
    "- plots data and draws a line between classes\n",
    "    - the line is called a hyperplane\n",
    "    - the hyperplane is the decision boundary\n",
    "- assumes that like things are in the same class\n",
    "    - i.e. they clump together\n",
    "- scikit-learn\n",
    "    - svm.LinearSVC\n",
    "        - linear SVM\n",
    "        - uses one-vs-rest\n",
    "        - can be used for multiclass classification\n",
    "    - svm.NuSVC\n",
    "        - non-linear SVM\n",
    "        - uses one-vs-one\n",
    "        - can be used for multiclass classification\n",
    "    - svm.SVC\n",
    "        - non-linear SVM\n",
    "        - uses one-vs-one\n",
    "        - can be used for multiclass classification\n",
    "        - uses a kernel trick to transform the data into a higher dimension\n",
    "            - e.g. from 2D to 3D\n",
    "            - allows the data to be separated by a hyperplane\n",
    "            - the kernel trick is computationally expensive\n",
    "    - SVC vs NuSVC\n",
    "        - different kernels\n",
    "        - different regularization parameters\n",
    "        - if the class can be divided by a straight line\n",
    "            - use LinearSVC or SVC/NuSVC with a linear kernel\n",
    "        - if the class cannot be divided by a straight line\n",
    "            - use SVC/NuSVC with a non linear kernel\n",
    "- strengths\n",
    "    - non-probabilistic\n",
    "        - i.e. no assumption of gaussian distribution or feature independence\n",
    "    - effective when features > samples\n",
    "    - use subset of training points in decision function\n",
    "        - support vectors\n",
    "    - versatile\n",
    "        - different kernel functions can be specified for the decision function\n",
    "- weaknesses\n",
    "    - over-fitting when features >> samples\n",
    "    - kernel selection is critical\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Trees\n",
    "- good when boundaries between classes are non-linear\n",
    "- non probabilistic because they don't assume specific feature dependence or independence\n",
    "- you can make multivariate decision trees\n",
    "    - they don't really improve accuracy and can reduce it\n",
    "- weaknesses\n",
    "    - tend to overfit\n",
    "    - can be unstable\n",
    "        - small changes in data may generate a completely different tree\n",
    "    - impossible to determine the globally \"optimal\" tree structure\n",
    "### Random Forests\n",
    "-  created to mitigate the weaknesses of decision trees\n",
    "- ensemble learning method\n",
    "    - combines multiple decision trees to produce a better classifier\n",
    "    - each tree is trained on a random subset of the training data\n",
    "    - majority vote from the forest is used to determine the predicted class\n",
    "- extremely randomized trees\n",
    "    - similar to random forests\n",
    "    - splits are chosen at random\n",
    "    - splits are chosen from the entire feature set\n",
    "    - splits are chosen to be the best possible split\n",
    "    - splits are chosen to be the best possible split from a random subset of the feature set\n",
    "- random forests and extremely randomized trees are effective because\n",
    "    - they reduce overfitting\n",
    "    - they reduce variance\n",
    "    - they improve accuracy\n",
    "    - they are computationally efficient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In Class 07Sep23\n",
    "- 1. a) LinearSVC because the classes can be easily split by a straight line\n",
    "- 1. b) SVC/NuSVC because the classes cannot be easily split by a straight line\n",
    "- 2. R1: IF (age<30) AND (student = no) THEN no\n",
    "- 2. R2: IF (age<30) AND (student = yes) THEN yes\n",
    "- 2. R3: IF (30<age<40) THEN yes\n",
    "- 2. R4: IF (40<age) AND (credit_rating = excellent) THEN yes\n",
    "- 2. R5: IF (40<age) AND (credit_rating = fair) THEN no\n",
    "- 3.                 | Taxable Income |\n",
    "                    /         |        \\\n",
    "                  <85k      85k-95k   >95k\n",
    "                   |          |         |\n",
    "                  (no)      (yes)      (no)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.035722900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T02:14:06.036722Z",
     "start_time": "2023-09-15T02:14:06.035722900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.036722Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.036722Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T02:14:06.037726300Z",
     "start_time": "2023-09-15T02:14:06.036722Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.037726300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T02:14:06.037726300Z",
     "start_time": "2023-09-15T02:14:06.037726300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T02:14:06.037726300Z",
     "start_time": "2023-09-15T02:14:06.037726300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T02:14:06.037726300Z",
     "start_time": "2023-09-15T02:14:06.037726300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T02:14:06.039726400Z",
     "start_time": "2023-09-15T02:14:06.037726300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.038725600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.038725600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.038725600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.038725600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.038725600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T02:14:06.038725600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
